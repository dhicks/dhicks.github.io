[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Thanks to a series of academic and policy opportunities, I’ve been able to work on a variety of multidisciplinary research topics, using a variety of qualitative and quantitative methods. This page briefly describes some of my major research interests, including links to relevant publications. A full list of papers and other research outputs can be found on my ORCID profile.\nLast updated 27 June 2019.\n\n\nAcademic institutional effectiveness\nConceptual frameworks from philosophy of science, science and technology studies, and science policy; analytic methods from statistics and data science; and institutional data sources can be combined to support understanding of how science operates. Some of the specific questions I’ve examined in this area include:\n\nIs novel toxicological research at the Environmental Protection Agency integrated into the broader research community?\n\nHow have interdisciplinary funding programs fostered novel collaborations in fields such as robotics and genomics?\n\nWhat factors predict whether underrepresented groups will major in philosophy?\n\nWhat conceptual frameworks can be used to understand the academic politics of research metrics?\n\nHow can article metadata and text mining methods be used to uncover forgotten contributions of women to philosophy of science?\n\n\nSelected publications\n\nHicks, Daniel J., David A. Coil, Carl G. Stahmer, and Jonathan A. Eisen. 2019. “Network Analysis to Evaluate the Impact of Research Funding on Research Community Consolidation.” PLOS ONE 14 (6): e0218273. https://doi.org/10.1371/journal.pone.0218273.\nHicks, Daniel J., Carl Stahmer, and MacKenzie Smith. 2018. “Impacting Capabilities: A Conceptual Framework for the Social Value of Research.” Frontiers in Research Metrics and Analytics 3. https://doi.org/10.3389/frma.2018.00024.\nHicks, Daniel. 2017. “The Underproduction of Philosophy PhDs.” Daily Nous. December 18, 2017. http://dailynous.com/2017/12/18/underproduction-philosophy-phds-daniel-hicks/.\n\n\n\n\nPublic scientific controversies\nPublic scientific controversies are my entry point into discussions of science policy and the role of expertise in democracy. Traditional models of such controversies focus on gaps between “scientists” and “the public,” and explain these gaps in terms of public ignorance and irrationality. In contrast, my approach to these controversies are based on ideas of power, structural oppression, and democratic accountability, in line with my background in feminist philosophy of science and political philosophy.\nI am interested in many different specific issues that fall under the heading of “public scientific controversies.” Some of my current work in this area addresses ideals of transparency and fairness in algorithmic injustice, as well as controversies over obesity and pesticide regulation. My publications in this area are often written for policy audiences and published in venues beyond academic philosophy.\n\nSelected publications\n\nFernández Pinto, Manuela, and Daniel J. Hicks. 2019. “Legitimizing Values in Regulatory Science.” Environmental Health Perspectives 127 (3): 035001. https://doi.org/10.1289/EHP3317.\nHicks, D. J. 2018. “The Safety of Autonomous Vehicles: Lessons from Philosophy of Science.” IEEE Technology and Society Magazine 37 (1): 62–69. https://doi.org/10.1109/MTS.2018.2795123.\nHicks, Daniel J. 2017. “Scientific Controversies as Proxy Politics.” Issues in Science and Technology, January 2017. http://issues.org/33-2/scientific-controversies-as-proxy-politics/\nGMO project\n\nHicks, Daniel J. 2017. “Genetically Modified Crops, Inclusion, and Democracy.” Perspectives on Science, July, 488–520. https://doi.org/10.1162/POSC_a_00251.\nHicks, Daniel J. 2015. “Epistemological Depth in a GM Crops Controversy.” Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences 50 (April): 1–12. https://doi.org/10.1016/j.shpsc.2015.02.002.\n\n\n\n\n\nScience and values\nThis is the most disciplinary or narrowly philosophical topic among my major research interests. My dissertation (Notre Dame 2012) developed Alasdair MacIntyre’s conception of a social practice, informed by feminist philosophy, and applied this conception to the then-current debate over the ideal of value-free science. Since finishing my dissertation, I’ve become known for defending a distinctively Aristotelean version of the “aims approach” to values in science.\nDue to the nature of my employment over the last several years, I’ve been unable to devote much time to narrowly philosophical work. I have been able to give presentations in which I apply and defend my version of the “aims approach” to topics such as academic freedom and “dangerous ideas,” open science, and objectivity. I intend to write up and publish these talks over the next few years.\n\nHicks, Daniel J., and Thomas A. Stapleford. 2016. “The Virtues of Scientific Practice: MacIntyre, Virtue Ethics, and the Historiography of Science.” Isis 107 (3): 449–72. https://doi.org/10.1086/688346.\nHicks, Daniel J. 2014. “A New Direction for Science and Values.” Synthese 191 (14): 3271–95. https://doi.org/10.1007/s11229-014-0447-9.\nHicks, Dan. 2011. “On the Ideal of Autonomous Science.” Philosophy of Science 78 (5): 1235–48. https://doi.org/10.1086/662255."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "philosophy of science, data science, environmental policy",
    "section": "",
    "text": "Misinformation and trustworthiness: Frenemies in the analysis of public scientific controversies\n\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nRevisiting the Nazi Problem\n\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nNew preprint: tmfast fits topic models fast\n\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nAnarchy and Covid-19\n\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nPluralism about problematic situations\n\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nThe Nature endorsement and the value-free ideal\n\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nConstructivist accounts of representation, in science and politics\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nUndue influence of epistemic values\n\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2023\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nThe presence-absence model and epistemic representation\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nTechnocratic legitimacy and the value-free ideal\n\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2023\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nLeftist dynamics for city simulation games\n\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2021\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nTalk: Open science can’t solve the replication crisis\n\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2021\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nTeaching critical thinking in 2020\n\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2020\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nConfiguring Github and Travis-CI for Automated Lab Feedback\n\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2020\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nReplication, reproducibility, and Strengthening Transparency\n\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2020\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nTalk: “When Virtues are Vices: The Weaponization of Scientific Norms”\n\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2020\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nCOVID-19 needs adaptive management\n\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2020\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nWhy I won’t give you a hug\n\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2020\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nPaper: Census Demographics and Chlorpyrifos Use in California’s Central Valley, 2011–15: A Distributional Environmental Justice Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2020\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nA “Toolbox-lite” for the First Day of Philosophy of Science\n\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2020\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nRadical Immanent Critique\n\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2020\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nComment: Inductive Risk, Science, and Values: A Reply to MacGillivray\n\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2020\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nWhite Supremacy and Evolutionary Games\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2019\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nPaper: Network analysis to evaluate the impact of research funding on research community consolidation\n\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2019\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nLuck and the Academic Job Market\n\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2019\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nPaper: Legitimizing Values in Regulatory Science\n\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2019\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nSlides: Explainable Machine Learning: An Integrated Epistemic-Ethical Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2019\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nSlides: Why Baier? Feminism, Trust, and Power\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2018\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nAuto-tagging posts + the nDH metric\n\n\n\n\n\n\n\ndata\n\n\ndeveloping\n\n\nidf\n\n\nlocal\n\n\nmarket\n\n\nmeat\n\n\nrights\n\n\nstudents\n\n\ntf\n\n\nvegetarianism\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2018\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nPoster: Contributions of Women to 20th Century Philosophy of Science\n\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2018\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nDifference-in-differences in R\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2018\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nA care ethics for data science\n\n\n\n\n\n\n\ndata\n\n\nscientist\n\n\nstudents\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2018\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nOld posts\n\n\n\n\n\n\n\nn\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2018\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nProblems with Trolley Problems\n\n\n\n\n\n\n\nreasoning\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2017\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nNo, psychologists haven’t shown that GMO opponents don’t care about evidence\n\n\n\n\n\n\n\ndata\n\n\ngm\n\n\npolicy\n\n\nreasoning\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2016\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nAgainst ‘Science-Based Policy’\n\n\n\n\n\n\n\nhypothesis\n\n\npolicy\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2015\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nIndustry-funded research and publishing independence\n\n\n\n\n\n\n\nfunding\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2015\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nStatistical essentialism\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2015\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nPublic Policy:  What Philosophers of Science can Contribute\n\n\n\n\n\n\n\ndata\n\n\ndeveloping\n\n\npolicy\n\n\nscientist\n\n\nstudents\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2015\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nWhen “Feeding the World” Doesn’t Mean “Feeding the World”\n\n\n\n\n\n\n\nfunding\n\n\ngm\n\n\nlocal\n\n\nmarket\n\n\nmarkets\n\n\nregime\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2014\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nFeminism and Science: Frenemies?\n\n\n\n\n\n\n\nmedicine\n\n\nscientist\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2014\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nRegulatory Science Reform?\n\n\n\n\n\n\n\ndata\n\n\npolicy\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2014\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nHSS Reflection: Practices, Science, and Criticism\n\n\n\n\n\n\n\ndata\n\n\nmotivated\n\n\nreasoning\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2014\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nWork and Hackwork\n\n\n\n\n\n\n\nautomated\n\n\nlove\n\n\nmoocs\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2014\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nVirtue Ethics for Robots\n\n\n\n\n\n\n\nautomated\n\n\ncreativity\n\n\ndata\n\n\nrights\n\n\nrobot\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2014\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nThoughts on FIRST\n\n\n\n\n\n\n\nfunding\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2014\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nIs the Finnish Model Irrelevant to North America?\n\n\n\n\n\n\n\nfactors\n\n\nfinnish\n\n\npolicy\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2014\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nNotes for an Ethics of Community Interventions\n\n\n\n\n\n\n\nlocal\n\n\npunishment\n\n\nrights\n\n\nstudents\n\n\nwrongdoer\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2014\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nIs Motivated Reasoning Bad Reasoning? III\n\n\n\n\n\n\n\ncritic\n\n\nhypothesis\n\n\nmotivated\n\n\npolicy\n\n\nreasoning\n\n\nscientist\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2014\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nIs Motivated Reasoning Bad Reasoning? II\n\n\n\n\n\n\n\nhypothesis\n\n\nmotivated\n\n\nreasoning\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2014\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nIs Motivated Reasoning Bad Reasoning? I\n\n\n\n\n\n\n\ndata\n\n\ngm\n\n\nmotivated\n\n\nreasoning\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2014\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nDiversity and “the” Philosophical Tradition\n\n\n\n\n\n\n\nstudents\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nSexism, Philosophy, and the Reciprocity of Virtue\n\n\n\n\n\n\n\nvirtues\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nThe Climate Debate: Ignorance, and/or Complexity?\n\n\n\n\n\n\n\ndata\n\n\nfactors\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nObjectivity is a Unicorn\n\n\n\n\n\n\n\ncitations\n\n\ngm\n\n\nlocal\n\n\nmaize\n\n\npolicy\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nHow to Use Citations to Create Ignorance\n\n\n\n\n\n\n\ncitation\n\n\ncitations\n\n\ndata\n\n\ngm\n\n\ngmof\n\n\nmaize\n\n\npolicy\n\n\nscientist\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nThe History of Philosophy as a Social Network\n\n\n\n\n\n\n\ndata\n\n\nmath\n\n\nmoocs\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nMOOCs do not Make for Successful Math Classes\n\n\n\n\n\n\n\nautomated\n\n\ndata\n\n\ninstructor\n\n\nmath\n\n\nmoocs\n\n\nstudents\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nThe Adversarial Method: Where’s Moulton?\n\n\n\n\n\n\n\nadversarial\n\n\nbirth\n\n\ndata\n\n\nreasoning\n\n\nrights\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nLocal Food, Global Justice\n\n\n\n\n\n\n\ndeveloping\n\n\nlocal\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nThe Funding Effect and Noxious Markets: Weak Agency\n\n\n\n\n\n\n\ndeveloping\n\n\nfunding\n\n\nmarket\n\n\nmarkets\n\n\nmedicine\n\n\nsatz\n\n\nscientist\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nWhy Local?\n\n\n\n\n\n\n\nlocal\n\n\nvirtues\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nThe Tangled (Food) Web We Weave\n\n\n\n\n\n\n\nbirth\n\n\nmeat\n\n\nstudents\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nThe Funding Effect\n\n\n\n\n\n\n\ndata\n\n\nfunding\n\n\ngm\n\n\nhypothesis\n\n\nlocal\n\n\nmarket\n\n\nmarkets\n\n\nmedicine\n\n\nrights\n\n\nsatz\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nNotes on Predators and Animal Ethics\n\n\n\n\n\n\n\ncompassion\n\n\nmeat\n\n\npredators\n\n\nrights\n\n\nvegetarianism\n\n\nvirtues\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nSatz’ Account of Noxious Markets\n\n\n\n\n\n\n\ncitations\n\n\nmarket\n\n\nmarkets\n\n\nrights\n\n\nsatz\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2013\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nTwo Conceptions of Human Nature\n\n\n\n\n\n\n\nmarket\n\n\nrights\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2012\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nFetishism and Innate Differences\n\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2012\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nThe Agroecological Argument for Eating Meat\n\n\n\n\n\n\n\ndata\n\n\nmeat\n\n\nstudents\n\n\nvegetarianism\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2012\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nFriedersdorf: The Sex-Friendly Case Against Free Birth Control, part II\n\n\n\n\n\n\n\nbirth\n\n\nmedicine\n\n\npreventative\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2012\n\n\nDan Hicks\n\n\n\n\n\n\n  \n\n\n\n\nFriedersdorf: The Sex-Friendly Case Against Free Birth Control, part I\n\n\n\n\n\n\n\nbirth\n\n\npolicy\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2012\n\n\nDan Hicks\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-06-26-city-sim-dynamics.html",
    "href": "posts/2021-06-26-city-sim-dynamics.html",
    "title": "Leftist dynamics for city simulation games",
    "section": "",
    "text": "I’ve been playing Cities: Skylines recently, and that prompted me to go back and re-read Kevin Baker’s Model Metropolis, on the libertarian assumptions baked in to the dynamics of SimCity and its descendants, including C:S.\nCity simulators aren’t static. Successive games have added new dynamics and more complex models of older dynamics. Turning all of this over, I asked myself what dynamics I would like to see in a city simulator, that would (a) mitigate or replace the libertarian assumptions Will Wright took from Forrester, and (b) would enable users/players to explore some of the pressing issues in today’s cities? I can’t claim any expertise whatsoever in urban geography, sociology, or dynamics, so I would really be interested in how folks who do have such expertise would answer these questions. For what it’s worth, here are some of the things I came up with.\nCost of living and land/building/business ownership. Even in rural California we have conversations about homelessness, lack of affordable housing, and the cost of living. Insofar as C:S has a general measure of how well your city is doing, it’s land value. But, in the real world, protecting and promoting land value reflects the interests of owners rather than renters or employees; increasing land value tends to increase the cost of living, gentrification, homelessness, and sprawl. The simulation could also explore uncommon forms of ownership, such as co-operatives and publicly-owned residences or businesses.\nAlternative services, especially for policing and jails. Last summer the revival of the Black Lives Matter movement prompted renewed calls for abolition of policing and jails. In C:S, increasing police presence causes reduced crime which increases land values. There are no downsides to the expansion of police, and no way to explore non-carceral alternatives such as greater mental health and counseling services. Notably, C:S provides a wide range of transportation options, giving players ways to explore cities that emphasize bicycles and public transit rather than cars. It would be great to be able to do the same with policing.\nFactions are the first of two mechanics I’d like to see adopted from Stellaris, a 4X strategy game developed by Paradox (Paradox is the publisher of C:S). Briefly, factions have a happiness score based on the policies that the player has adopted; depending on how (un)happy the faction is, and the share of the population that supports the faction, they can give bonuses or penalties to gameplay. For a city simulation, I can imagine factions based on real-world movements such as NIMBYs, YIMBYs, and social housing advocates; small businesses, unions; abolition and “law and order”; and environmental justice. While not the most sophisticated simulation of democracy, factions would bring a model of politics into a genre that’s basically autocratic on its face. Perhaps, simulating the tension between a mayor and city council, significantly powerful factions would be able to enact or retract policies against the player’s will.\nThe need to hire managers is the other mechanic I’d like to see brought over from Stellaris. In Stellaris, you need to hire various officials to run your empire — scientists to conduct expeditions, governors of regions, military leaders. These officials bring bonuses (and sometimes penalties), and can also be allied with factions. Over time they gain experience, becoming more effective; but can also die. In a city simulation these managers would be heads of major service areas: policing, fire, education, health, sanitation, traffic, parks, etc. I can imagine complex interactions with the faction dynamics; different factions might support rival police chiefs, for example.\nFinally, it would be really interesting to add explicit options to explore alternative dynamics, especially for controversial processes. For example, C:S uses the simple model that increased police causes lower crime causes people to be happy causes property values to increase. But Marxist and critical race analyses of policing suggest a different model of the effects of policing: increasing police protects ownership, which increases property values; this makes property owners and rich (white) people happy, but increases cost of living for poor people (of color) and (also) makes them unhappy directly, while increasing crime (as it’s measured by arrests and other data produced by the police), which can increase support for “law and order” factions. C:S already supports mods that change the population dynamics, and it’s easy to turn these on and off to compare different dynamics."
  },
  {
    "objectID": "posts/2012-05-23-fetishism-and-innate-differences.html",
    "href": "posts/2012-05-23-fetishism-and-innate-differences.html",
    "title": "Fetishism and Innate Differences",
    "section": "",
    "text": "(Going through my folder of notes and paper ideas, I came across the following, which I wrote last June.)\nWith the draft of my dissertation sent off to my committee for notes over the summer, I’m taking advantage of the free time to work through a pile of books-to-read. Over the last week, I’ve been working on G.A. Cohen’s classic of Analytic Marxism, Karl Marx’s Theory of History [KMTH]. For those who aren’t familiar: Cohen was raised Marxist (in the same sense in which one is ‘raised Catholic’, for example) in Montréal, then went off to graduate school in philosophy at Oxford, trained in Analytic methods by the likes of Isaiah Berlin and Gilbert Ryle. The purpose of the book is an Analytic reconstruction of the basic elements of Marxist historiography-economics-sociology-critical theory. It was the first of several brilliant books by a brilliant political philosopher. I highly recommend Cohen’s work, if only because I think he is an exemplar of the Analytic style and method. (And he belies utterly the lingering stereotype of politically-disengaged Analytic philosopher.)\nHowever, there is one notable deficiency in Cohen’s body of work: to my knowledge, he did not write even a single article on either gender or race issues. KMTH is a brilliant exposition of classical Marxism – but, as we learned well during the second wave, classical Marxism is profoundly limited for feminist purposes. So, as I’ve been reading KMTH, I’ve been asking myself two questions: (1) How can this be adapted to give an account of (aspects of) sexism and racism? (2) How can this not be adapted to give such an account? In this post, I want to give only a partial answer to (1).\nIn chapter V, Cohen gives his version of fetishism, as in the ‘commodity fetishism’ discussions of Marx’s Capital. Cohen distinguishes three versions of fetishism in capitalism; I’ll just present the second, capital fetishism, since it’s the easiest to explain here:\n(1) The productivity of physical work takes the form of the productivity of capital.\n(2) Capital is productive.\n(3) It is not autonomously productive.\n(4) It appears to be autonomously productive.\n(5) Capital, and the illusion accompanying it, are not permanent, but peculiar to a determinate form of society, viz., capitalism.\nAs I understand it, capital fetishism corresponds to a certain picture of the relationship between labor (the worker and her or his activities) and the means of production (the raw materials, instruments, etc., used in labor). On this picture, initially, labor and the means of production are separated: the worker is over here and the means of production are over there. The role of the capitalist is to bring these two together: using his capital, the capitalist buys the means of production and offers wages in exchange for labor, and so production occurs. (1) says that the widgets that are literally produced by the worker, using the widget-making machine, are thought to be produced ‘by capital’.  Now, this story is accurate enough as far as it goes, and thus (2): capital really does play this role in the production process, bringing together labor and means of production. This role, of course, can be played (3) only if there are workers and means of production available and they are separated in such a way that capital can bring them together. But, in a capitalist society, we tend to forget this. Consequently, we confuse the fairly minor and facilitating role that capital actually plays with the activity of production itself. Production becomes a sort of black box: you put capital in and get commodities out. Thus (4). Finally, (5) reminds us that this story is only accurate for capitalist societies. In other societies – such as feudal society, historically, and possibly socialist society in the future – capital does not play this role, and is not productive.  \nAll together, then, capital fetishism rationalizes the appropriation of wealth by the capitalist in the form of profit – his investment produced the commodities –  and, indeed, makes it seem ‘natural’ or ‘necessary’ – his investment is required to bring together labor and the means of production – despite the fact that it is utterly ‘artificial’ and ‘contingent’ (in the corresponding modalities) to organize production in this way – labor and the means of production aren’t separated in other systems of production. Thus, if claims (1-5) can be supported, capital fetishism serves as a form of ideological critique: these claims explain how certain forms of society seem to be natural or inevitable when in fact they are utterly artificial and contingent.\nAgain, my question upon reading this was: How can this be adapted to give an account of (aspects of) sexism and racism? Here’s what I came up with:\n(1) Racial/gender differences take the form of racial/gender inequalities in wealth, status, and power [‘inequalities’ for short].\n(2) There are racial/gender inequalities.\n(3) These are not the product of innate racial/gender differences in ability and virtue.\n(4) They appear to be the product of innate racial/gender differences in ability and virtue.\n(5) Racial/gender inequalities are not permanent, but peculiar to a determinate form of society, viz., racism and sexism.\nCertainly there are differences between people of different races and genders. Almost all women are physically capable of pregnancy; almost no men are. People whose ancestors come from different parts of the world tend to be more or less susceptible to certain diseases and tend to have different traditions of family and community organization and religious practice. But, in our society, these differences manifest as inequalities: women and nonwhites tend to have lower status and less wealth and power, compared to men and whites respectively. Our default explanations for these inequalities, almost always, point to innate differences in ability and virtue: Women aren’t in the physical sciences (or economics or philosophy) because of emotions or babies or subtle differences in intelligence distributions. African-Americans are impoverished because of a culture of promiscuity, neglect of children, and dependence on the state.\nOf course, other explanations are offered for racial and gender inequalities. But they must be presented as alternatives to the innate differences that are assumed. And this despite more than a century of research that has systematically demolished every previous explanation in terms of innate differences (not to mention obvious ways of obviating the inequalities even if the differences exist, e.g., robust laws prohibiting common types of discrimination against mothers). Indeed, in light of the long history of debunked explanations, I suggest that a reasonable induction is that all innate differences explanations are at least suspect, if not to be rejected out of hand. We can and should, by default, be looking for other kinds of explanation – and asking how to bring about a society in which these inequalities do not exist anyways.\nIn this light, meritocratic responses to feminist and antiracist charges appear as myths, deep-seated in sexist and racist societies – a fetishism of innate racial and gender differences. Like capital fetishism, they rationalize certain contingent features of our society, making them appear necessary.  \n\nMay 23rd, 2012 10:41am gender/sex"
  },
  {
    "objectID": "posts/2014-03-20-is-the-finnish-model-irrelevant-to-north-america-.html",
    "href": "posts/2014-03-20-is-the-finnish-model-irrelevant-to-north-america-.html",
    "title": "Is the Finnish Model Irrelevant to North America?",
    "section": "",
    "text": "Like many professional educators, I’m a fan of the Finnish education system, especially its emphases on recognizing the autonomy of teachers as highly-trained professionals, an egalitarian distribution of educational resources, and inquiry-based learning. This system not only aligns with my view of the aims and purposes of education, but it’s also made put Finland at or near the top of many international educational comparison lists.\nIn a post yesterday, the conservative writer Rod Dreher discussed some of the things that he likes about the Finnish system. But he’s not so optimistic about applying this model to North America:\n\nI suspect that a truth not too many people are eager to consider is that the Finnish education model works so well because they are teaching Finns. I’m not making a racial/genetic claim here; I’m making a cultural one. Finland is small, ethnically homogeneous and culturally uniform. Places like that tend to have a degree of social capital (e.g., trust, solidarity) that more diverse countries and polities do not have. More importantly, not knowing a thing about Finland, I am pretty sure that there are other qualities of the Finnish character that make a big difference on education policy and success — qualities that prevent the Finnish model from being successfully exported to most countries.\n\nThis actually dovetails with a point in philosophy of science that I’ve been thinking about lately. I’ll make that point first, then come back to Dreher’s objection.\nThe concept is called relevance, and it’s been developed by philosopher of science Nancy Cartwright (follow that link to her recent book with Jeremy Hardie, written for a broad audience). To illustrate relevance, consider a very familiar causal claim: if I turn the key in the ignition, my car will start. This causal claim will be true in many situations. But in some situations it won’t — if the battery has been disconnected, for example, or the starter is dead, or the car is totally out of gas. Cartwright calls these other requirements — a connected and charged battery, working alternator and starter, gas in the tank, and so on — support factors for the causal claim. If these support factors are missing, the causal claim will fail; but when they are present the causal claim will hold.\nAs a prediction, the causal claim is relevant to other situations with the support factors: we can predict with confidence that, given the support factors, if I turn the key in the ignition, the car will start. But it’s not relevant to other situations, where a totally different set of support factors is in play. Suppose we’re talking about an electric car. It has batteries, but no starter, alternator, or gas tank. The batteries need to be full, but the other things are irrelevant. And the electric car has other support factors that need to be working correctly in order for it to start when I turn the key in the ignition.\nBecause of the difference in support factors, we can’t take a complex causal claim (if support factors X, Y, and Z are in place, and I do C, then E will happen) and apply it directly to a very different kind of situation (where the support factors would be T, U, V instead of X, Y, Z). The causal claim is only relevant to situations with the same support factors.\nOkay, back to Dreher. It’s easy to believe that the Finnish model works thanks to a set of support factors. Dreher’s speculating that at least some of these support factors are tied to the fact that “Finland is small, ethnically homogeneous and culturally uniform” and that such countries “have a [high] degree of social capital.” The US and Canada are larger than Finland (Canada about 6x, the US about 55x), and much more ethnically and culturally diverse. If these are differences that make a difference — if they mean that some crucial support factors are absent or different in Canada and the US — then the causal claim “adopting the Finnish model will improve educational outcomes” will be false or irrelevant here, even as it’s true in Finland.\nBut are these differences really important to whether the Finnish model would work here? More precisely, what exactly are the support factors for the Finnish model?\nWe should note that, despite his confidence, Dreher’s concerns are speculative and sketchy. He’s guessing that cultural differences could make a difference, and he hasn’t explained how they could make a difference.\nFinnish education scholar Pasi Sahlberg, an internationally prominent advocate of the Finnish model, has raised some concerns about exporting the model to the US. Like Dreher’s concerns, Sahlberg’s concerns can be understood as concerns about missing support factors. But Sahlberg, unlike Dreher, is more specific, and he discusses several mechanisms that connect these support factors to educational outcomes:\n\nEqual distribution of educational resources\nUniversal access to “childcare, comprehensive health care, and pre-school”\nUniversal access to education “from preschool to university”\n“[A] strong sense of trust in schools and teachers to carry out these responsibilities”\nHigh standards of teacher education, including a research-based master’s degree requirement\nAnd, most relevant to Dreher’s concerns: “education is viewed primarily as a public effort serving a public purpose. As a consequence, education reforms in Finland are judged more in terms of how equitable the system is for different learners.”\n\nLet me conclude with an optimistic reading and a pessimistic reading. Assuming Sahlberg is right, then the Finnish model could work here only if it’s accompanied by a suite of structural changes — equal and universal access to education and health care, a radical overhaul to teacher training, and similar. But those things seem entirely doable. In the US, we’ve recently made a giant step towards universal healthcare, and we already have universal elementary and secondary education. We’ve taken a giant step away from universal university education in the last few decades, but in principle that could be reversed. Likewise, in principle we could overhaul our teacher training. And things look somewhat better in Canada, especially in more consistently egalitarian provinces.\nPessimistically, Sahlberg’s last point suggests a deep cultural difference between Finland, on the one hand, and the US and Canada, on the other. Sahlberg seems to be saying that Finland is coherently egalitarian-communitarian; while Anglophone North America is sharply split between egalitarian-communitarians and hierarchical-individualists. Because of this, Finns can generally agree on the nature and purpose of an educational system, and have designed and implemented a system that effectively realizes that purpose. In Anglophone North America, by contrast, egalitarian-communitarians and hierarchical-individualists will not be able to agree on the nature and purpose of an educational system, and the result is our the incoherent and middling systems.\n\nMarch 20th, 2014 10:13am (philosophy of) science education finnish model"
  },
  {
    "objectID": "posts/2014-03-14-notes-for-an-ethics-of-community-interventions.html",
    "href": "posts/2014-03-14-notes-for-an-ethics-of-community-interventions.html",
    "title": "Notes for an Ethics of Community Interventions",
    "section": "",
    "text": "Last week, in response to the Ludlow affair — the most recent in a string of high-profile cases of sexual harassment/assault in academic philosophy — students at Northwestern planned a walk-out of his class. Brian Leiter, a philosopher at the University of Chicago who runs a highly-read blog on academic philosophy, called this action “vigilante justice” and told an anonymous grad student who defended the action that s/he is “a danger to a university community, to the rule of law, and to the freedom of thought.” He also leveled similarly strong words against Rachel McKinnon, who publicly defended both the action of the Northwestern students and the anonymous grad student. Facebook, Twitter, and various philosophy blogs all erupted with heated “discussion” (that’s a euphemism) of Leiter’s comments and the situation as a whole. Terms like “lynch mob,” “witch hunt,” and “reign of terror” were used.\nLate in January, the progressive magazine The Nation published “Feminism’s Toxic Twitter Wars,” which aired concerns about a culture of “perpetual outrage and hair-trigger offense” in online feminism. It discussed, as a major example, the controversy over the #Femfuture report and the #SolidarityIsForWhiteWomen hashtag. The piece itself prompted a heated “discussion” (again, a euphemism) about whether or not online feminism really is “toxic” and self-destructive or whether this is just the way in which privileged professional white feminists perceive a community in which impoverished and working-class women of color enjoy significant clout.\nFor many, many reasons, I cannot act as a referee in either of these controversies. What I am interested in here is the general social practice of mass criticism — “calling out,” shaming, ostracizing, or ridiculing a (perceived) wrongdoer — especially when it takes place outside of established formal institutions and uses emerging media (blogs, Twitter). One of the issues raised by the two controversies is whether this practice is ethical. Calling the practice “vigilante justice” or “toxic” implies that it is not. But it seems clear to me that in some cases it can be — civil rights marches, union picket lines, and petition campaigns are the same kind of practice, just done without early twenty-first century media.\nFirst we need a neutral term to name this kind of practice. Rachel McKinnon suggested “community intervention,” and I think that’s about right. It seems to cover everything from petition campaigns to civil rights marches to publicly shaming a wrongdoer online. It also does not imply that all instances of this practice are ethical or unethical. My question, then, is: What are the conditions for an ethical community intervention?\nMore precisely: I assume we have some wrongdoer (an individual or collective agent). I assume that the wrongdoer has, in fact, done something harmful, bad, wrong, or vicious, at least by the standards of the community. The wrongdoer might be a member of the community (at least, before the intervention), or might be an outsider. The community is relatively un-organized, though I assume it has a few individual members who have significant de facto influence over some significant parts of the community; I call these individuals community leaders. The intervention might involve denunciations, criticism, shaming, petitions, protests, and so on. Taken together, are these actions ethical, that is, are they helpful or harmful, good or bad, right or wrong, virtuous or vicious?\nI don’t think previous attempts to think about this question have been successful, in large part because of the language that’s been used. “Vigilante justice” and “mob mental” prejudice things from the start. Very general concepts like “academic freedom,” “due process,” and “freedom of speech” seem to me to be too general to be useful. Some specific concepts that are brought in, like “bullying,” have vague or subjective criteria of application (it’s not clear exactly when someone’s a bully), and so might be used prejudicially.\nI think we need to start with an account of what kind of action the intervention is. It seems to be a collective action — one individual cannot carry out a community intervention. It also seems to be goal-oriented, that is, it has some aims or purposes. I suggest that one aim of many community interventions is to punish the wrongdoer. I don’t assume that all community interventions have this aim, or even that every individual involved in a given community intervention would think of it as having this aim. The aim might be direct punishment — the community intervention, itself, punishes the wrongdoer — or it might be indirect punishment — the community intervention gets institutional authorities to punish the wrongdoer.\nAgain, I’m going to assume that the wrongdoer did indeed do the deed in action, that this action was wrong, and that everyone recognizes both of these things. Those assumptions might be false in a particular case. Indeed, in both of the examples above, it’s controversial who the wrongdoers are. So we shouldn’t pass these issues by too lightly when we actually use this framework.\nPolitical philosophers have developed several different theories of punishment, which support several different ways of thinking about whether punishment is being carried out ethically. Christopher Wellman gives us a nice overview of several prominent theories: The purpose of punishment is, variously,\n\nretribution: Justice requires that people get what they deserve, and wrongdoers deserve punishment.\nutilitarian: Punishment brings about the best consequences for the greatest number of people, e.g., through deterrence.\nsecurity: The purpose of punishment is to maintain the security and stability of the social system.\nmoral education: “The purpose of punishment is to morally educate offenders so that they (and others) will know better than to behave criminally.”\nsymbolic: “Punishment is a conventional device for the expression of attitudes of resentment and indignation, and of judgments of disapproval and reprobation.”\nrestitution: To correct the harm that the victim has suffered, either directly (returning stolen property to them) or indirectly (compensating them financially) or symbolically (“publicly confirming that the victim has a moral standing that the criminal was wrong to disrespect”).\nsafety-valve: Punishment is “a safe, institutionally controlled release of destructive animosity and violent ill will.”\n\nThese theories are usually taken to be rivals. For example, someone will defend the utilitarian theory and criticize the symbolic theory. But I want to understand them as complementary. Each theory discusses one aspect of punishment, one thing that many punishments (and institutionalized systems of punishment) are trying to do. Punishments do not necessarily have to try to do all of these (that would probably be inconsistent). Instead, a given act of punishment will be trying to do some of them. By identifying which ones, we can clarify the purposes of the punishment, and then ask whether we’re pursuing those purposes and indeed whether we should be pursuing them at all in this a particular case.\nAll together, here’s the idea: Taking a community intervention as a punishment, we start by asking what kind of punishment it’s supposed to be. Is it meant to get retribution against the wrongdoer? Maybe provide moral education for the community, or perhaps even the broader population? With this purpose in mind, we can use the issues and questions that I’ve listed below to consider whether the community intervention is ethical and, if not, how the community leaders might steer the community back on track.\n\nretribution: Is the punishment proportionate to desert, that is, no less, but also no more, than the wrongdoer deserves? Is the punishment specific to the wrongdoer’s actions? For example, is it focused on the specific vices that the offender exhibited?\nutilitarian: How reliably can we make predictions about the consequences of the punishment? What consequences do we predict? Are they the best possible consequences?\nsecurity: This aim assumes that the social system should be secure and stable. Often community interventions are responses to particular injustices that are symptoms of broader, structural injustices. So, structurally speaking, are there tensions between security and stability? Whose security has been violated by the offense? Whose security, if anyone’s, will be violated by the punishment? In what ways?\nmoral education: What, exactly, is the behavior we’re trying to educate people about? How will this education work? Can we make reasonable predictions about whether it will succeed?\nsymbolic: I take it that many community interventions are aiming at this. What, exactly, is the attitude or judgment to be expressed? Will the members of the community be satisfied by merely expressing attitudes and judgments? What will the wrongdoer and other members of the community do with those expressions?\nrestitution: Who, exactly, has been wronged, and how? How will the community intervention restore the victim? What does the victim think of these efforts?\nsafety-valve: Community interventions, by their nature, have limited institutional self-control. What institutional controls, if any, are in place? What, if anything, is there to prevent individuals from, for example, physically attacking the wrongdoer? How will community leaders respond if this kind of thing happens?\n\nI want to stress, again, that I’m not claiming that any of the community interventions discussed at the top of the post are either unethical or ethical. So I’m not offering answers to any of these questions about those particular interventions. I also don’t claim that the questions I’ve listed are all of the questions that should be asked. And notice that I haven’t told you what the “right” and “wrong” answers will be to these questions.\nIn short, these notes are not determinate judgments or a fixed set of principles, but rather a place to start deliberation.\nAre there other aims for community interventions that I’ve overlooked? Other questions that we should ask relevant to the ones I’ve listed? What do you think?\n\nMarch 14th, 2014 10:01am ethics metaphilosophy"
  },
  {
    "objectID": "posts/2013-02-25-the-funding-effect-and-noxious-markets-weak-agency.html",
    "href": "posts/2013-02-25-the-funding-effect-and-noxious-markets-weak-agency.html",
    "title": "The Funding Effect and Noxious Markets: Weak Agency",
    "section": "",
    "text": "(In this series of posts, I’m applying Debra Satz’ account of noxious markets to a specific aspect of commercialized science, the funding effect. In the first post, I summarized Satz’ account. In part two, I explained the funding effect.)\nWe are now prepared to pose the primary question of this series: In light of the funding effect, is the commodification of scientific research – at least in areas of research where the effect is substantial and well-documented – noxious?\nAn affirmative answer to this question does not imply directly that commodification should be illegal. First, because my use of “commodification” is quite loose; further inquiry would have to investigate precisely which forms of commodification – which kinds of transactions and institutional arrangements – are noxious. Second, because thorough deliberation might conclude that commodification is indeed noxious, yet all things considered better than any alternative institutional arrangement. For example, regulatory capture and interest-group politics might render state funding of research just as noxious. Market reform and regulation, not a simple prohibition of a market in scientific research, might be preferable. But again, I am concerned here with the prior question of whether the commodification of scientific research is noxious.\nPrevious answers to this question generally fall into three categories. First is the favorable argument that commodification directs research funding towards products that are valuable for the public, in the way that markets in general produce Pareto-optimal distributions of goods. Second is the category of appeals to Merton’s “ethos of science,” which argue that the ethos of commercialization is incompatible with the scientific ethos. And third is a family of epistemological arguments that claim in various ways that commodification does or will frustrate the epistemological quality of the research.\nPhilip Mirowski and Robert Van Horn — who take a sui generis approach — call people in the first category “Economic Whigs,” and those in the second “Mertonian Tories” (Mirowski and Van Horn). They level roughly two basic criticisms of these two views. First, both work with highly simplified and uncritical – perhaps even naïve – conceptions of the institutions involved in commodified science. Mertonian Tories uncritically adopt the framework of Merton’s ethos, ignoring the empirical work in Merton’s day and since that has found “counternorms” to this ethos. (For example, Mitroff’s study of Apollo-project moon scientists.) Economic Whigs, for their part, rely on the simplifying assumptions, for example, required for the efficient market theorems. Second, and consequently, any problems with commodification are attributed to individual bad actors (as the epithets suggest, the Economic Whigs see far fewer problems than the Mertonian Tories), neglecting the structural or institutional causes that make these problems pervasive and limit the effectiveness of, for example, conflict of interest reporting requirements.\nExamples of the third approach include papers and books by James Robert Brown, and Thomas McGarity and Wendy Wagner, among others (J.R. Brown, McGarity and Wagner). Brown, for example, has argued recently that epistemologically high-quality randomized controlled studies are so expensive that they cannot be run more than a small number of times; but, given the funding effect, we cannot be confident in the findings of these studies if they are funded by industry; and so they should be publicly funded. However, as we’ve seen, the inference from industry funding to low epistemological quality is not as easy as this argument seems to assume. Furthermore, in light of Mirowski and Van Horn’s institutionalist critiques in the last paragraph, we might question these approaches’ assumptions about what happens when one institution (the state) replaces another (the market), and whether public funding is so much more likely to avoid the problems of commodified research.\nHere I take a fourth approach, bringing Satz’ account of noxious markets to bear. As we saw above, Satz identifies four dimensions of noxious markets. In what follows, I’ll consider to what extent the funding effect – and the mechanisms that lie under it – indicate problems along each of these dimensions. [It turns out that I had plenty to say for each of the four dimensions, so I’m presenting the subsections in separate posts.)\nRecall that Satz often works with a simple model of a transaction, in which exactly one individual (the seller) offers a good or service to exactly one other individual (the buyer) in exchange for money. Following this, I’ll assume that the seller is a scientist, who is selling the services of his or her lab to an individual product manufacturer."
  },
  {
    "objectID": "posts/2013-02-25-the-funding-effect-and-noxious-markets-weak-agency.html#weak-agency",
    "href": "posts/2013-02-25-the-funding-effect-and-noxious-markets-weak-agency.html#weak-agency",
    "title": "The Funding Effect and Noxious Markets: Weak Agency",
    "section": "Weak Agency",
    "text": "Weak Agency\nSatz’ first concern was with weak, limited, or no agency, especially in the forms of asymmetric knowledge between transactors, externalities, and principal-agent problems. The funding effect does not seem to be worrisome in terms of asymmetric knowledge. Certainly the scientist knows more than the manufacturer – this is the reason why the manufacturer wishes to buy the services of the scientist, after all – but this does not seem likely to make the transaction problematic. I suppose there is the possibility of a fake scientist defrauding the manufacturer, but I know of no prominent examples of this.\nPrincipal-agent problems are a distinct possibility when the transaction is arranged by a third party (the agent) on behalf of the scientist herself (the principal). For example, in a private research firm, a marketing department might be responsible for negotiating the research contract (Mirowski and Van Horn 535). The marketing department is effectively acting as an agent on the scientist’s behalf in the negotiation. The negotiations over the funding arrangement between the Plant and Microbial Biology Department [PMB] at UC Berkeley and Novartis Agricultural Discovery Institute [NADI], a subsidiary of the multinational biotechnology conglomerate Novartis, also involved principal-agent negotiations. After the partnership with Novartis was approved by the PMB faculty, the details were worked out by a team of nine people: “three PMB scientists, two senior people from NADI, two senior administrators from [Berkeley’s Office of Technology Licensing], and two attorneys from Novartis” (Busch, et al. 28). These three scientists acted as agents on behalf of all of the other members of the department.\nAre there important externalities involved in these kinds of cases? This, I think, is a tricky question. If the scientist is represented by an agent in the transaction, it’s plausible to think that her interests may not be well represented by the marketing department, but quite well represented by fellow faculty members.\nAnother important third party — influenced by the transaction but not represented in it — are consumers of the manufacturer’s product, who (to simplify again) rely on the scientist’s findings in making their consumption decisions. That is, at the conclusion of her research the scientist produces claims about the safety, technical effectiveness, and so on, of the manufacturer’s product; since the consumer cannot investigate these claims himself, the only source of information he has about the product is the scientist’s testimony. (In more realistic scenarios, this testimony may be mediated by a regulator, who assesses the testimony before approving the sale of the product to the consumer. Or by a journalist, who produces a nontechnical summary of the scientist’s claims. To keep the scenario simple, suppose that the consumer simply receives the scientist’s testimony directly.)\nIn light of the funding effect, we might worry that, as it were, the producer’s interests are overrepresented in the scientist’s testimony, and the consumer’s interests are underrepresented. For example, suppose the scientist claims that the product is safer and more effective than it actually is. Then some consumers would consume the product who, if they had more accurate information, would not. Or, consumers might be willing to pay the manufacturer a higher price for the product than if they (the consumers) had more accurate information. That is, the weak agency of consumers in the transaction between the scientist and the manufacturer could lead to information asymmetry or exploitation in the later transaction between the consumer and the manufacturer. The failure of the downstream market is a product of the failure of the upstream market; and this downstream noxiousness implies that the upstream transaction is also noxious.\nHowever, this worrisome scenario is more complex than it initially seems. The funding effect is a statistical generalization, not a claim about any particular research. That is, industry-funded research in general tends to produce claims that are more favorable to industry. We cannot infer directly from the fact that this particular scientist received industry funding to the conclusion that her claims are more favorable to industry than those of an independently-funded scientist. And even if it is more favorable, this may be because, as we saw in the last post, it has examined a more thoroughly vetted or refined product or because of low (or high) but still reasonable standards for making causal claims.\nI argue that this complexity, far from excusing worries about the weak agency of consumers, is the core of the problem. In order to make a well-informed decision, consumers must be able to evaluate the scientist’s testimony by, for instance, relating the standards the scientist used – say, the threshold for statistical significance used – to their own. Thus, if and insofar as the scientist is not explicit about how and why they adopted these standards, the consumer cannot appropriately evaluate the scientist’s testimony. Even worse, he (the consumer) may believe that he is well-informed about the product while actually being quite ignorant. For example, consider a single claim C that the product increases the risk of cancer. Suppose that the industry-funded scientist adopts a high (but reasonable) standard of statistical significance for claim C, and on this basis concludes that the claim is false, that is, it doesn’t cause cancer. Suppose in addition that the consumer adopts (or would adopt) a low (but reasonable) standard for claim C. By this low standard, the evidence adequately supports this claim. The consumer does not assess the evidence directly; instead, he relies strictly on the scientist’s testimony that not-C. Hence, if and insofar as the consumer is not aware that the scientist is working with a higher standard and indeed believes that the scientist is working with his lower standard, the consumer mistakenly believes that C is false. (In the classic justified-true-belief analysis of knowledge, the consumer’s mistake is a matter of the justification rather than the truth of his belief that not-C.) Consequently, in his negotiation with the manufacturer, the consumer cannot accurately (relative to his preferences) evaluate the risk of developing cancer, is more likely to settle for a higher price, or will be harmed in other ways. If the consumer could make the easy inference from industry funding to inaccuracy, it would be much easier for him to evaluate the scientist’s testimony. It is the combination of (a) the consumer’s weak agency in the transaction between the scientist and the manufacturer and (b) the complexities of the funding effect that produces the consumer’s weak agency.\nSimilar arguments lead to similar assessments for the other mechanisms underlying the funding effect. For example, suppose the consumer is trying to decide whether to buy organic or conventional (pesticide-sprayed) food. The scientist claims that organic food is not significantly healthier than conventional food, but using a metric that looks only at select nutritional components and does not look at pesticides. (This is a simplification of some events that played out in Fall 2012. Crystal Smith-Spangler and her coauthors published a study of the nutritional content of organic produce in the Annals of Internal Medicine. A few days later, New York Times opinion columnist Roger Cohen characterized their findings this way: “I cheered this week when Stanford University concluded, after examining four decades of research, that fruits and vegetables labeled organic are, on average, no more nutritious than their cheaper conventional counterparts.” Responding to Cohen and others a few weeks later, Mark Bittman, another New York Times opinion columnist, wrote that “It was the equivalent of comparing milk and Elmer’s glue on the basis of whiteness. It did, in short, miss the point …. In fact, the Stanford study … does say that ‘consumption of organic foods may reduce exposure to pesticide residues and antibiotic-resistant bacteria.’” Both Cohen and Bittman accurately reported the findings of Smith-Spangler et al., but using different metrics.) The mismatch between the metrics and standards used by the scientist and assumed by the consumer effectively, combined with the inability of the consumer to recognize this mismatch, weaken her agency in her transaction with the producer.\nIt might be thought that this problem can be resolved simply by improving transparency: making it clearer when scientists are funded by industry through conflict-of-interest disclosures, requiring more explicit statements of standards and metrics used, the statistical power of the experiments and analysis, and so on. But this response assumes that consumers are in a position to evaluate these disclosures. Specifically, it assumes that consumers understand such notions as statistical power and have at least somewhat well-developed views on, for example, what the threshold of statistical significance should be for claims that a product causes cancer. That is, it assumes that a consumer can reason more or less literally along the following lines:\n1. The threshold of significance for claim C should be at p0, or less demanding.\n2. This scientist uses threshold p1 &lt; p0.\n3. A lower p-threshold is a more demanding standard of significance. 4. Hence, I should not rely on the testimony of this scientist.\nBut many – if not almost all – consumers are effectively statistically innumerate, that is, lack the ability to understand, much less evaluate, claims such as premises (1-3). Similarly, in the case of organic food, we might simply require making (more) explicit that nutritional content, and only nutritional content, is being used as a proxy for health benefits. But again, this assumes that consumers recognize the limitations of this proxy and understanding the distinction between nutritional content and the presence of pesticides. Making commercialized science more transparent would seem to be necessary but far from sufficient for addressing this structural problem. Perhaps surprisingly, the noxiousness of commercialized science depends in part on the strength of our education system."
  },
  {
    "objectID": "posts/2013-02-25-the-funding-effect-and-noxious-markets-weak-agency.html#references",
    "href": "posts/2013-02-25-the-funding-effect-and-noxious-markets-weak-agency.html#references",
    "title": "The Funding Effect and Noxious Markets: Weak Agency",
    "section": "References",
    "text": "References\n\nBittman, Mark. “That Flawed Stanford Study.” New York Times, http://opinionator.blogs.nytimes.com/2012/10/02/that-flawed-stanford-study/.\nBrown, James Robert. “One Shot Science,” in Radder, ed., The Commodification of Academic Research, Pittsburgh UP.\nBusch, Lawrence, et al. External Review of the Collaborative Research Agreement between Novartis Agricultural Discovery Institute, Inc. and The Regents of the University of California. East Lansing, MI: Institute for Food and Agricultural Standards, Michigan State University.\nCohen, Roger. “The Organic Fable,” New York Times, http://www.nytimes.com/2012/09/07/opinion/roger-cohen-the-organic-fable.html.\nMcGarity, Thomas and Wendy Wagner. Bending Science, Harvard UP.\nMirowski, Philip, and Robert Van Horn. “The Contract Research Organization and the Commercialization of Scientific Research.” Social Studies of Science 35, no. 4: 503–548. doi:10.1177/0306312705052103.\nMitroff, Ian I. “Norms and Counter-Norms in a Select Group of the Apollo Moon Scientists: a Case Study of the Ambivalence of Scientists.” American Sociological Review 39, no. 4: 579–595.\nSmith-Spangler, Crystal, et al. “Are Organic Foods Safer or Healthier Than Conventional Alternatives? A Systematic Review,” Annals of Internal Medicine 157: 348-66.\n\n\nFebruary 25th, 2013 10:27am food (philosophy of) science economics"
  },
  {
    "objectID": "posts/2014-11-21-regulatory-science-reform-.html",
    "href": "posts/2014-11-21-regulatory-science-reform-.html",
    "title": "Regulatory Science Reform?",
    "section": "",
    "text": "Over the last few days, several people have brought to my attention this story about two regulatory science bills passed earlier this week in the US House. Since the linked story doesn’t do a very good job of explaining why the two bills are controversial — indeed, it doesn’t even clearly indicate that there are actually two bills involved — I thought I’d take some time to unpack things.\n\nHR 1422\nThe controversial bits of “the EPA Science Advisory Board Reform Act of 2014” seem to be the following:\n\n\nparagraph 2(c): “persons with substantial and relevant expertise are not excluded from the Board due to affiliation with or representation of entities that may have a potential interest in the Board’s advisory activities, so long as that interest is fully disclosed to the Administrator and the public and appointment to the Board complies with section 208 of title 18, United States Code,” and\nparagraph 2(E): “Board members may not participate in advisory activities that directly or indirectly involve review or evaluation of their own work.”\n\n2(c) seems to allow scientists affiliated with — or even employed by — the fossil fuels industry, say, to sit on EPA advisory boards. While I agree that it’s troubling, the widely-accepted principle of state neutrality or “neutralism” actually endorses this approach. While there’s disagreement about the details of this principle, they all seem to support something like paragraph 2(c). According to the principle, since the EPA is a state body, it should remain neutral with respect to disagreements between environmentalist and industry over what’s good and important. One simple, practical way to try and do this is to solicit scientific advice from the whole range of perspectives, including from industry.\nYou might argue that EPA advisory boards are not actually a state body, since they don’t actually make regulatory decisions. Rather, they’re just responsible for offering scientific advice to the regulators, and so the principle of state neutrality doesn’t apply to them.\nHowever, at this point the defender of 2(c) can argue that the science advice process, as an instance of the broader scientific process, should be objective and unbiased, at least in the sense of surviving critical scrutiny from any (relevant) perspective — including that of industry.[1] All together, it seems hard to criticize 2(c) without abandoning the kind of neutrality that requires us to treat environmentalism and industry as just a pair of opposed “interest groups,” with an equal right to influence the science advisory process.[2]\n2(E) seems to be what commentators have in mind when they complain that “experts would be forbidden from sharing their expertise in their own research.” It seems to me that this depends on how “review or evaluation” is read in 2(E). Suppose we can distinguish vetting — making a decision about whether or not the advisory board should take some body of research into account — from incorporating — actually taking the vetted research into account. Does “review or evaluation” apply to just vetting, or both vetting and incorporating? If it applies to just vetting, then 2(E) doesn’t seem so bad: it would just seem to require that expert X recuse herself temporarily while the rest of the board makes a decision about whether to take expert X’s research into account. But, if “review or evaluation” applies to incorporating as well, then it seems that expert X’s research could be taken into account only if expert X weren’t on the advisory board. That’s a little more troubling. 2(E), by itself, doesn’t seem to settle this point, and I don’t know whether other documents clarify the meaning of “review or evaluation.” (Note that 2(E) seems to apply equally to both academic and industry researchers.)\n\n\nHR 4012\nThe “Secret Science Reform Act of 2014” is quite pernicious, but in a much more subtle way than either of the controversial points in HR 1422. This bill requires that “scientific and technical information relied on to support” regulation must be “publicly available online in a manner that is sufficient for independent analysis and substantial reproduction of research results.”\nThis seems perfectly acceptable, and indeed in line with the scientific ideal of replicability and the democratic ideal of transparency: regulatory decisions should be based on “good science” and “good reasons,” which requires among other things that anyone should be able to double-check the data analysis and interpretation. That requires that the underlying data be made publicly available.\nHowever, Andrew Rosenberg argues that privacy considerations mean that the underlying data for some regulatory decisions cannot be made publicly available. It would be unethical to release medical data that could be used to identify individual patients, for example. Rosenberg doesn’t note this, but in addition research data are often private property. They might be the intellectual property of the company that gathered or aggregated the data (as with public survey data, or climate data). In other cases, in order to get the materials needed for the research, researchers might have had to sign materials transfers agreements (or other contracts) with clauses that restricted their ability to public the resulting data. You might (like me) think this kind of private property in data is ethically illegitimate, but this is still the situation that the EPA has to work with right now.\nYou might also try to argue that the general public should defer to scientific experts, and consequently that there’s no reason to make the data publicly available. Rosenberg seems to say something like this: “As many politicians have taken pains to point out, they are not scientists, so they should listen to scientific advice instead of making spurious demands for unanalyzed data.”\nI have to question this argument. There are plenty of cases in the history of science in which non-scientists and marginalized scientists have correctly criticized the scientific majority. In democratic terms, it’s important that scientists be held accountable to the general public.\nThat said, HR 4012 does very little to make scientists accountable to the general public. The overwhelming majority of members of the public lack the technical skills (and, in some cases, the computational resources) necessary to double-check scientists’ data analyses, much less distinguish reasonable and unreasonable interpretations of the results. It doesn’t matter whether the data are published or unpublished, if you don’t have the training necessary to work with them.\nInstead, HR 4012 will be much more effective at obstructing the regulatory process and generating confusion and ignorance. This is arguably the case with its predecessor, the Shelby Amendment or Data Access Act.\n\n\n\n\nPhilosophers of science should recognize the reference to the views of Helen Longino and, in his most recent work, Philip Kitcher. While there are significant disagreements between Longino and Kitcher, as of about 2011 both reject “value-free” conceptions of objectivity, and instead require that scientific findings must pass actual or hypothetical (respectively) critical scrutiny in order to be acceptable.  ↩︎\n\n\nOne of the main concerns in my research — as both a philosopher of science and political philosopher — is to articulate views of science and politics that don’t require this kind of neutralism. I think that there are good reasons to think that protecting the environment and human health are more important than protecting the profits of industry and promoting economic growth. By these lights, paragraph 2(c) is much more troubling.  ↩︎\n\n\n\n\nNovember 21st, 2014 2:12pm (philosophy of) science science and values science for policy"
  },
  {
    "objectID": "posts/2023-04-12-anarchy-covid19.html",
    "href": "posts/2023-04-12-anarchy-covid19.html",
    "title": "Anarchy and Covid-19",
    "section": "",
    "text": "Note\n\n\n\nI started working on this two or three weeks ago, got about 1500 words in, and then had to shelve it to deal with a minor deluge of grading and some other things. I thought I would be able to pick it back up, but I’ve lost the thread. In the “writer’s notebook” spirit of this project, I’m just going to post it.\nIn our Facebook discussion of my last post, Matt Brown referred to me a more recent paper of his on Feyerabend (Brown 2021). I really like this paper, and want to focus on it instead of the older one.\nBrown discusses Feyerabend’s critique of expertise from “How to Defend Society against Science” (Feyerabend 1975) and Science in a Free Society [Feyerabend (2017); originally published 1978 . After tying Feyerabend’s critique to the current literature on science, values, and policy, and noting that Feyerabend had some overly simplistic political philosophy in the ’70s (which don’t really impact the argument Brown is interested in), Brown summarizes the critique of expertise as a sequence of “four increasingly radical claims about science and its place in society that, together, point toward a decentering of experts in society and a rejection of expert authority” (195).\nIn this post, I want to read these four claims together with Lee et al. (2021), a fantastic study of the data analysis practices of Covid-19 skeptics. If you haven’t read this paper before, I can’t recommend it enough. Lee et al. (2021) challenge the widespread (among natural scientists and public health official) deficit model of public scientific controversies, which blames controversies on public ignorance. Examining social media communications from 2020, Lee et al. (2021) show that “anti-mask groups on Twitter often create polished counter-visualizations that would not be out of place in scientific papers, health department reports, and publications like the Financial Times” and that “these groups leverage the language of scientific rigor—being critical about data sources, explicitly stating analytical limitations of specific models, and more—in order to support ending public health restrictions despite the consensus of the scientific establishment” (Lee et al. 2021, 1–2). I think this is a useful test case for distinguishing my own views from those of both Feyerabend and Brown."
  },
  {
    "objectID": "posts/2023-04-12-anarchy-covid19.html#footnotes",
    "href": "posts/2023-04-12-anarchy-covid19.html#footnotes",
    "title": "Anarchy and Covid-19",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe first is based on Mill’s argument in chapter 2 of On Liberty. My thoughts about this argument are complex and not what I’m interested in here.↩︎\nBrown uses “citizen” 45 times in the paper, compared to 28 uses of “public.” I’m guessing this is following Feyerabend. But because of my location in California’s Central Valley, with its large population of migrant and often undocumented farmworkers, I prefer the broader term.↩︎"
  },
  {
    "objectID": "posts/2014-05-06-thoughts-on-first.html",
    "href": "posts/2014-05-06-thoughts-on-first.html",
    "title": "Thoughts on FIRST",
    "section": "",
    "text": "The Frontiers in Innovation, Research, Science, and Technology (FIRST) Act is the current biannual funding authorization bill for the NSF, one of the major science funding agencies in the US. (If you’re already familiar with the bill, feel free to skip down a bit.) The current draft of the bill has been strongly condemned by some prominent representatives of the scientific community: John Holdren, President Obama’s science advisor, criticized the bill last week, as did the National Science Board, the body that oversees the NSF. Writers at the Boston Globe and the Huffington Post have criticized the draft bill; the latter uses the (empirically flawed) “Republican war on science” framing.\nYou can read an overview of the bill here, courtesy of the American Institute of Physics, or read the March 10 draft of the bill.\nMuch of the controversy surrounds two elements of the draft bill. First, it reduces funding for certain areas of scientific research, namely, geosciences (a couple of percentage points) and social science (dramatically). Second, Section 105 and 106 of the draft bill (pages 13-16) lay out new requirements for “accountability and transparency” in the grant award process. Specifically, it requires that each grant award must include a written justification for the award, which must explain\n\nwhy the research grant or cooperative agreement … is in the national interest, as indicated by the potential to achieve\n\nincreased economic competitiveness in the United States;\nadvancement of the health and welfare of the American public;\ndevelopment of a STEM workforce and increased public scientific literacy in the United States;\nincreased partnerships between academia and industry in the United States;\nsupport for the national defense of the United States; or\npromotion of the progress of science in the United States.\n\n\nThis requirement is similar to the Coburn Amendment, some proposed language in last year’s omnibus spending bill that would have restricted NSF’s political science funding to “national security or economic development.”\n(The substance of the post starts here.)\nThese accountability requirements have been widely criticized as inappropriate “political meddling” in scientific research. However, that charge seem to assume a version of the old idea that science is, and should be, “value free.” Specifically, it seems to assume that the scientific community should be autonomous, free to pursue whatever ends it sees fit without any sort of accountability or positive obligation to the rest of society.\nWhile other versions of the ideal of value free science are controversial, this notion of autonomy is not. I am not aware of any contemporary philosopher of science or science studies scholar who believes that science should be autonomous in this sense. To be sure, many scholars think that it’s extremely important to engage in “pure,” “basic,” or “exploratory” research, that is, research that doesn’t have any immediate practical applications. But we don’t need to think that science must be totally free to do whatever it likes to think that exploratory research is valuable and worth supporting.\nSo I’m not worried about the proposed accountability requirements because they would violate the autonomy of science. But I’m still very much concerned by the proposal, for two reasons.\nFirst, while it might seem quite broad, the proposal understands “the national interest” in narrow, neoliberal terms. The NSF doesn’t fund much health- or military-related research; these are funded primarily by the NIH and Department of Defense. So, for most proposals, items 2 and 5 are irrelevant. That leaves only two non-economic justifications: “increased public scientific literacy” — which, I expect, will only apply to educational initiatives — and the so-vague-it’s-basically-meaningless “promotion of the progress of science.”\nThat means, for example, that research on climate change would probably have to be justified in terms of “promotion of the progress of science.”\nSecond, while the proposal is being defended in terms of “accountability”, it only gives Congress more fine-grained power over the NSF; it does nothing to make the NSF’s funding process more accountable to ordinary citizens. More broadly, it does nothing to make the NSF’s process more democratic.\nIn his book Science in Democracy, Mark Brown lays out five “elements of democratic representation”: authorization, accountability, participation, deliberation, and resemblance. Brown argues that accountability is important in part because, in democratic politics, “citizens expect one another to justify their political actions and opinions with reasons.” As I read him, this requires a dialogue between — in the case of the NSF — leaders of the funding body and ordinary citizens. It’s not good enough for the NSF to simply announce its reasons; citizens must find these reasons to be reasonable and acceptable. The FIRST act proposal does nothing to promote this.\nFurthermore, the proposal does nothing to promote participation or resemblance. It does not provide a means for ordinary citizens to participate in the process of deciding what research to fund. And it does nothing to ensure that NSF peer review committees resemble the citizenry of the US as a whole, either statistically (e.g., requiring that 50% of reviewers are women and 12% are African-American) or in terms of identity (i.e., requiring each committee or the agency as a whole to have representatives for women, African-Americans, and other groups who have historically been excluded from the scientific community).\nSo the accountability proposal is disingenuous, since it does nothing that will actually improve democratic representation at the NSF. This is especially frustrating since — in terms of accountability, participation, and resemblance — the NSF could and should be significantly more democratic.\n\nMay 6th, 2014 6:13pm science and values (philosophy of) science"
  },
  {
    "objectID": "posts/2019-12-06-White-supremacy-and-evolutionary-games.html",
    "href": "posts/2019-12-06-White-supremacy-and-evolutionary-games.html",
    "title": "White Supremacy and Evolutionary Games",
    "section": "",
    "text": "This morning I’m productinating (productively procrastinating) by reading Ann Cudd’s review of Cailin O’Connor’s The Origins of Unfairness. I started to read O’Connor’s book a couple of months ago, but game theory is not my thing, so I think I only made it as far as the introduction. Both the introduction and Cudd’s review frame O’Connor’s project as primarily trying to explain the origins of patriarchy — understood as a systematically unfair gendered division of labor and goods — using evolutionary game theory. But O’Connor has also taken this approach to explain the origins of white supremacy. Her work in this area includes a preprint of a paper with Liam Kofi Bright and Justin Brunner (which I have read), several forthcoming papers cited in that paper (which I haven’t read), and apparently some of the later chapters of O’Connor’s book (again, which I haven’t read).\nIn both the paper and (based on Cuddy’s summary) the book, white supremacy is operationalized as a systematically unfair division of labor and goods between two groups, one in the majority (i.e., more than 50% of the individuals) and the other in the minority. The models show how, under certain conditions, minority individuals will come to adopt a “non-aggressive strategy,” requesting less than their fair share in a division of goods when interacting with majority individuals. Here’s Cudd’s summary of the dynamics:\n\nWhen there is a minority-majority interaction, if the majority risks playing the aggressive strategy some of the time, the minority type will more quickly learn to play the non-aggressive strategy, and this will result in an unequal split becoming the stable outcome of minority-majority interactions. This is because minorities meet more majorities than they meet each other, and so they quickly learn that the other type are more likely to play aggressive than same type and thus minorities more quickly learn that they must play the less aggressive strategy when they meet the other type.\nThus, the fact of being a minority interacting with a majority social category itself becomes a cause of inequality.\n\nCall this process the minority disadvantage dynamics. With respect to white supremacy, I take it that the overall thesis that O’Connor and her collaborators want to defend is that the minority disadvantage dynamics explain the development of white supremacy. Cudd finds their analysis compelling: “My own view is that the models hit the mark and provide a powerful statement about how unfairness between genders and races is likely to arise in a wide variety of actual conditions.”\nIn the last two paragraphs of the review, Cudd considers an objection, namely, that O’Connor’s models neglect the ways in which “historical acts of violence, terror, and greed have played critical roles in the oppressive structures we see today in any given society.” Cudd responds that “these forces simply reinforce the explanations [O’Connor] has given, but do not nullify them” and that O’Connor “has provided is a set of models to frame the work of empirical social scientists, historians, ethicists, and social philosophers who can put the flesh on the system.” Using new mechanist terminology, we might say that O’Connor et al. have provided a “mechanism schema,” “a truncated abstract description of a mechanism that can be filled with descriptions of known component parts and activities” (MDC 15) to produce an empirically-informed, conceptually-well-supported explanation for actually existing systematic injustice. Specifically, by mapping the entities and activities involved in the minority disadvantage dynamics to entities and activities in the actual history of gendered or raced divisions of labor, we can produce such explanations of patriarchy or white supremacy.\nAs I understand it, our knowledge of the actual origins and development of gendered divisions of labor is extremely limited. Archaeology is hard, especially when we’re talking about processes that were unlikely to leave unambiguous material traces (what was the gender distribution of the people who used that bowl?) that have survived ten thousand years or more. The diversity of gender systems across thousands of human cultures also complicates things a bit: kinds of work done by men in one culture might have been done by women in another culture, or not been gendered at all, which makes it difficult to generalize; and of course many human cultures don’t have binary gender systems.\nBy contrast, we know a lot about the origins of white supremacy. Briefly, Europeans abducted thousands of Africans, transported them thousands of miles over the Atlantic Ocean, tortured them, and kept them culturally isolated from nearby populations; all in order to force them to work on land that had been appropriated from indigenous people using genocidal methods. African slaves and their descendants were not particularly satisfied with this arrangement, but a system of horrific violence and the ideology of biological race were effective at creating what were, up to that point, some of the wealthiest societies in human history.\nCan the minority disadvantage dynamics be mapped on to this history of white supremacy? The entities would seem to be straightforward: the white settler-slavers are the agents who get the unfair advantage, while the black slaves are the agents who get the unfair disadvantage. Beyond this, however, things get more difficult.\nOne issue is that evolutionary game theory idealizes interactions between agents as cases of free choice. In each play of the game, all of the agents involved are free to choose among the available options. While Hobbes thought that threats of violence — and even instantiated violence — were entirely compatible with a free choice, most of us today would call this coercion. Slaves were not free to decide whether or not they would work for their owner; that’s why we call them “slaves.”\nHowever, we might say that slaves were free to decide whether to comply or rebel, either on particular occasions or with the slave system as a whole. The violence they would face for not complying — and the value of freedom if their rebellion were successful — could be bundled into the payoff matrix. “Free choice” might not accurately capture the phenomenology or injustice-as-domination of slavery, but those are not the aspects of white supremacy that evolutionary game theory is trying to explain.\nIf we’re making the decision to comply or rebel the primary choice faced by slaves, then another issue arises. The chances of successful rebellion likely depend on the decisions made by other slaves, including the decision of whether to join the rebellion but also the decision of whether to let the slavers know about the impending rebellion. However, in principle this could be accounted for by moving to a more complex multi-player game with multiple different strategies available to each type of player. I’m not sure how robust evolutionary game theoretic results tend to be when they’re generalized in this way, but for the purposes of this blog post let’s suppose that the basic picture of the minority disadvantage dynamics still holds up.\nThere’s still another issue, though, which I think is a much more serious problem for the empirical project suggested by Cudd on behalf of O’Connor. In the slave societies of the US South and the Caribbean, slaves were the majority group, not the minority. White people were the minority in South Carolina from about 1708 until about 1910. In 1788, Haiti (then Saint-Domingue) had about 25,000 Europeans, 22,000 “free coloreds,” and 700,000 slaves. The white settlers were always the minority in what we might call the “non-genocidal” settler societies, such as in Africa and Asia, where the indigenous population was not destroyed or removed. (You might object that the case of Haiti is consistent with O’Connor et al.’s model, because it shows that a majority can overthrow an oppressive minority. But presumably slaves were in an overwhelming majority for several decades before the revolution began in 1791.)\nThe minority disadvantage dynamics are driven entirely by how frequently individuals encounter members of their same and different groups. Minorities have a disadvantage because they less frequently encounter individuals who will give them a fair deal (i.e., members of their same group), and so the majority gradually accrues the lion’s share of the wealth. But in actual white supremacist slave societies, it was the slaver minority who captured the wealth, not the enslaved majority. The minority disadvantage dynamics cannot explain the minority advantage phenomenon observed in most actual white supremacist societies. It seems that we need to appeal to power relations, not population shares.\nO’Connor, Bright, and Bruner do develop models that incorporate bargaining power, as an assumption that “when members of social groups bargain, and when bargaining breaks down, the fall back position for one group tends to be better than for the other” (7). They use this to model intersectional scenarios which have majority-minority dynamics along one axis of oppression (e.g., a black racial minority) and similar populations but asymmetric bargaining power along another axis of oppression (e.g., women have less wealth than men) (§4). They show that these dynamics disadvantage the minority group, the less powerful group, and especially disadvantage the intersectional less power-minority group (i.e., black women). The more power-majority group (i.e., white men) enjoy the largest advantage.\nHowever, in this model, the power dynamics exacerbate the inequality produced by the minority disadvantage dynamics; they do not negate or reserve it. The majority still has an advantage. If we swap the black-white share of the population (which is equivalent to swapping the black and white labels attached to the model), then black men would enjoy the largest advantage. And so this way of combining power and minority disadvantage can’t explain the minority advantage phenomenon.\nAn alternative approach could have a single axis of oppression, with both minority-majority and power dynamics operating along this single axis. Namely, one group would be larger than the other, but this group would also have significantly less bargaining power. For some parameter values, this kind of model would produce minority advantage; namely, if the bargaining power asymmetry were strong enough, it would overcome the minority disadvantage dynamics. But here the minority disadvantage dynamics do not explain the minority advantage phenomenon. Rather, this model shows how bargaining power can produce minority advantage despite the minority disadvantage dynamics.\nIn short, I don’t see how the minority disadvantage dynamics can be mapped on the actual historical development of white supremacy in slave and settler societies. So then I further don’t see how it can be used to explain the origins of white supremacy.\nFor all that I’ve argued here, O’Connor’s overall project may provide useful mechanism schemata for explaining the origins of patriarchy. This is O’Connor’s primary aim in The Origins of Equality, and so perhaps one could agree with my critique here and still accept most of Cudd’s conclusion that “the models hit the mark.” And, in the same way, alternative evolutionary game theoretic models — ones emphasizing power asymmetries — might provide useful mechanism schemata for explaining the origins of white supremacy. But I think the minority disadvantage model, highlighted by Cudd in her review and at the center of O’Connor’s collaborative work on white supremacy, misses the mark."
  },
  {
    "objectID": "posts/2015-07-09-statistical-essentialism.html",
    "href": "posts/2015-07-09-statistical-essentialism.html",
    "title": "Statistical essentialism",
    "section": "",
    "text": "Somehow this piece on the character traits of chronically late people showed up several times on my Facebook feed this morning. The sweeping generalizations in the quotations reminded me that I’ve been meaning to write a post on a common fallacy that I like to call statistical essentialism. In this fallacy, we erroneously move from claims about statistical patterns in a group as a whole to claims about “all” or “typical” members of a group.\nStatistical essentialism is especially common and pernicious in the context of research on sex, gender, and race. Consider research findings that African-Americans have, on average, lower scores on IQ tests than Caucasian-Americans. Strictly speaking, this research finds a statistical pattern concerning the average: the mean of scores for African-Americans is lower than the mean of scores for Caucasian-Americans. This is not a claim about any particular individuals, and indeed by itself it doesn’t imply anything about any particular individual. It doesn’t even imply that, given an arbitrary African-American and an arbitrary Caucasian-American, the Caucasian-American would probably have a higher IQ score than the African-American.\nTo understand why statistical essentialism is fallacious, consider the following computer simulation experiment. Suppose we have two groups, A and B, and that we measure 200 members of each group using some test or measurement. (To make things easier to read, I’ll call the result of the measurement a “score,” but it could also be something like height or weight.) Suppose, in addition, that in the whole population of people in group A, their mean score is 0. And for the whole population of people in group B, their mean score is 0.5. Finally, assume that the distribution of scores for both groups is Gaussian (sometimes misleadingly called a “normal” distribution, or even more misleadingly a “bell curve”), with a standard variation of 1.\nHere is what the resulting data might look like:  \nIn this plot, the points are each of the 400 scores, with different columns, colors, and shapes for the two groups. The dashed horizontal lines show the population mean scores (0 and 0.5), and the shorter horizontal lines show the upper and lower bounds of a 95% confidence interval for our data (they’re fairly narrow, about 0.25 wide). (There’s also a slightly larger point showing the observed mean. In this case, the mean is close to the population mean for both groups.) Each column has 200 points, centered around the values of 0 and 0.5, but spread out quite far up and down the plot. The points have been jittered — spread out a bit within their columns, to the left and right — in order to make them easier to see.\nThis is a statistically powerful study, and it easily detects a statistically significant difference between groups A and B. (In fact, if we re-ran this study, more than 99% of the time we would detect the difference between the groups as statistically significant.)\nBut the fact that the mean for group A is 0 doesn’t mean that every individual score is right at 0: the scores range between about -3 and +3. Here we’ve included confidence intervals, but they also doesn’t represent where “many” or the “typical” individuals are.1 These statistics tell us that on the whole, the scores for group A are clustered around 0, but that’s a pattern in the group, not a prediction for an individual.\nSo the mean — even in a fairly large, statistically powerful study — doesn’t tell us what the score will be for a “typical” individual. In the same way, the fact that there’s a statistically significant difference between groups A and B doesn’t tell us that “typically” members of group B have higher scores than members of group A. In fact, about 35% of the members of group A have higher scores than the group B mean; and about 27.5% of the members of group B have lower scores than the group A mean. And when we pair up one member of A and one member of B from our sample, about 35% of the time the A individual will have a higher score than the B individual.\nI call this fallacy “statistical essentialism” because it often seems to involve some thinking that the statistical pattern points to an essence or ingrained nature. In terms of our example, “the mean score in group A is 0” is reinterpreted as “the essence of group A-ness is to have a score of 0.” With two groups, “the mean of group B is statistically significantly higher than the mean of group A” is replaced with “Bs have a higher score than As,” meaning “the B-essence has a higher score than the A-essence.”\nOne way to resist statistical essentialism is to think in focus on distributions and variation, rather than single numbers. In the example, the mean values for the two groups are statistically significantly different. But the distributions — the full range of scores — have considerable overlap. Just knowing the mean isn’t enough for us to reconstruct the distribution that produced our sample. We also need to know the standard deviation, or some other measure of variation.\n\n\n\n\nReaders who know some statistics will already know this: the confidence interval is better understood as a measure of how precise our estimate of the mean is (though even this is a little sketchy), and not at all an estimate of the range for the “typical” members of the group. ↩︎\n\n\n\n\nJuly 9th, 2015 2:02pm statistics logical fallacies"
  },
  {
    "objectID": "posts/2014-11-14-hss-reflection-practices-science-and-criticism.html",
    "href": "posts/2014-11-14-hss-reflection-practices-science-and-criticism.html",
    "title": "HSS Reflection: Practices, Science, and Criticism",
    "section": "",
    "text": "This is the second of my posts this week about the 2014 HSS/PSA conference. Here, I’d like to think a little about Andrea Woody’s comments on the paper I read at a HSS session on “technoscience as practice.” The session was organized by Tom Stapleford— a friend from my time at Notre Dame — and also included Catherine Jackson and Edward Jones-Imhotep, who are all historians.\nMy paper — you can read it at the link above — was a philosophical (too philosophical for a few members of the audience; I don’t blame them) account of practices, based largely on the account that I developed in my dissertation. The account draws a great deal on the conception of practices sketched by Alasdair MacIntyre in several of his works (most famously, the second half of chapter 14 of After Virtue.) In brief, a practice is a complex, collaborative, socially organized, goal-oriented, sustained activity. The account is designed to incorporate insights from work in the history and sociology of science over the past 50 years, without falling into the trap of “anything-goes relativism” or noncognitivism that (many philosophers think) is a central commitment of that research.\nIn her comments, Andrea raised two questions for my view, which go right to the heart of what I’m trying to do. Here’s how I would paraphrase them, based on her slides (which she was kind enough to share with me and the other speakers in the session) and my notes:\n\nCan we distinguish goods of excellence from goods of efficiency while remaining outside of a practice? What if scientists adopt a set of norms and goals that make it much more like football than what we would recognize as science? (That example comes from Ernan McMullin giving a critique of Larry Laudan, if I recall correctly.)\nIs it really true that practitioners are (or even must be) “realists” about the norms and goals of the practice?\n\nThese are important questions for my account. Let me first explain why they’re important, and then gesture towards some answers.\nRegarding the first question, I’m not so concerned about science gradually becoming football, simply because it doesn’t seem like a live possibility. But I am concerned about actual cases in which science promotes oppression, such as racism and sexism, and the commodification of science. Reflecting these concerns, I very much want an account that can help us recognize that, when science becomes oriented towards oppression and domination, something has gone wrong; and indeed help us clarify in exactly what sense something has gone wrong. In a paper published earlier this year, I suggested that this problem is now at the center of work in science and values. A negative answer to Andrea’s first question means that our critical resources for evaluating scientific practice are quite limited, and far more limited than I would like. It seems to push us too closely to “anything goes relativism.”\nAndrea’s question might be motivated, in part, by the seeming thinness of my account of the distinction between goods of excellence and goods of efficiency. I characterize goods of excellence as having four features: they’re intrinsically valuable, progressive (in a complex technical sense), integrated (valuable for realizing other goods of excellence), and owned collectively. How does this help us say what’s wrong with commodified or sexist science?\nBy definition, commodified science is science done for the sake of wealth. But wealth isn’t intrinsically valuable. It’s not valuable for its own sake; instead, it’s valuable only because it can be used to do other things. Even if we think of Scrooge McDuck, taking great pleasure from swimming in his money bin, the money itself is still not intrinsically valuable; Scrooge values it because it gives him pleasure. Furthermore, commodified science is owned privately, and there’s some reason to think that our institutions for the private ownership of knowledge conflict with and frustrate ongoing scientific research. So commodified science fails to have three of the four features.\nThe problem with sexist science is that it collaborates with patriarchal power structures, among other forms of oppression. It thereby frustrates the practices of feminists and other activists working to alleviate oppression. We might also question whether sexist science is owned collectively — i.e., we might be able to argue that it’s “owned” by men rather than by women, and hence is the private property of just one gender — and whether it’s intrinsically valuable — especially if it leads to false belief, but perhaps even in some cases where it leads us to true belief.\nRegarding the second question, I wasn’t entirely comfortable with the language of “realistic” and “relativistic” either. So let me explain what I was trying to do.\nI wanted to contrast two views of the norms and goals of a practice. On the one hand, for practitioners qua practitioners, engaged in practical deliberation (i.e., practitioners trying to decide what to do), at least some of the norms and goals of the practice serve as assumptions or premises in their practical reasoning. Insofar as someone calls these assumptions into question, they are or should not be members of the community of practitioners. Consider the way the scientific community responds to data fabrication, for example.\nOn the other hand, from a perspective outside the practice, it’s easy to see that these norms and goals are the contingent products of very messy political and economic interactions, and consequently it’s extremely easy to call these norms and goals into question. Consider, again, history and sociology of science over the last 50 years, and the skeptical implications some have attempted to draw out of this work.\nTo add a third hand, I think the role of feminism in science has been so important because many feminist and women scientists have combined both views, to the great benefit of scientific practice. Recognizing that androcentrism and sexism in science are products of the way science has been embedded in patriarchy, these scientists have been able to identify various methods and assumptions as androcentric and sexist. Replacing these assumptions and methods has led to great progress in some fields of science, even by the lights of scientists who don’t share any deep feminist commitments.\nAll together, I think philosophy of science needs to be able to incorporate the empirical insights of historians and sociologists into the ways science actually works, without giving up on the possibility of evaluation, criticism, and other kinds of normative assessment. And feminist science has shown us that these empirical insights can actually provide leverage for valuable critiques that lead to progress. (Note all the evaluative language in that last sentence.)\nSo I think this is what I was trying to get at with the language of “realistic” and “relativistic”: We want to recognize both that the norms and goals of science have substantial normative authority, but also that the content of these norms and goals are contingent and sometimes require (by their own lights, and the lights of other perspectives) modification. Many scientists engaged in normal science (in Kuhn’s sense) recognize the normative authority of the norms and goals of scientific practice, but fail to recognize their contingency. Many historians and philosophers have exposed the contingency, but at the risk of completely discarding normative authority. Feminist science is especially valuable because it can recognize both of these things at once.\n\nNovember 14th, 2014 1:43pm hsspsa2014 (philosophy of) science scientific practice feminist science science and values commodified science"
  },
  {
    "objectID": "posts/2013-07-21-the-history-of-philosophy-as-a-social-network.html",
    "href": "posts/2013-07-21-the-history-of-philosophy-as-a-social-network.html",
    "title": "The History of Philosophy as a Social Network",
    "section": "",
    "text": "As I discussed in a post last month, I recently took a Coursera class on network theory and analysis. I did this in part because I wanted to evaluate the MOOC format from a student’s perspective. But I also did it because network theory is very hot right now and appeals to my old life as a mathematician.\nDuring the first couple of weeks of the class I decided that I needed a real-life network to play around with. Inspired by this post, I decided to look at the history of philosophy as described by the “influence” relationship in Wikipedia. I plan to write a paper — maybe for a digital humanities project — on my findings, complete with some background on network theory for non-philosophers. Here I’m going to skip the technical details and show you some pretty pictures and some basic results about identifying the “most influential” philosophers.\nMy dataset is large by the standards of the traditional humanities, though not so large by the standards of many digital humanities projects and tiny compared to, say, a genome sequence. The base graph contains 1,997 distinct philosophers (assuming no repeats — one step in acquiring the dataset consolidates repeats, but I have to identify these manually) with 4,022 connections between them. Connections represent one philosopher influencing another. Because of this size, pretty much any effort to represent the entire graph is an incomprehensible mess. So here’s a picture of the 177 philosophers in the dataset who are connected to at least 10 other philosophers.\n\n\n\nphilosophy.v1-0.alt\n\n\nThe size of the circle indicates something called the eigenvector centrality of the node in the network — one of the measures of influence that I investigated and will discuss below. The color picks out distinct communities of highly-inter-connected philosophers, though I think the algorithm used to determine this doesn’t work so well for this particular network.\nThe post that I linked to in the second paragraph did this much. But I wanted to use some of the tools for quantitative analysis that I was learning in my course. Since the contemporary culture of philosophy seems to care a lot about ranking — programs, schools, journals, individual philosophers — I decided to focus on that.\nFrom a network perspective, one obvious ranking question to ask is “how well-connected is each node to the rest of the network?” These are measures of the centrality of nodes. And there are several different ways to do it. One way is simply to measure the number of immediate connections, or neighbors, that a node has. More sophisticated ways look at the node within the overall structure of the network. I used three measures of centrality: out-degree, the number of philosophers directly influenced by the given philosopher; betweenness, which looks at the position of the given philosopher along distant connections in the network; and eigenvector, which looks at how well-connected the given philosopher is to other well-connected philosophers. Here are the rankings:\n\n\n\n\nOut-degree\nBetweenness\nEigenvector\nCondorcet\nLeiter “Most Important”\n\n\n\n\n1. Kant\n1\n2\n1\n44\n3\n\n\n1. Hegel\n3\n1\n2\n42\n11\n\n\n3. Marx\n4\n5\n4\n35\n17\n\n\n4. Aristotle\n2\n\n3\n27\n2\n\n\n4. Descartes\n11\n4\n7\n26\n5\n\n\n6. Plato\n5\n16\n5\n22\n1\n\n\n6. Nietzsche\n6\n\n6\n20\n18\n\n\n6. Heidegger\n8\n10\n10\n20\n\n\n\n6. Leibniz\n\n3\n9\n20\n12\n\n\n6. Spinoza\n14\n6\n8\n20\n13\n\n\n11. Hume\n13\n7\n15\n13\n4\n\n\n11. Rousseau\n\n8\n12\n12\n20\n\n\n11. Husserl\n10\n15\n11\n12\n\n\n\n14. Wittgenstein\n7\n\n\n9\n7\n\n\n14. Aquinas\n9\n\n\n7\n10\n\n\n14. Mill\n\n9\n\n7\n14\n\n\n\n(Sorry this looks like crap — Tumblr seems to be eating my style tags. Blanks indicate that the given philosopher wasn’t ranked in the top-20 by that centrality measure. I’m going to come back to the final column.) The first thing that I notice is that there is both significant agreement and significant disagreement among these rankings. All three agree that Kant and Hegel are at the top, followed a little ways by Marx, and Heidegger near the bottom of the top-10. But there’s also significant disagreement about who comes in the middle.\nTo some extent, this is probably due to the way the different measures work. For example, Ancient philosophers have a disadvantage on the betweenness measure: most of the philosophers in the network came centuries later, and so the Ancient philosophers don’t mediate many connections. I want to discuss this further in the paper.\nTo aggregate these three lists, I used a variation on Condorcet polling. In a Condorcet poll, each voter ranks all of the candidates in order of preference; we then aggregate these votes by seeing how among how many voters each candidate beats the others. Here I used the three lists as my three voters, and replaced the one-to-one comparisons of a Condorcet poll with a simpler counting scheme. For example, Hegel outranks the other 15 a total of 42 times among the three lists; Marx outranks the other 15 a total of only 35 times, and so comes below Hegel. We can then assign rankings, ignoring differences of 2 “points” or less.\nFinally, it’s interesting to contrast these rankings with the results of a poll on the “most important philosophers of all time,” conducted by Brian Leiter on his blog in 2009. That’s the final column of the table above. Compared to my analysis, Leiter’s poll ranks Plato significantly higher, and ranks Hegel, Marx, and Nietzsche significantly lower. Heidegger and Husserl don’t even show up in Leiter’s top-20; conversely, my top-20 doesn’t include Socrates, Locke, and Frege.\nWhat could explain this difference? My dataset ultimately comes from Wikipedia; Leiter’s comes from the readers of his blog. Insofar as “anyone can edit a Wikipedia page” and Leiter’s audience is primarily professional philosophers, it seems reasonable to think that this probably explains a lot of the difference: professional philosophers generally think less of Heidegger and Husserl, and more of Plato and Frege, than the “anyone” who is editing the Wikipedia pages on these philosophers.\nDoes this mean that Leiter’s rankings are, in some sense, better than the results of my analysis? I’m skeptical, for at least two reasons. First, the voters in Leiter’s poll knew that they were voting in a poll, and so simply a priori there’s the possibility of people voting multiple times or engaging in strategic voting to manipulate the results. The Wikipedia editors had no idea that I was going to carry out this analysis, and so it’s much less plausible to think that they might have tried to manipulate the dataset.\nBut that’s a minor concern. My second and deeper reason is it’s not clear what counts as a “better” ranking. We might think a better ranking is “more accurate”; but more accurate to what? Leiter asks which philosophers are “most important,” and it’s not clear to me that there’s an objective fact of the matter about whether Plato is more or less important than Marx. If the poll had been taken in the eighteenth century, Aristotle and Aquinas probably wouldn’t have even been candidates; late in the nineteenth century, Hegel would have been much closer to the top and Hume much further down. Furthermore, even if there is an objective fact of the matter, the poll’s methodology seems to be better designed to measure who is liked more by readers of Leiter’s blog than who is objectively more important.\nMy analysis is based on data about philosophical influence. It seems to me more plausible to think that these data are more objective than importance. But influence is quite vague. Is it reasonable to say that Descartes was influenced by Aristotle or al-Ghazali? What about someone like John Rawls, who was arguably influenced by a hundred different thinkers in the history of ethics, but in the dataset is only influenced by Locke? Furthermore, we have to draw disciplinary boundaries somehow. Should these be based on the boundaries of the contemporary university? Should we include Newton, or Thomas Kuhn?\nAt this point, it seems that we must take the purposes of the rankings into account. What question are we trying to answer, and why are we trying to answer it?\n\nJuly 21st, 2013 11:12am"
  },
  {
    "objectID": "posts/2020-01-06-macgillivray-commentary.html",
    "href": "posts/2020-01-06-macgillivray-commentary.html",
    "title": "Comment: Inductive Risk, Science, and Values: A Reply to MacGillivray",
    "section": "",
    "text": "Last fall Jessey Wright, P.D. Magnus, and I put together a short commentary replying to a paper on inductive risk published in the journal Risk Analysis. The paper is now available here.\nAbstract:\nThe argument from inductive risk (AIR) is perhaps the most common argument against the value‐free ideal of science. Brian MacGillivray rejects the AIR (at least as it would apply to risk assessment) and embraces the value‐free ideal. We clarify the issues at stake and argue that MacGillivray’s criticisms, although effective against some formulations of the AIR, fail to overcome the essential concerns that motivate the AIR. There are inevitable trade‐offs in scientific enquiry that cannot be resolved with any formal methods or general rules. Choices must be made, and values will be involved. It is best to recognize this explicitly. Even so, there is more work to be done developing methods and institutional support for these choices.\nhttps://doi.org/10.1111/risa.13434"
  },
  {
    "objectID": "posts/2012-03-14-friedersdorf-the-sex-friendly-case-against-free-birth-control-part-i.html",
    "href": "posts/2012-03-14-friedersdorf-the-sex-friendly-case-against-free-birth-control-part-i.html",
    "title": "Friedersdorf: The Sex-Friendly Case Against Free Birth Control, part I",
    "section": "",
    "text": "Link  \nFriedersdorf is a right-of-center libertarian and a blogger for The Atlantic, and one of a handful of conservative bloggers whom I read regularly.  In this post, he articulates his own position on birth control:  ‘an enthusiastic embrace of easy access to birth control pills and intrauterine devices, coupled with a rejection of … free access to birth control for any woman’.  In other words, Friedersdorf (a) is not a social conservative, opposed to birth control as such, but (b) is a right libertarian, opposed to public subsidies for birth control.  \nIn the second titled section of this post,‘The Case Against Subsidized Birth Control For All’, Friedersdorf gives what I think is best read as two separable arguments against public subsidies for birth control.  Here are some quotations – strung together from a few consecutive paragraphs – that give the first argument:  \n\nWe’re now in the last stages of a transition to a health care system where what’s covered by health insurance …. is a matter of what our polity, through its elected representatives, bureaucrats accountable to them, and judges who aren’t, decide that we ought to provide to all citizens.\nThere are going to be redistributive consequences, or the equivalent of subsidies, no matter what health care system we choose. Resources are just going to flow from healthy people to diseased people. And it is widely thought that resources are spent unequally, but justly, when one man lives a healthy life and dies at age 60 in his sleep while another, who pays the same insurance premiums or taxes, contracts multiple sclerosis at age 45, requires two decades of costly care, and dies at 65.\nIncluding birth control (as distinct from contraceptives used for other purposes) in universally mandated health-care coverage has its own unique redistributive effect, one that seems more problematic in a pluralistic society than funneling resources from the healthy to the sick or malfunctioning. Mandating participation in an insurance risk pool that covers birth control redistributes resources based partly on lifestyle choices, values, and conceptions of what is fulfilling. For example, gays and lesbians have no use for birth control, but are being made to participate in risk pools that cover it, effectively leaving them with fewer resources as a result of their status as a cultural minority group, rather than a part of the majority that desires birth control.\n\nThe crucial claim in this argument is that subsidizing birth control is unfair to GLBTQ citizens or (in a version that shows up later) is unfair to citizens who are morally opposed to birth control.  What’s interesting is that this is an egalitarian liberal complaint, not a libertarian one – and recall that Friedersdorf is a right libertarian.  A libertarian complains that redistribution (typically, redistribution as such) is involuntary; an egalitarian liberal complains that redistribution (typically, the distribution involved in a particular public policy proposal) is unfair.  \nNow, how does Friedersdorf understand fairness?  If we focus on the ‘effectively leaving them with fewer resources’ bit, it might seem fairness is identified with Pareto optimality:  a change to the status quo is unfair if and insofar as there is some individual with fewer resources after the change compared to the status quo.  If this is right, then the egalitarian liberal talk of fairness is actually just a cover for right libertarianism:  by this conception of fairness, taking more money from Mitt Romney to pay for homeless shelters is unfair.  (The right libertarian philosopher Jan Narveson explicitly appeals to Pareto optimality in the crucial chapters of his book The Libertarian Idea.)  In general, any policy that involves redistribution is unfair.  \nBut this might not be the best way to read Friedersdorf.  Let’s continue the quotation from the last paragraph:  ‘effectively leaving them with fewer resources as a result of their status as a cultural minority group, rather than a part of the majority that desires birth control’.  This suggests at least two ways of modifying the conception of fairness from the last paragraph:  \n\nA change to the status quo is unfair if and insofar as there is some minority group opposed to the change with fewer resources after the change compared to the status quo.  \nA change to the status quo is unfair if and insofar as there is some historically oppressed group with fewer resources after the change compared to the status quo.  \n\nNote that (1) leads us, more or less, back to right libertarianism again:  Mitt Romney and most of the other members of the 1% are a minority group opposed to increasing the highest marginal tax bracket to 50% to pay for homeless shelters, and they would end up with fewer resources after the change compared to the status quo.  So, according to (1), this change would be unfair.  Now, if a majority voted to raise taxes on themselves and only themselves, this would qualify as fair.  But I think most right libertarians would be fine with this, only quibbling about using the state rather than setting up a voluntary charitable organization. \n(2), I think, is an honest-to-goodness egalitarian liberal principle.  Mitt Romney is not a member of a historically oppressed group (at least with respect to his wealth; his religious beliefs are another matter), and so according to (2) it is not unfair to raise his taxes to pay for homeless shelters.  But GLBTQ folks are a historically oppressed group, and the proposed subsidy would compel them to sacrifice for the benefit of historically privileged straights.  \nIt’s a decent point.  But I have two responses to it.  First, I suggest that the strength of the argument depends on the relative size of the sacrifice:  how much GLBTQ folks would be compelled to sacrifice and how much straights would stand to gain.  I’m too lazy to look up estimates at the moment, but I expect the sacrifice would be rather small – on the order of a few dollars a month.  Second, the primary beneficiaries would be not all straight folks, but women, also a historically oppressed group.  So this argument, which seems to be about oppression vs. privilege, is more accurately about one kind of oppression vs. another."
  },
  {
    "objectID": "posts/2013-01-13-notes-on-predators-and-animal-ethics.html",
    "href": "posts/2013-01-13-notes-on-predators-and-animal-ethics.html",
    "title": "Notes on Predators and Animal Ethics",
    "section": "",
    "text": "In this post, I’ll be discussing one of the common arguments against (moral) vegetarianism. (Moral vegetarianism is the view that vegetarianism is morally required, or that eating meat is morally impermissible. I’ll just use “vegetarianism” here.) I’ve encountered it every single time I’ve taught an argument for vegetarianism; at a few conversations at my co-op; and even at a job interview. I’ll argue that the force of the argument depends a great deal on how the proponent of vegetarianism has argued for her position.\nThe argument from predators is, I think, a reductio of vegetarianism:\nStep (2) follows from premise (1) by instantiating an implicit universal quantifier. Step (3), I think, is typically taken to be obvious; but the anti-vegetarian might argue from obligate carnivores such as housecats, or wild predators such as wolves and mountain lions. Given the contradiction between (2) and (3), it follows that (1) cannot be true.\n(N.B. I’m simply going to ignore science fiction proposals to genetically engineer predators into peaceful herbivores.)\nAgain, my claim is that the dialectical force of this argument – how compelling an objection it is to vegetarianism – depends a great deal on how the proponent of vegetarianism has argued for premise (1). I’ll discuss arguments for vegetarianism from four different families of normative ethics."
  },
  {
    "objectID": "posts/2013-01-13-notes-on-predators-and-animal-ethics.html#animal-rights-deontology",
    "href": "posts/2013-01-13-notes-on-predators-and-animal-ethics.html#animal-rights-deontology",
    "title": "Notes on Predators and Animal Ethics",
    "section": "Animal Rights (Deontology)",
    "text": "Animal Rights (Deontology)\nRights-based arguments for vegetarianism are exemplified by Tom Regan’s work; Evelyn Pluhar has another, rather different sort of argument for animal rights in her contribution to Sapontzis’ anthology Food for Thought: The Debate over Eating Meat. For proponents of a rights approach, the argument for vegetarianism proceeds from the claims that (a) animals have rights and (b) eating them violates these rights. Since it’s morally impermissible to violate any agent’s rights, it’s impermissible to eat animals.\nWhat appears to be the standard reply among proponents of animal rights to the argument from predators is to distinguish between moral agents and moral patients. Both moral agents and moral patients have rights; but only moral agents have obligations to respect rights. Or, moral patients cannot violate the rights of other moral patients (or of moral agents).\nSo, human beings are moral agents, and we violate the rights of pigs and cattle when we kill and eat them. However, pigs and cattle and cats and wolves and deer and so on are all moral patients, not moral agents. So the wolves do not violate the rights of the deer when the former kill and eat the latter. Hence, in the argument from predators, (1) does not actually imply (2). Indeed, (1) as given is ambiguous. Stated more precisely, the vegetarian’s claim is that eating meat is morally impermissible for moral agents. Since non-human predators – at least the ones we know about – are not moral agents, this premise implies nothing at all about their eating habits.\nI find this response to the argument unsatisfying for three reasons. First, in generally I dislike appeals to rights. Rights approaches – both in animal ethics and more generally – seem to spend far too much time on whether the beings in question have rights (and why this is the case), and almost none on exactly which rights they have. Given that my cat has rights because he is a subjects-of-a-life, do I violate his rights by not letting him out of my apartment? Or, given that, when I let him out, he chases, catches, and kills the mice and chipmunks that leave in the bushes, do I (as the moral agent who’s causally responsible here) violate the rights of the mice and chipmunks? I don’t see how a rights approach can even begin to answer these question.\nA rights-based vegetarian might respond that their account does at least clearly support claims (a) and (b), which together with the general notion of rights is enough to imply vegetarianism. So, even if a rights-based approach can’t answer all of our questions about animal ethics, it can at least answer this one.\nSecond, I find Val Plumwood’s critique of Regan’s view quite compelling:\n\nRegan attempts to meet this objection by claiming that since the wolf is not itself a moral agent (although it is a moral patient), it cannot violate the sheep’s rights not to suffer a painful and violent death (Regan 1986, 285). But the defense is unconvincing, because even if we concede that the wolf is not a moral agent, it still does not follow that on a rights view we are not obliged to intervene. From the fact that the wolf is not a moral agent it only follows that it is not responsible for violating the sheep’s rights, not that they are not violated or that others do not have an obligation (according to the rights view) to intervene. If the wolf were attacking a human baby, it would hardly do as a defense in that case to claim that one did not have a duty to intervene because the wolf was not a moral agent. But on Regan’s view the baby and the sheep do have something like the same rights. So we do have a duty, it seems, (on the rights view) to intervene to protect the sheep-leaving us where with the wolf? (ibid., 8)\n\nThat is, the wolf no more violates anyones rights when stalking and eating a human than when stalking and eating a deer. But then a rights approach seems either to conclude that nothing wrong happens in either case – which I take it to be clearly incorrect – or to fail to provide practical guidance – which, after all, is what ethics should do.\nNote the similarity between my first objection and Plumwood’s objection. We both consider scenarios – not vegetarian scenarios, though predation scenarios – in which the rights-based approach does not seem to provide practical guidance. So I suppose the proponent of animal rights could respond in the same way as above, namely, that their account doesn’t claim to speak to all of these issues. But then we might begin to suspect that the rights-based approach can only answer very few of our animal ethics questions.\nThird, the distinction between moral agents and patients strikes me as ad hoc, and perhaps even self-destructive. Natural rights approaches get much of their appeal from the idea that all persons are as such equal – that distinctions between lords and serfs, men and women, whites and blacks, humans and animals, are morally irrelevant. If animals are (mere) moral patients, while we are (mighty) moral agents, then why shouldn’t we sacrifice the interests of animals when they conflict with our own? This sort of slide might be especially attractive when we consider Plumwood’s child-stalking wolf."
  },
  {
    "objectID": "posts/2013-01-13-notes-on-predators-and-animal-ethics.html#utilitarianism",
    "href": "posts/2013-01-13-notes-on-predators-and-animal-ethics.html#utilitarianism",
    "title": "Notes on Predators and Animal Ethics",
    "section": "Utilitarianism",
    "text": "Utilitarianism\nUtilitarian arguments are probably the most familiar family of arguments for vegetarianism, thanks to Peter Singer. In contrast with rights-based approach (human and animal), utilitarianism is context-sensitive, that is, it can support different normative claims for different individuals in different situations. (Different versions of utilitarianism will be differently context-sensitive, of course: act utilitarianism will be much more sensitive to context than rule utilitarianism.)\nContext sensitivity gives at least some versions of utilitarianism enough room to separate (contemporary) humans (in industrialized societies) from non-human predators. For contemporary humans in industrialized societies, the meat we might eat would be produced in factory farms, inflicting great suffering on the animals involved, and providing us with only a small amount of pleasure; assuming vegetarianism for us would reduce this suffering (and perhaps be just as pleasurable), it follows that eating meat is morally impermissible. On the other hand, if non-human predators refrained from eating meat, then they would die out and herbivore populations would skyrocket, leading to significantly greater suffering than under what we might call ordinary moderate predation. So then ordinary moderate predation, not vegetarianism, would be morally required for predators.\nNote that utilitarianism can separate us from non-human predators without resorting to questionable distinctions, like the moral agents / moral patients distinction. The rights-based approach responded to the argument from predators by claiming that different rules apply to different kinds of beings – this led to my third criticism, that this response undermined the appeal of the approach. Utilitarianism claims that the same rule – maximize net pleasure – applies to all beings, but that the implies of the rule are different in different situations.\nBecause of its context sensitivity, the utilitarian response depends very much on its empirical, causal claims: that vegetarianism for us and ordinary moderate predation will reduce net suffering, compared to their respective alternatives. First, it’s not clear that decades of moral exhortation encouraging individuals to become vegetarian has been at all effective. Measured in pounds per capita, US meat consumption increased about 26% between 1960 and 2007. It has decreased about 12% since that high point – but this seems to be largely due to the Great Recession. On the other hand, it’s true that the average dressed weight of cattle has increased over that same time, by about 33% (see the third graphic down on that page). And the number of cattle is currently at about 30 million, down from a break of about 45 million in the 1970s; though this is still a net increase compared to the 25 million or so in 1960 (see the second graphic on that same page). And in pounds per capita we eat about three times as much chicken as we did in 1960.\nSecond, agroecological practices clearly involve much less animal suffering than industrial meat production; by sustainably managing agroecosystems and their required resources (in some sense of “sustainable”) would probably maintain higher levels of net pleasure over time; and arguably require eating some of the animals involved. So a utopian utilitarian argument – see my explanation of “utopian” in the previous link – might conclude that eating meat is morally required."
  },
  {
    "objectID": "posts/2013-01-13-notes-on-predators-and-animal-ethics.html#environmentalist-ethics",
    "href": "posts/2013-01-13-notes-on-predators-and-animal-ethics.html#environmentalist-ethics",
    "title": "Notes on Predators and Animal Ethics",
    "section": "Environmentalist Ethics",
    "text": "Environmentalist Ethics\nRoughly, ecocentric environmentalism is the view that functionally integrated biological systems have intrinsic ethical significance, and so there are at least some defeasible reasons to promote and protect such systems. There are numerous terms in that definition that really need further explication, but I’m going to take it to be adequate for my purposes here. The standard-bearers for ecocentric environmentalism, as I understand it, are Aldo Leopold and J. Baird Callicott; one might also include Arne Næss’s Deep Ecology, though I do prefer Leopold’s (and Baird’s) concrete, rather American, Land Ethic to Næss’ mysticism.\nA somewhat different approach to environmental ethics is agrarianism, as exemplified by Wendell Berry and Paul Thompson (and actually Leopold, notably). Agrarianism is closely tied to community- and place-based agricultural practices and a conception of stewardship of the land, and may not place much value on “wilderness.” Specifically, agrarians are much more likely than ecocentrics to condone human management of/coproduction with/meddling in the extra-human parts of the world.\nBoth ecocentrics and agrarians are deeply opposed to CAFOs and other methods of intensive, “factory” farming. Ecocentrics are probably more likely than agrarians to emphasize the pollution and environmental destruction produced by CAFOs; agrarians are probably more likely to emphasize the damage intensive farming does to agricultural communities, both cultural (as economic values replace stewardship and community) and environmental (the harm suffered by humans and animals from pollution and overexploitation of the land).\nFurthermore, both ecocentrics and agrarians will have complex attitudes towards more traditional, less intensive forms of animal agriculture. In ecological terms, both tend to think of humans as keystone species in our food webs, managing the populations of other species. Agrarians will see traditional forms of animal agriculture as paradigmatic of a well-functioning, human-managed food web, and note that animal husbandry is traditionally one aspect of stewardship. So, insofar as stewardship should be promoted; and stewardship involves raising, working with, and then consuming animals; it seems to follow that consuming animals should be promoted. However, ecocentrics might argue that even traditional animal agriculture is harmful and disruptive to functionally integrated biological systems – the difference between CAFOs and peasant agriculture is one of degree, not kind — and we should return to a hunter-gatherer lifestyle. Or maybe just a gatherer, vegetarian lifestyle? I don’t think either ecocentrism or agrarianism points decisively either towards or away from vegetarianism.\nEcocentrics will almost certain condone predation as the way in which a functionally integrated ecosystem maintains itself – keeps the deer from reproduction out of control and crashing the system, in other words. While agrarians in the past may have condoned killing predators, either systematically or opportunistically, the grounds would have been the threat predators posed to humans. That is, we need to kill the wolves to protect our flocks and herds. However, I think contemporary agrarians are more inclined to see human agroecosystems as integrated parts of broader systems, such as watershed systems. Such systems will incorporate, for example, agricultural fields, woodlands, and wetlands, and predators will be essential to keep both the whole system and its various parts functioning. For example, predation by owls and other birds of prey might be necessary to keep mice populations under control in the bushes on the border between the forest and fields; if mice populations get out of control, they may eat the seeds in the fields."
  },
  {
    "objectID": "posts/2013-01-13-notes-on-predators-and-animal-ethics.html#care-ethics",
    "href": "posts/2013-01-13-notes-on-predators-and-animal-ethics.html#care-ethics",
    "title": "Notes on Predators and Animal Ethics",
    "section": "Care Ethics",
    "text": "Care Ethics\nCare ethics emphasizes particular, personal relationships and dependency; it has its roots in work on the relationship between parents, especially mothers, and children, but has been extended to animals by Josephine Donovan, Carol Adams, and Lori Gruen. A simple (oversimplified) care argument for vegetarianism might run as follows: since (a) compassion in general and within dependent-caregiver relations specifically is a crucial virtue, (b) the animals that we raise for food are dependent on us, and (c) eating someone is incompatible with having compassion for them, care ethics would recommend vegetarianism as virtuous.\nNow we can bring in the argument from predators. Predators depend on their prey; so then they should show compassion to their prey; and so they should be vegetarians, just like us.\nCare ethics is a virtue ethics, rather than a rule ethics. The recommendations offered by care ethics are always defeasible and sensitive to particularities, by contrast with the universal and absolute prohibitions of rights-based approaches and utilitarianism. So it seems to me that care ethics can say that compassion can be qualified by nutritional needs: insofar as predators need to eat their prey to survive, carnivorism is not a vice. However, obligatory carnivorism can be tempered by other aspects of compassion. Bringing a chipmunk home to torture for a while, before unnecessarily killing it, is vicious, whether this is done by a human or a housecat or a mountain lion. Thus, it is not the case that eating meat is morally impermissible; but rather that eating meat without compassion or respect for the animal who is eaten is vicious.\nBut what about other kinds of needs? My cat, for example, has no nutritional need to stalk and torture chipmunks. But it does seem like these activities are required for his flourishing – for him to lead a good life for a cat. He certainly has substitutes: toy mice (and other things that he thinks are good to bat around, such as the pen I’m trying to write with), and watching the chipmunks through the window. But it’s quite clear after more than a few days indoors – whether this is because I haven’t been home or because he refuses to walk on snow – that he’s quite frustrated at being confined, and especially at not being able to hunt.\nI suspect that many care ethicists would recognize the existence of moral dilemmas: situations in which we have no morally permissible options, or in which we cannot exemplify all of the virtues at once. In the case of my cat, I must make a choice between showing compassion for him – letting him outside to hunt chipmunks – or showing compassion for the chipmunks – keeping him inside, no matter how frustrating this is for him. I cannot exercise compassion towards both my cat and the chipmunks, and so there is no completely virtuous option.\nIf that is right, then the best way to proceed would be to look for options that don’t require completely sacrificing both virtues. That is, options where (a) the cat can hunt and (b) the chipmunks don’t suffer too badly. For a while, I considered putting a bell on his collar. But he likes to crawl under the chain-link fence in the next yard, and lost two collars in the first month that he lived with me. (The jingling when he’s playing inside at night would also keep me awake.) Still, I can keep him from bringing chipmunks back inside; put them back outside when he does manage to bring them in (or has caught them inside our apartment); and scare him into dropping a chipmunk when I see through the window that he’s caught one. Also, at least while it’s cold, he’s not interested in being outside for more than about half an hour a day, and this doesn’t seem like enough time for him to do too much to the poor chipmunks. None of this completely resolves the dilemma; but it does reduce some of the tension. (Thanks to several people who helped me think through this section up to this point: Amelia Hicks, Daniel Immerman, Emily Dawson, Samantha Noll, Kathryn Pogin, and a few others whose names I cannot recall.)\nAs a final aside, care ethics also seems relevant to recent controversies over “happy meat.” Proponents of “happy meat” argue against vegetarianism on the grounds that there is little or nothing wrong with eating animals who led decent lives before being humanely slaughtered. “Happy meat” arguments are perhaps typically responses to utilitarian arguments for vegetarianism: we’re not talking about the egregious, lifelong suffering of animals in CAFOs, but instead a moment of suffering at the end of a very pleasant existence in a natural, agroecological setting. But these arguments can also be cast in terms of compassion, and so in terms of care ethics: “happy meat” comes from animals whom we care for properly during their lives and at slaughter. Indeed, proponents of urban farms sometimes maintain that we should only eat meat if we are willing to look the animal in the eye as we kill her or him.\nVegetarians generally respond to such arguments with horror, as the words of a psychopath. How could you claim to both care about an animal and slaughter it? I suspect, however, that the proponents of “happy meat” and vegetarians are working with rather different conceptions of compassion and care, and so talk past each other in these debates. Vegetarians assume that compassion is incompatible with killing and eating. Proponents of “happy meat” disagree. I think they think that compassion requires treating these animals with respect and dignity, but that it’s possible to do so while killing them. For example, a painless, terror-free, quick death that acknowledges the significance of sacrificing their life for ours – looking into their eyes – might seem respectful and dignified.\nI can see how this kind of treatment treats the animal with dignity, and maybe even respect. But I don’t think it’s compassionate. So the disagreement I have with “happy meat” seems to come down to our rival understandings of compassion.\n\nJanuary 13th, 2013 1:14pm food"
  },
  {
    "objectID": "posts/2020-01-21-toolbox-lite.html",
    "href": "posts/2020-01-21-toolbox-lite.html",
    "title": "A “Toolbox-lite” for the First Day of Philosophy of Science",
    "section": "",
    "text": "This semester I get to teach an undergraduate philosophy of science course for the first time ever, and today was the first meeting of the class. I’ve always struggled with first days. You can’t get students to do any work beforehand, which makes it really hard to have a substantive discussion. So you call roll to check the roster and then you go over the syllabus, which is somewhat useful but very boring. And because my preferred teaching style involves as little lecturing as possible, me reading through the syllabus also radically misrepresents the feel of every other meeting of the class.\nLast fall, I had the idea of adapting the workshop format from the Toolbox Dialogue Intiative at Michigan State. The Toolbox workshops were originally developed for interdisciplinary team science groups. During the workshop, the members of the group first complete a survey on their views about science; then the workshop facilitator runs a philosophical discussion of points of agreement and disagreement among the group members.\nIn the plan I conceived last fall, my students would engage with the Toolbox format twice: once as participants in class and then as facilitators, running a version of the workshop with lab groups across campus. The students wouldn’t need any particular preparation for the survey, it would help preview the main topics of the course, and it would give us a good starting point for the kind of open-ended discussion that characterizes my preferred teaching style. So it would work well as an activity for the first class meeting. As facilitators, students would be organized into groups of 3 and each group would be assigned a lab or other research group on campus. During the final week of classes, each group would give a presentation on highlights of their workshop discussion. This would allow them to directly tie the class content into active scientific research across campus, which is one of the four program learning outcomes for our philosophy major.\nUnfortunately the second part didn’t work out. I have 35 students registered, meaning I’d need to recruit 12 research groups on campus as participants for my students’ workshops. After just one semester at UC Merced, I do know at least a dozen other faculty; but not enough well enough to get them to sacrifice one of their lab meetings. The standard Toolbox workshop is most of a day; I felt like I could shorten it to 90 minutes or 2 hours while still keeping enough of the workshop to be useful for my students. But that would still be a significant ask from people whom I barely know.\nBut I was able to incorporate a “Toolbox-lite” into the first meeting of my class. Starting with the Scientific Research Toolbox Instrument (linked above), I selected a few questions that were immediately relevant to my course, and wrote a few more. While we’re not talking directly about the definition of science or the demarcation problem (which I think is a pseudo-problem), I decided to add a set of questions about whether specific fields are sciences. The resulting list was below; students responded on a 3-point Likert scale, with anchors at “agree” and “disagree” and the middle option unmarked.\n\nAll fields of science use the same basic scientific method.\nScientific research primarily uses experimental methods.\nScientific research primarily uses quantitative methods.\nThere are strict requirements for determining when empirical data confirm a tested hypothesis.\nScientific research aims to identify truths about a world independent of the investigators.\nGood scientists let the data speak for themselves.\nScientific research reveals the laws that govern how the world operates.\nThe world is fully explicable as the assembly of its constituent parts.\nThe principal value of research stems from the potential application of the knowledge gained.\nAllowing values to influence research is unscientific.\nWhen it comes to making public policy, scientific facts are more important than people’s feelings.\nMembers of the general public shouldn’t question scientific findings.\nAre these academic fields sciences?\n\nEvolutionary biology\nAgriculture\nPublic health\nChemical engineering\nParticle physics\nClimate science\nLiterature\nHistory\nPsychology\nEconomics\nAnthropology\nSociology\nPhilosophy\n\n\nI had the students fill this out on Qualtrics while I was passing out the hard copies of the syllabus. I wrote a little dashboard in R to retrieve the responses from Qualtrics and display summary plots.\nHere are the results.\n   \nI had the students divide into small groups, introduce themselves to their group members, and then discuss “Scientific research primarily uses quantitative methods” for about 10 minutes. Then we came back together, I solicited a few highlights from these discussions, and then we had a short class discussion (maybe another 10 minutes) on “Allowing values to influence research is unscientific.”\nThe discussions were lively. I heard a few students relating their views to their majors (the most popular major among registrants is cognitive science, followed by psychology and economics), which I think is really key to the way philosophy of science can engage students without much philosophy background.\nI think these results are really interesting. The students generally agree with each other on a few points, but most prompts have a mix of responses. The majority disagreement on “Allowing values to influence research is unscientific” and “Members of the general public shouldn’t question scientific findings” are especially interesting. We’re talking about feminist philosophy of science, race (in) science, and values in science in the middle of the term; I’m looking forward to see what the students have to say about those topics.\nThe field responses were also really interesting. For every field, even Literature and History, a majority of students said the field is a science. Philosophy gets treated more like a social science than the other two humanities fields, with a supermajority thinking that it is a science but a few students having doubts. (As a registration requirement, all of the students had to have taken an intro-level philosophy course; but only one or two are philosophy majors.) Unfortunately we didn’t have any time to talk about these responses.\nAll together, these responses and the discussions made me even more excited for this course. The students are already thoughtful about what science is and how it works, and I think every one will have their views challenged by their classmates and readings at some point."
  },
  {
    "objectID": "posts/2020-05-18-strengthening-transparency.html",
    "href": "posts/2020-05-18-strengthening-transparency.html",
    "title": "Replication, reproducibility, and Strengthening Transparency",
    "section": "",
    "text": "Strengthening Transparency in Regulatory Science is an open science rule that the US Environmental Protection Agency proposed in April 2018. If adopted, the rule would restrict the scientific information that EPA uses in policymaking, requiring that the data and analysis code be available either publicly or in a secure data enclave for independent reanalysis. Strengthening Transparency is non-accidentally similar to the HONEST Act and Secret Science Act, which were attempts by Republicans in previous congresses to introduce the same requirement. Since at least the 1990s, industry and its allies have attempted to argue that environmental public health research can’t be trusted unless “we” can inspect the “raw data.”\nStrengthening Transparency tries to justify itself with explicit appeals to the replication crisis that’s been unfolding in, primarily, social psychology. So the rule has a nice convergence between my ongoing research on public scientific controversies and my side interest in the replication crisis.\nIn March of this year, EPA published a Supplemental Notice of Proposed Rulemaking, which added definitions for several of the key terms and opened another round of public comments. I submitted a comment based on the distinction between reproducibility (can we run the same data through the same code to get quantitatively identical output) and replicability (can we run a similar experiment to get new data that support qualitatively similar conclusions). I argued that open science can promote reproducibility, but not replicability; while Strengthening Transparency is nominally based on concerns about replicability. So the proposed rule can’t do anything about the problem it’s supposed to address.\nWhile I’ve been following the replication crisis — and even taught an undergraduate course on it last fall — the focus of my research has really been on more public, policy-relevant controversies. So there’s a good chance my comment is the only thing I’ll ever write on the replication crisis. You can read it here: https://drive.google.com/open?id=1Ze4EgDgtQJCQoW1_p4ebuDGOgtX2oi79"
  },
  {
    "objectID": "posts/2014-03-11-is-motivated-reasoning-bad-reasoning-ii.html",
    "href": "posts/2014-03-11-is-motivated-reasoning-bad-reasoning-ii.html",
    "title": "Is Motivated Reasoning Bad Reasoning? II",
    "section": "",
    "text": "This is part II of a three-part series. This series will be posted simultaneously on Je Fais, Donc Je Suis, my personal blog, as well as the Rotman Institute Blog.\nIn the first part of this post, I discussed the work of social psychologist Dan Kahan on motivated reasoning. As he defines it, motivated reasoning is “the unconscious tendency of individuals to process information in a manner that suits some end or goal extrinsic to the formation of accurate beliefs.” According to what I called the antagonistic picture, motivated reasoning is bad reasoning; it leads us to have false or unjustified beliefs. And Kahan’s work shows that motivated reasoning is pervasive; specifically, I discussed some work that shows that high science literacy and numeracy seems to exacerbate, not remove, motivated reasoning.\nAll together, this leads us to a gloomy conclusion. But, in this post, I’ll argue that things aren’t necessarily so gloomy. Specifically, I’ll argue that motivated reasoning isn’t necessarily bad reasoning. I’ll do this by first thinking a bit more about why we expected high science literacy and numeracy to lead to agreement, then introducing two models of motivated reasoning, one from STS scholar Daniel Sarewitz and one from philosopher of science Heather Douglas.1\nIn the first part of the post, we saw that science literacy and numeracy seem to increase disagreement, at least about climate change. This was exactly the opposite of what we had predicted, namely, that science literacy and numeracy would decrease disagreement, and it led to our gloomy conclusion that we are doomed to bad, motivated reasoning. But why did we expect science literacy and numeracy to have this effect? In other words, why did we expect highly science literate and numerate people to agree on what the evidence says about climate change?\nPart of the answer, I think, is that we assumed that the reasoning involved — going from some evidence to accepting or rejecting a hypothesis — is unambiguous and certain. In other words, given the available evidence, it is clear whether the hypothesis should be accepted or rejected; and there is no reason to think that we could be wrong to accept or reject the hypothesis.\nIf the reasoning involved in, say, assessing the risks of climate change really is unambiguous and beyond reasonable doubt, then we would expect good reasoners to agree. But if one or the other of these assumptions is false, then the door is open for good reasoners to disagree.\nSarewitz and Douglas, respectively, start their analyses by rejecting these assumptions. Sarewitz points out that scientific evidence is often quite ambiguous, and Dougas starts by recognizing that inductive inferences can never be certain. In different ways, each goes on to argue that values have a role to play in recognizing these ambiguities and uncertainties.\nBut that means that motivated reasoning can be good reasoning. If motivated reasoning leads us to recognize when our best science is ambiguous and uncertain, and we respond to this ambiguity and uncertainty properly, then our reasoning can be good. Indeed, in this kind of case, if non-motivated reasoning would have led us to assume (incorrectly) that our findings are unambiguous and certain, then motivated reasoning would be better than non-motivated reasoning. (We’ll take a closer look at this possibility in part III.)\nLet’s turn now to Sarewitz and Douglas for a little more detail. I’m going to stick with the example of climate change to illustrate things.\nThe computer simulations we use to study the global climate are enormously complex; arguably, some of them are the most complex things that human beings have ever created. But even these extraordinarily complex systems involve significant simplifications and approximations in the ways they represent the global climate. Choices have to be made about which parts of the system will be modeled in which ways, and which parts will be left out entirely. When we move from modeling the climate itself to modeling the social and economic effects of climate change, the choices ramify.\nConsequently, Sarewitz argues,\n\nnature itself — the reality out there — is sufficiently rich and complex to support a science enterprise of enormous methodological, disciplinary, and institutional diversity. I will argue that science, in doing its job well, presents this richness, through a proliferation of facts assembled via a variety of disciplinary lenses, in ways that can legitimately support, and are causally indistinguishable from, a range of competing, value-based political positions.\n\nIn other words, choices are unavoidable; “when cause-and-effect relations are not simple or well-established, all uses of facts are selective.” Then, once we see where a certain set of choices is taking us, we seem to be free to endorse those choices — if they agree with our values — or call them into question — if they don’t.2 Specifically, once we see the implications of the choices made by climate scientists, liberals are free to endorse those choices and conservatives are free to call them into question.\nThis doesn’t mean that all sets of choices are equally good. Rather, Sarewitz’ starting point is that no one set of choices is unambiguously the best. At this point, motivated reasoning can lead us go with one set rather than another, without our reasoning being flawed in any way whatsoever. Indeed, motivated reasoning can help us recognize that someone else’s findings depend on choices that they have made unconsciously.\nDouglas’ model is built on the idea of inductive risk. When we accept or reject a general hypothesis or a prediction about the future based on limited evidence, there’s always a possibility that we’ve gotten things wrong — that our sample wasn’t representative of the whole population, that some unanticipated factor changed the way things turned out. Douglas points out that getting things wrong in this way can have negative downstream consequences. For example, if we accept the hypothesis that climate change will cause massive population displacements (due to sea level rise and desertification), make serious economic sacrifices to try to forestall these displacements, and then it turns out that the hypothesis was wrong, then our serious economic sacrifices were unnecessary. Similarly, if we reject this hypothesis, do nothing to forestall the displacements, and it turns out that we’re wrong, then we’ll have massive population displacements on our hands.\nThe values that we attach to the downstream consequences of a hypothesis can and should play a role in determining how much evidence we need to accept or reject the hypothesis. If the consequences of incorrectly accepting the hypothesis are relatively minor, then we should be satisfied with relatively little and weak evidence. But if the consequences are relatively major, then we should demand much more and more stringent evidence.\nBecause of this, when everyone can agree on the values of the various consequences, we can expect agreement on how much evidence is required to accept or reject the hypothesis, and so we can expect everyone to act the same way (that is, everyone accepts it or everyone rejects it). On the other hand, when people don’t agree on the values at stake, we expect disagreement about whether we have enough evidence.\nThis could help explain why climate change is politically polarized. Liberals generally think the economic consequences of doing something about climate change will be minor, while the social and ecological consequences of not doing something will be major. Conservatives (at least pro-capitalist conservatives) generally think exactly the opposite: the economic consequences are major and the social and ecological consequences are minor. So liberals are satisfied with the available evidence concerning climate change and conservatives want more and better evidence.\nIn this explanation of the controversy, both sides are using motivated reasoning. Indeed, on Douglas’ model, motivated reasoning is absolutely necessary. Without motivated reasoning — without taking into account the significance of the consequences — we have no way to make a non-arbitrary decision about whether we have enough evidence to accept the hypothesis. Good reasoning requires emotions and values.3\nIf this explanation is right, then the controversy over whether “the science is settled” (about climate change) is disingenuous, in two ways. First, we can never be certain about climate change, and in this sense the science can never be “settled.” It’s disingenuous for conservatives to demand this, and likewise disingenuous for liberals to claim that it has been achieved. The controversy is really over whether the evidence is sufficient to accept the key claims about climate change (humans are responsible, it will have specific bad consequences, and so on). But even this is disingenuous, because liberals and conservatives are working with different standards of sufficient evidence. Due to motivated reasoning, the evidence can be both sufficient for liberals and at the same time insufficient for conservatives.\nSo motivated reasoning is not necessarily bad reasoning. Because of ambiguity and uncertainty, emotions and values have a role to play in our reasoning. But does this mean that “reasoning” degenerates into an anything-goes free-for-all? No. That will be the topic of part III.\n\n\n\n\nTo be precise, Douglas and Sarewitz write more about “value-freedom” and “objectivity” than “motivated reasoning.” But they’re closely connected. In my research, I define value-freedom as the normative ideal or principle that ethical and political values should not play a role in accepting (or rejecting) a claim. Value-freedom is one way (but only one way) of understanding objectivity. Motivated reasoning — and cultural cognition specifically — often seems to violate value-freedom. Now, Douglas and Sarewitz are both arguing that non-value-free science can still be good science. If they’re right, then in the same way motivated reasoning can still be good reasoning. ↩︎\n\n\n“We are free to do X” is shorthand here for something like “good reasoning does not require that we do not-X.” ↩︎\n\n\nWe might worry that, say, conservatives are putting too much weight on the economic consequences, and not enough on the social and ecological consequences. That is, we might worry that conservatives are working with wrong or unreasonable values. I think a weakness of both of these models is that they treat values as exogenous — values just sort of come in from beyond the scope of rational debate and disagreement. ↩︎\n\n\n\n\nMarch 11th, 2014 10:01am (philosophy of) science science and values climate change"
  },
  {
    "objectID": "posts/2014-03-11-is-motivated-reasoning-bad-reasoning-ii.html#alternatives-to-antagonism-ambiguity-and-uncertainty",
    "href": "posts/2014-03-11-is-motivated-reasoning-bad-reasoning-ii.html#alternatives-to-antagonism-ambiguity-and-uncertainty",
    "title": "Is Motivated Reasoning Bad Reasoning? II",
    "section": "",
    "text": "This is part II of a three-part series. This series will be posted simultaneously on Je Fais, Donc Je Suis, my personal blog, as well as the Rotman Institute Blog.\nIn the first part of this post, I discussed the work of social psychologist Dan Kahan on motivated reasoning. As he defines it, motivated reasoning is “the unconscious tendency of individuals to process information in a manner that suits some end or goal extrinsic to the formation of accurate beliefs.” According to what I called the antagonistic picture, motivated reasoning is bad reasoning; it leads us to have false or unjustified beliefs. And Kahan’s work shows that motivated reasoning is pervasive; specifically, I discussed some work that shows that high science literacy and numeracy seems to exacerbate, not remove, motivated reasoning.\nAll together, this leads us to a gloomy conclusion. But, in this post, I’ll argue that things aren’t necessarily so gloomy. Specifically, I’ll argue that motivated reasoning isn’t necessarily bad reasoning. I’ll do this by first thinking a bit more about why we expected high science literacy and numeracy to lead to agreement, then introducing two models of motivated reasoning, one from STS scholar Daniel Sarewitz and one from philosopher of science Heather Douglas.1\nIn the first part of the post, we saw that science literacy and numeracy seem to increase disagreement, at least about climate change. This was exactly the opposite of what we had predicted, namely, that science literacy and numeracy would decrease disagreement, and it led to our gloomy conclusion that we are doomed to bad, motivated reasoning. But why did we expect science literacy and numeracy to have this effect? In other words, why did we expect highly science literate and numerate people to agree on what the evidence says about climate change?\nPart of the answer, I think, is that we assumed that the reasoning involved — going from some evidence to accepting or rejecting a hypothesis — is unambiguous and certain. In other words, given the available evidence, it is clear whether the hypothesis should be accepted or rejected; and there is no reason to think that we could be wrong to accept or reject the hypothesis.\nIf the reasoning involved in, say, assessing the risks of climate change really is unambiguous and beyond reasonable doubt, then we would expect good reasoners to agree. But if one or the other of these assumptions is false, then the door is open for good reasoners to disagree.\nSarewitz and Douglas, respectively, start their analyses by rejecting these assumptions. Sarewitz points out that scientific evidence is often quite ambiguous, and Dougas starts by recognizing that inductive inferences can never be certain. In different ways, each goes on to argue that values have a role to play in recognizing these ambiguities and uncertainties.\nBut that means that motivated reasoning can be good reasoning. If motivated reasoning leads us to recognize when our best science is ambiguous and uncertain, and we respond to this ambiguity and uncertainty properly, then our reasoning can be good. Indeed, in this kind of case, if non-motivated reasoning would have led us to assume (incorrectly) that our findings are unambiguous and certain, then motivated reasoning would be better than non-motivated reasoning. (We’ll take a closer look at this possibility in part III.)\nLet’s turn now to Sarewitz and Douglas for a little more detail. I’m going to stick with the example of climate change to illustrate things.\nThe computer simulations we use to study the global climate are enormously complex; arguably, some of them are the most complex things that human beings have ever created. But even these extraordinarily complex systems involve significant simplifications and approximations in the ways they represent the global climate. Choices have to be made about which parts of the system will be modeled in which ways, and which parts will be left out entirely. When we move from modeling the climate itself to modeling the social and economic effects of climate change, the choices ramify.\nConsequently, Sarewitz argues,\n\nnature itself — the reality out there — is sufficiently rich and complex to support a science enterprise of enormous methodological, disciplinary, and institutional diversity. I will argue that science, in doing its job well, presents this richness, through a proliferation of facts assembled via a variety of disciplinary lenses, in ways that can legitimately support, and are causally indistinguishable from, a range of competing, value-based political positions.\n\nIn other words, choices are unavoidable; “when cause-and-effect relations are not simple or well-established, all uses of facts are selective.” Then, once we see where a certain set of choices is taking us, we seem to be free to endorse those choices — if they agree with our values — or call them into question — if they don’t.2 Specifically, once we see the implications of the choices made by climate scientists, liberals are free to endorse those choices and conservatives are free to call them into question.\nThis doesn’t mean that all sets of choices are equally good. Rather, Sarewitz’ starting point is that no one set of choices is unambiguously the best. At this point, motivated reasoning can lead us go with one set rather than another, without our reasoning being flawed in any way whatsoever. Indeed, motivated reasoning can help us recognize that someone else’s findings depend on choices that they have made unconsciously.\nDouglas’ model is built on the idea of inductive risk. When we accept or reject a general hypothesis or a prediction about the future based on limited evidence, there’s always a possibility that we’ve gotten things wrong — that our sample wasn’t representative of the whole population, that some unanticipated factor changed the way things turned out. Douglas points out that getting things wrong in this way can have negative downstream consequences. For example, if we accept the hypothesis that climate change will cause massive population displacements (due to sea level rise and desertification), make serious economic sacrifices to try to forestall these displacements, and then it turns out that the hypothesis was wrong, then our serious economic sacrifices were unnecessary. Similarly, if we reject this hypothesis, do nothing to forestall the displacements, and it turns out that we’re wrong, then we’ll have massive population displacements on our hands.\nThe values that we attach to the downstream consequences of a hypothesis can and should play a role in determining how much evidence we need to accept or reject the hypothesis. If the consequences of incorrectly accepting the hypothesis are relatively minor, then we should be satisfied with relatively little and weak evidence. But if the consequences are relatively major, then we should demand much more and more stringent evidence.\nBecause of this, when everyone can agree on the values of the various consequences, we can expect agreement on how much evidence is required to accept or reject the hypothesis, and so we can expect everyone to act the same way (that is, everyone accepts it or everyone rejects it). On the other hand, when people don’t agree on the values at stake, we expect disagreement about whether we have enough evidence.\nThis could help explain why climate change is politically polarized. Liberals generally think the economic consequences of doing something about climate change will be minor, while the social and ecological consequences of not doing something will be major. Conservatives (at least pro-capitalist conservatives) generally think exactly the opposite: the economic consequences are major and the social and ecological consequences are minor. So liberals are satisfied with the available evidence concerning climate change and conservatives want more and better evidence.\nIn this explanation of the controversy, both sides are using motivated reasoning. Indeed, on Douglas’ model, motivated reasoning is absolutely necessary. Without motivated reasoning — without taking into account the significance of the consequences — we have no way to make a non-arbitrary decision about whether we have enough evidence to accept the hypothesis. Good reasoning requires emotions and values.3\nIf this explanation is right, then the controversy over whether “the science is settled” (about climate change) is disingenuous, in two ways. First, we can never be certain about climate change, and in this sense the science can never be “settled.” It’s disingenuous for conservatives to demand this, and likewise disingenuous for liberals to claim that it has been achieved. The controversy is really over whether the evidence is sufficient to accept the key claims about climate change (humans are responsible, it will have specific bad consequences, and so on). But even this is disingenuous, because liberals and conservatives are working with different standards of sufficient evidence. Due to motivated reasoning, the evidence can be both sufficient for liberals and at the same time insufficient for conservatives.\nSo motivated reasoning is not necessarily bad reasoning. Because of ambiguity and uncertainty, emotions and values have a role to play in our reasoning. But does this mean that “reasoning” degenerates into an anything-goes free-for-all? No. That will be the topic of part III.\n\n\n\n\nTo be precise, Douglas and Sarewitz write more about “value-freedom” and “objectivity” than “motivated reasoning.” But they’re closely connected. In my research, I define value-freedom as the normative ideal or principle that ethical and political values should not play a role in accepting (or rejecting) a claim. Value-freedom is one way (but only one way) of understanding objectivity. Motivated reasoning — and cultural cognition specifically — often seems to violate value-freedom. Now, Douglas and Sarewitz are both arguing that non-value-free science can still be good science. If they’re right, then in the same way motivated reasoning can still be good reasoning. ↩︎\n\n\n“We are free to do X” is shorthand here for something like “good reasoning does not require that we do not-X.” ↩︎\n\n\nWe might worry that, say, conservatives are putting too much weight on the economic consequences, and not enough on the social and ecological consequences. That is, we might worry that conservatives are working with wrong or unreasonable values. I think a weakness of both of these models is that they treat values as exogenous — values just sort of come in from beyond the scope of rational debate and disagreement. ↩︎\n\n\n\n\nMarch 11th, 2014 10:01am (philosophy of) science science and values climate change"
  },
  {
    "objectID": "posts/2015-08-21-against-science-based-policy-.html",
    "href": "posts/2015-08-21-against-science-based-policy-.html",
    "title": "Against ‘Science-Based Policy’",
    "section": "",
    "text": "Today, as part of the preparation for my AAAS science policy fellowship, I filled out a brief survey on science and public policy. Several of the questions dealt with “science-based policy,” and overall the wording of the questions assumed that policy should be “science-based.” I was uncomfortable with this language, and in this post I want to explain why.\n(I should note first that, since I already filled out the survey, I no longer have access to it, and so I can’t confirm the wording that was actually used. But I’m pretty confident that “science-based” accurately represents the way the survey talked about the relationship between science and policy.)\n“Science-based policy” suggests an asymmetrical relationship. Science is a foundation; it is firm and unshakeable, and it provides structural support to the policy that is “based on” it. It is also chronologically (and, by metaphor, logically) prior to policy. First the solid, secure foundation of science is laid; and then policy is built on top of it.\nIn his book Sustainability, Bryan Norton calls this the “serial” view of the relationship between science and policy, and he criticizes it as one of the major problems with contemporary environmental policymaking institutions (including, by name, the EPA):\n\nSerial approaches fail because they are based on a false image and an associated myth that is perhaps the greatest barrier to an improved understanding of ecosystem management. The image is that of an ideal environmental decision maker, one who has gathered all the descriptive information regarding the functioning of an ecological system; determined the likely outcomes of further impact from human activities; polled the population to determine the values, goals and preferences in good democratic fashion; and, armed with all the facts, decides what policy to pursue to maximize total welfare. (140-2)\n\nThe problem with the serial view, and thus with the language of “science-based policy,” is that it assumes policy-relevant science can be done prior to, and hence independently of, considerations of aims, goals, and values. There are at least two reasons to think this is assumption is flawed.\nFirst, policy-relevant science needs to provide knowledge and understanding concerning the variables and processes that are relevant to the aims, goals, and values of a piece of policymaking. In some policy contexts, the aims, goals, and values are widely shared, and there are institutionalized, standardized variables and processes for policy-relevant science to focus on. For example, when we’re talking about health, there’s general agreement that we’re concerned about mortality and cancer, and mortality and cancer rates are institutionalized as the variables of interest for toxicology research.\nIn other contexts, however, the aims, goals, and values are more controversial, and consequently there’s much less agreement on the appropriate variables and processes. Consider agricultural policy. There’s general agreement on productivity, as measured by yields. But there’s much less agreement on things like nutritional quality, sustainability, the economic viability of small family farms, and preserving the traditional cultures of agricultural communities. In these kinds of more contested contexts, there’s a very live risk that “policy-relevant” science will turn out to be irrelevant, incomplete, or contestable. For example, researchers might focus on certain variables — say, productivity and farmer profit margins — and neglect other variables — such as biodiversity and qualitative measures of culture.\nSecond, as I’ve noted, among philosophers who specialize in the role of values in science there’s general agreement that ethical and political values have a legitimate role to play in every stage of scientific inquiry, including the use of evidence to evaluate hypotheses. Consider the argument from inductive risk, which in recent years has been associated with the work of Heather Douglas. This argument points out that whether we have sufficient evidence to accept an hypothesis H depends, among other things, on the non-epistemic consequences of wrongly accepting H. On the one hand, if H is the hypothesis that I have enough soymilk for my tea tomorrow morning, the risks associated with H are small: at worst, I’ll have to go around the corner for some more soymilk or drink some tea that’s a little more bitter than I like. By contrast, if H is the hypothesis that bisphenol A is not an endocrine disruptor, then the risks are quite large: many people could suffer cancer or developmental disorders if we accept H and it turns out to be false.\nTo extend this argument further, I think we need to recognize that different ethical and political values and social locations can lead to different assessments of the non-epistemic consequences of accepting a hypothesis. Consider the hypothesis that humans are responsible for climate change, or anthropogenic global warming, “AGW.” For many city-dwellers, the main costs of accepting this hypothesis will be purely a matter of paying a bit more for fuel and electricity. This might mean less money to pay for other things, but as harms goes it’s not so very serious. But for people in the fossil fuel industry — and for communities that are economically dependent on the fossil fuel industry, such as in northern Alberta, West Virginia, and western Pennsylvania — accepting AGW could very likely lead to a major economic crisis. Consequently, while many city-dwellers might be led to accept AGW based on relatively little evidence, people in the fossil fuel industry and fossil fuel-dependent communities might require much more evidence to be convinced.\nIn short, policy-relevant science needs to be informed by the aims, goals, and values that are relevant to the policy process, in order to produce knowledge and understanding that’s relevant to policy and in order to be appropriately incorporate values — and potential value-based disagreements — into the evaluation of hypotheses. But this means that it can’t be carried out in a way that’s logically prior to any consideration of these aims, goals, and values. The problem with the “science-based policy” language, then, is that it seems to assume exactly the opposite.\nWhat language should we use instead? I’d suggest “science-informed policy.” This suggests a more egalitarian relationship between science and policy, and recognizes that science is only one thing that should inform good policy. (That points to another problem with the “science-based policy” language.) Most importantly for this post, “science-informed policy” recognizes that policy can also inform science, in the ways that I described above.\n\nAugust 21st, 2015 5:55pm science for policy science and values"
  },
  {
    "objectID": "posts/2013-01-30-the-funding-effect.html",
    "href": "posts/2013-01-30-the-funding-effect.html",
    "title": "The Funding Effect",
    "section": "",
    "text": "In this series of posts, I’m applying Debra Satz’ account of noxious markets to a specific aspect of commercialized science, the funding effect. In the first post, I summarized Satz’ account. In this post, I explain the funding effect.\n(Please note that this series is an experiment in publicly posting sections of a paper as I write them. I look them over briefly for typos and the like, but my ideas here are still in development.)\nThe funding effect refers to a correlation between the findings of scientific research and the financial interests of the funders of the research. Industry-funded science is often significantly more likely to have reached conclusions that are favorable to industry than non-industry funded science. In a recent paper (2012), Sheldon Krimsky has collected together several of the most prominent and well-established examples of the funding effect, including pharmaceuticals, tobacco, and bisphenol A (BPA). Another example, I think well-established by meta-analysis, is nutrition research:\nAnd several authors have speculated in print about funding effects in environmental justice research (Steel and Whyte 2012, 177) and safety testing of GMO crops (Byravan 2010). Kevin Elliott discussed the possibility of a funding effect in hormesis research in his book Is a Little Pollution Good for You?. (Elliott 2011; when I convert this series into a paper, I plan to discuss this book in more detail. For the sake of space, I won’t be doing that here.) Using the more general concept of selective ignorance, Elliott has also recently discussed something like the funding effect in agricultural research. (Elliott 2012)\nIt is important to recognize that, even when the funding effect is well documented, it is not necessarily caused by fabrication, deliberate manipulation of data, or individual scientists otherwise behaving badly. It is not even necessarily epistemologically problematic. For example, Krimsky notes that pharmaceuticals go through a long development process before a clinical trial; this process might screen out compounds that are less likely to be effective and more likely to have dangerous side effects. (Krimsky 2012, 7) Or “me-too” pharmaceuticals – yet another statin, for example – might be tested on (and marketed to) “slightly different outcomes in slightly different kinds of patients” (6, quoting Angell). This may be an ordinary instance of Simpson’s paradox due to the complexity of human physiology: for unknown reasons, the pharmaceuticals really are more effective at promoting these specific outcomes in this specific small group.\nThese mechanisms underlying the funding effect are not so worrisome. However, Elliott identifies six other, more worrisome mechanisms.\nAgain, none of these six mechanisms is best characterized or explained in terms of fabrication or manipulation of data. The influence of values that underlies the funding effect can be much more subtle than this, while still being worrisome.\nDoes that mean that the funding effect, and the various recognized and its various possible underlying mechanisms, give reason to think that the market in scientific research is noxious? That’ll be the subject of part three."
  },
  {
    "objectID": "posts/2013-01-30-the-funding-effect.html#references",
    "href": "posts/2013-01-30-the-funding-effect.html#references",
    "title": "The Funding Effect",
    "section": "References",
    "text": "References\n\nBrownell, Kelly and Kenneth Warner. “The Perils of Ignoring History: Big Tobacco Played Dirty and Millions Died. How Similar is Big Food?” The Milbank Quarterly, 87, no. 1: 259-294.\nByravan, Sujatha. “The Inter-Academy Report on Genetically Engineered Crops: Is It Making a Farce of Science?” Economic and Political Weekly, XLV, no. 43: 14-16.\nElliott, Kevin Christopher. Is a Little Pollution Good for You?. Oxford and New York: Oxford University Press, 2011.\nElliott, K C. “Selective Ignorance and Agricultural Research.” Science, Technology & Human Values. doi:10.1177/0162243912442399.\nKrimsky, S. “Do Financial Conflicts of Inteterest Bias Research? an Inquiry Into the ‘Funding Effect’ Hypothesis.” Science, Technology & Human Values. doi:10.1177/0162243912456271.\nLesser, Lenard I, Cara B Ebbeling, Merrill Goozner, David Wypij, and David S Ludwig. “Relationship Between Funding Source and Conclusion Among Nutrition-Related Scientific Articles.” PLOS Medicine 4, no. 1: e5. doi:10.1371/journal.pmed.0040005.\nRonald, Pamela and Raoul Adamchak. Tomorrow’s Table. Oxford and New York: Oxford University Press, 2008.\nSteel, Daniel, and Kyle Powys Whyte. “Environmental Justice, Values, and Scientific Expertise.” Kennedy Institute of Ethics Journal 22, no. 2: 163–182.\n\n\nJanuary 30th, 2013 9:07am (philosophy of) science economics"
  },
  {
    "objectID": "posts/2023-02-08-demagoguery.html",
    "href": "posts/2023-02-08-demagoguery.html",
    "title": "Constructivist accounts of representation, in science and politics",
    "section": "",
    "text": "Political theorists developed a constructivist approach to political representation around 2010; Saward (2010) seems to be the canonical citation for this approach, but I think M. B. Brown (2009) is doing something quite similar. Prior to 2010, work by political theorists on representation focused on either (a) formal procedures of elections and voting or (b) deliberative practices, either in civil society (that is, outside of government) or in boundary institutions that connected public deliberation to policymakers. Recently several philosophers of science have picked up ideas from the latter (citations in a previous post).\nConstructivists challenge an implicit assumption of these approaches, namely, the there is a constituency or public that exists prior to representation and has more-or-less well-defined interests or preferences. This assumption might (might) make sense when we’re talking about legislative districts: the State of California existed before Diane Feinstein was first elected Senator in 1992. But it doesn’t work as well for less formal types of representation, for example, Martin Luther King, Jr., or Greta Thunberg as representatives of social movements.\nSaward (2010) apparently (I haven’t actually read the book) draws on Gricean philosophy of language that will be familiar to most philosophers today. Specifically, Saward (2010) examines representative claims as speech acts, claims that the speaker represents a certain public or constituency to an audience in a certain context and that this constituency believes or wants or does something. These acts are successful insofar as they are taken up by the audience. In other words, to be a representative for a constituency is nothing more or less than to be taken as a representative for that constituency.\nImportantly, the constituency does not need to exist for a representative claim to be successful. One practical effect of the speech act might be that the constituency comes into existence as members of the audience identify themselves as members of that constituency. For example, what we now call the “MAGA movement” or “MAGA Republicans” didn’t exist until after Trump presented himself as a representative for that then-unnamed constituency in 2015. Drawing on Hobbes, M. B. Brown (2009) calls this virtual representation1, and he uses it to develop an account of (natural) scientists as representatives of nature in policy contexts. Even if it makes sense to talk about “nature” as a single coherent entity, that entity can’t do something like authorize a scientist as its representative through a formal election or have beliefs or interests that are accurately or inaccurately represented by a scientist’s representative claims.\nUrbinati (2018) discusses this constructivist approach to political representation. I found her writing somewhat difficult to follow, so I’m not going to try to fully reconstruct her views. But she does seem to present three objections to constructivism:\nThe first objection echoes Young (2000)’s critique of deliberative democracy, which (prior to Young’s critique) tended to focus on idealized deliberative scenarios and norms of deliberation that favored highly educated white men. The third objection seems to provide the best support for Urbinati (2018)’s “diarchic” approach, which emphasizes the relationship between formal institutions of political power and the open-ended deliberation of civil society. (I suspect some of the later contributions to this collection will do this as well.)\nOn the “pre-constructivist” models of representation, representatives are primarily accountable to the constituency that they represent. This might be done formally — the representative needs to stand for (re)election — or it might happen informally — as constituents call representatives to give an account of their actions qua representatives.2 Urbinati’s point seems to be that, by basing legitimacy on audience uptake, constructivists lose the demand for democratic accountability. Audience uptake matters, and representative and constituency co-construct each other (again, consider Trump), but good representatives must still represent their constituents accurately.\nFor M. B. Brown (2009), a constructivist account of scientific representation (scientists as representatives of nature) is attractive because democratic accountability to “nature” seems to be nonsensical.3 So if scientists are to be held accountable to anyone besides themselves, the audience seems to be the only option.\nIn the context of Brown’s account, we might understand Urbinati’s second concern as a critique of “wishful thinking” or “anything-goes” relativism: nothing seems to be stopping the audience from rejecting scientists whose claims are inconvenient or unfavorable (no matter how well-supported), and accepting scientists whose claims are convenient and favorable (no matter how flimsy) (Sarewitz 2004; Steel 2017; Lichtenstein 2022).\nOne constructivist response to this critique might be to reject the notion of accuracy, as in the formulation “good representatives must still represent their constituents accurately.” Such a constructivist might argue that “accuracy” assumes a pre-existing constituency; but representative claims are prior to constituencies, etc. On the science side, this leads to the interminable and fatally abstract debates over “realism” of the 1970s and ’80s.\nAn alternative approach might draw on Dewey’s account of inquiry, specifically as articulated by M. Brown (2020). On this account, inquiry starts with a problematic situation — things aren’t working — and inquiry is successful insofar as it resolves the situation — develops alternative things that do work. Importantly, “working” here has both “realist” and “constructivist” aspects. It’s realist insofar as the agent can incorrectly believe that a certain alternative will work when “in fact” it will not. But it’s also constructivist insofar as one tactic to resolve the situation is for the agent to change their understanding of the activity and its goals, and resolving the situation does not require a “true” representation of the world (in a classical correspondence sense).4\nI suggest we can view political representation as “inquiry” using this account. The problematic situation is that some people5 have some felt dissatisfaction with the status quo. A representative claim is then a hypothesis that there is a group (the constituency) whose interests are not being served by the policy status quo (the diagnosis), and that adopting a different policy will resolve the problematic situation (the remedy).\nOn this model, a representative claim will have locutionary success criteria, that is, it needs to be taken up by the audience, especially people with the power to enact the proposed policy. And the constituency can be constructed by the representative claim, insofar as the “some people” subsequently decide that they are indeed members of the group and their interests are not being served by the status quo. But the representative claim also has pragmatic success criteria: would the remedy actually resolve the problematic situation as characterized by the diagnosis? This is unlikely to be the case if, for example, there is good evidence that the felt dissatisfaction experienced by group members is not caused by the factors identified in the diagnosis.\nConsider how this model might work for Trump’s 2016 campaign. We can reconstruct the central representative claim of Trump’s campaign as something like the hypothesis that rural poverty is caused by mass undocumented immigration and so strong immigration restrictions will solve this problem. This claim was taken up by the media, who did numerous profiles of “Trump voters” (the newly-constructed constituency) in impoverished rural areas and their attitudes towards immigration. So it was evidently a locutionary success. But its pragmatic success is tenuous at best; agricultural consolidation and demographic shifts (depopulation and aging as young adults go to college and move to urban areas) are probably much more important causes of rural poverty than immigration, undocumented or otherwise."
  },
  {
    "objectID": "posts/2023-02-08-demagoguery.html#footnotes",
    "href": "posts/2023-02-08-demagoguery.html#footnotes",
    "title": "Constructivist accounts of representation, in science and politics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr maybe “fictional representation”? Need to check.↩︎\nYoung cite here?↩︎\nYesterday I was preparing Simpson (2014) as a reading on Indigenous epistemology and land relations for the Environmental Philosophy course that I’m teaching this semester. From the perspective of Indigenous land relations, accountability to “nature” — or rather, nonhuman agents — does make sense. Indeed, as presented by various Indigenous authors (Simpson 2014; Liboiron 2021) a key idea in Indigenous knowledge-making practices is consent and collaboration with nonhuman agents. I’m going to bracket this thought for the purposes of this post.↩︎\nSorry Matt Brown, nobody else thinks Dewey was a correspondence theorist.↩︎\nThis should be understood as a Boolosean plural quantifier: “there are some \\(x\\)s,” rather than “there is a set \\(X\\).”↩︎"
  },
  {
    "objectID": "posts/2023-03-22-endorsement.html",
    "href": "posts/2023-03-22-endorsement.html",
    "title": "The Nature endorsement and the value-free ideal",
    "section": "",
    "text": "In October 2020, Nature published an editorial endorsing Joe Biden and criticizing Trump (“Why Nature Supports Joe Biden for US President” 2020). A few days ago, Nature Human Behavior published an online survey experimental study (Zhang 2023) that appeared to find that “some who saw the endorsement lost confidence in the journal as a result” (Lupia 2023). The study has prompted a lot of discussion on social media, especially after the editors of Nature stood by their decision to endorse Biden (“Should Nature Endorse Political Candidates? Yes — When the Occasion Demands It” 2023) and Holden Thorp, editor-in-chief of Science, posted a short Twitter thread in agreement.\nIn many of these discussions, Zhang (2023) is being interpreted in terms of the value-free ideal (VFI). This interpretation is mistaken, because the Nature endorsement doesn’t violate VFI. It does violate a more general principle, but by putting Zhang (2023) in the context of my own empirical research I argue that violations of this principle don’t explain the headline findings. I develop a better explanation using another branch of my research, developing the “aims approach” to values in science."
  },
  {
    "objectID": "posts/2023-03-22-endorsement.html#footnotes",
    "href": "posts/2023-03-22-endorsement.html#footnotes",
    "title": "The Nature endorsement and the value-free ideal",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically “at,” not “for.”↩︎\nIt’s common in the literature to use “non-epistemic values” interchangeably with something like “social and political values.” But I think this is incorrect, because — on my preferred definition of epistemic values, namely, features of scientific practice that promote the pursuit of epistemic aims such as truth and understanding — social and political values can sometimes be epistemic (Hicks 2022, esp. pages 1-2).↩︎\nThis is actually tricky in the Nature endorsement, because their value judgments are bundled into thick claims. For example, one of their premises is that Trump’s management of the pandemic was “disastrous.” This is a thick concept, with both descriptive criteria for application and evaluative implications: Trump’s handling of the pandemic resulted in hundreds of thousands of preventable deaths, and this is a severely bad thing. (Note that “preventable death” is another thick concept.) The controversy here isn’t over the evaluative side, the idea that preventable deaths are bad. Instead it’s over the descriptive side: that Trump’s handling of the pandemic had these effects.↩︎\nSpecifically, someone might try to concede VFI and preserve value neutrality by rejecting the alignment between epistemic/non-epistemic and neutral/controversial values. Schroeder (n.d.) might potentially be read in this way: democratically-endorsed non-epistemic values are treated as not controversial, and so appealing to them does not violate neutrality.↩︎\nI’m confused by the scale used in reporting this effect, so I’m just calling it “substantial” here.↩︎\nVarying these independently means that we can distinguish any effects related to the value judgment from effects of the conclusion. The design in Zhang (2023) can’t do this, because in a sense the value judgment is the conclusion.↩︎\nOne incidental concern about Zhang (2023) is that they used a single item for every variable of interest. They did ask about two aspects of trustworthiness, whether the trustee was informed and whether they were biased. But that was done using exactly two questions, and responses weren’t combined. Psychologists seem to prefer multi-item instruments. For example, the METI has 14 items covering three aspects of trustworthiness: competence, benevolence, and integrity. I’m not entirely sure why psychologists prefer multi-item instruments, though, so I decided not to discuss it in the body of the post.↩︎\nElliott et al. (2017) report a second study with a policy conclusion, namely, whether BPA should be regulated. Due to limited funds my collaborator and I didn’t replicate this part.↩︎"
  },
  {
    "objectID": "posts/2021-03-19-open-science-i.html",
    "href": "posts/2021-03-19-open-science-i.html",
    "title": "Talk: Open science can’t solve the replication crisis",
    "section": "",
    "text": "I gave this talk yesterday at the Digital Studies for Digital Science (DS2) online conference. You can watch the recording on YouTube."
  },
  {
    "objectID": "posts/2021-03-19-open-science-i.html#open-science-cant-solve-the-replication-crisis",
    "href": "posts/2021-03-19-open-science-i.html#open-science-cant-solve-the-replication-crisis",
    "title": "Talk: Open science can’t solve the replication crisis",
    "section": "Open science can’t solve the replication crisis",
    "text": "Open science can’t solve the replication crisis\nIn the last few years, concerns about the replication crisis in biomedicine and social psychology have bolstered the open science movement, and played a significant role in arguing for open science requirements at scholarly journals and even government agencies — as in the case of the US Environmental Protection Agency’s “Strengthening Transparency in Regulatory Science” proposed rule. However, the discourse surrounding the replication crisis frequently conflates two very distinct desiderata of experimental-computational science, namely, replicability and reproducibility. Following definitions proposed by the US National Academies, reproducibility is a purely computational notation: the ability of an independent researcher to produce numerically identical output from an analysis, using the same data and analysis code. By contrast, replicability is the ability of an independent researcher to reach qualitatively similar conclusions by repeating an experiment, using the same analytical approach but collecting different data. I first argue that reproducibility has minimal (but non-zero) epistemic value, comparable to mere logical consistency. Next I survey a variety of proposed causes for the replication crisis: p-hacking, publication bias, insufficient statistical power, unrepresentative samples, publish-or-perish incentive structures, noisy measurement, underspecified phenomena, imprecise theory, data mismanagement, software bugs, and fraud. I argue that open science requirements effective promotely reproducibility, but promote replicability only insofar as replication failures are due to causes that leave traces (as in historical science) in data and code. Because very few of the proposed causes leave such traces, open science cannot solve the replication crisis."
  },
  {
    "objectID": "posts/2018-01-01-old-posts.html",
    "href": "posts/2018-01-01-old-posts.html",
    "title": "Old posts",
    "section": "",
    "text": "Posts dated prior to 2018 were migrated from an old Tumblr site during a website redesign in September-October 2018. I selected the posts to migrate based on a quick skim of their contents; they generally reflect my current views, but I did not necessarily read them carefully. In a few cases I noticed and updated some language that I now recognize as problematic (mostly some trans-insensitive phrasing). But there are probably cases that I overlooked in my quick review. Also, the posts contain Tumblr timestamps and tags, and sometimes some uncorrected formatting errors.\nTumblr HTML files were converted to Markdown using Pandoc, then the following R script was used to adjust the Markdown formatting for Jekyll.\nlibrary(tidyverse)\n\ndataf = tibble(file = list.files(path = '.', '*.md'))\n\ndataf %&gt;%\n    mutate(text = map_chr(file, read_file)) %&gt;%\n    separate(col = text, into = c('title', 'body'), \n             sep = '[=]{2,}') %&gt;% \n    mutate(title = str_replace(title, '\\n', '')) %&gt;%\n    ## Parsing\n    mutate(title_san = str_replace_all(tolower(title), '[^[:word:]]+', '-'),\n           body_nobreaks = str_replace_all(body, '\\n', ' '),\n           dt_str = str_extract(body_nobreaks, \n                                  regex('::: \\\\{#footer\\\\}.*')), \n           dt_str = str_extract(dt_str, '\\\\[[^\\\\]]*\\\\]'), \n           dt_parsed = parse_datetime(dt_str, \n                                     format = '[ %B %d%.%., %Y %I:%M%p ]'), \n           date = lubridate::date(dt_parsed)) %&gt;%\n    ## Output values\n    mutate(text_out = str_c('---\\n',\n                            'layout: post\\n',\n                            'title: \"', title, '\"\\n',\n                            '---\\n', \n                            body),\n           filename = str_c(date, '-', title_san, '.md')) %&gt;% \n    rowwise() %&gt;%\n    mutate(out = write_file(text_out, path = filename))"
  },
  {
    "objectID": "posts/2013-09-06-objectivity-is-a-unicorn.html",
    "href": "posts/2013-09-06-objectivity-is-a-unicorn.html",
    "title": "Objectivity is a Unicorn",
    "section": "",
    "text": "Following up on my last post and a semi-related conversation with a new officemate, it seems to me that a lot of people might react something like this:\n\nWell, no surprise that we can’t trust these scientists: They have ties to Monsanto. What we need are objective, disinterested scientists who aren’t dependent on industry sponsors.\n\nIn this post I’m going to criticize this response. As the title puts it, objectivity is a unicorn. It doesn’t exist, it’s a myth, and so it’s not going to help us solve the problem.\nThat’s hyperbolic, for a few reasons. First, there are a few things we might mean by “objectivity.” Over the past few decades, philosophers have developed several different conceptions of objectivity. The one I’m targeting here is a “classical” notion of objectivity as disinterestedness or value-freedom. Specifically, I’m going to argue that the way we answer factual, scientific questions (like whether GM maize is more productive) depend on value judgments.1 Second, there are a few areas of science where this isn’t so much the case and it’s (more or less) possible to achieve disinterestedness. Pure mathematics and theoretical physics are good examples — though even here we might have questions about “physics fundamentalism.”\nTo make things concrete, I’m going to focus on one seemingly-simple, factual question: Is GM maize more productive than non-GM maize? My argument, in brief, is that, whenever we investigate something as complex as the food system, before we answer this question we need to settle numerous methodological issues — we need to make a lot of assumptions. And the way we settle these issues — the specific assumptions scientists make when they try to answer this question — depend on value judgments. Hence, the way we answer this question depends on value judgments.2\nIndeed, when it comes to questions like the productivity of GM maize, the value judgments are often deeply controversial. As I’ll argue below, the assumptions scientists make when they evaluate crop productivity are closely related to assumptions about the kind of food system we should have. The upshot is that we need something like what philosopher Matt Brown calls “the democratic control of the scientific control of politics,” which brings scientific and political concerns together more deliberately and (hopefully) constructively, rather than — as the suggestion at the top of the post would have it — trying to separate them. Rather than chasing mythical objectivity, we need new institutions that integrate science and democracy.\nNow to the details of the argument. Again, I’m looking at one seemingly-simple, factual question: Is GM maize more productive than non-GM maize? There are at least five different kinds of methodological issues and assumptions that we need to settle before we can answer this question.\nThe first kind of issue to settle is which GM traits. The two most commonly used GM traits are resistance to the herbicide glyphosate (RoundUp, manufactured by Monsanto) and the production of an insecticide using genes from a bacterium (this trait is called Bt, for the scientific name of the bacterium). (In the last post, the paper was looking at Bt maize and cotton.) Other traits that are available for farmers to grow today and in the near future, but aren’t used as much, include drought tolerance, disease resistance, and enhanced nutritional content. Clearly, the productivity of GM maize depends a lot on exactly which traits we’re talking about — drought tolerance might boost productivity a lot, but herbicide-tolerance probably won’t.\nThe second kind of issue is the range of ecological contexts for the maize. Different places have different insect populations, and the Bt toxin is effective against some kinds of pests but not others. Likewise with the presence of pathogens. Water, soil chemistry and structure, and temperature (both in general and over the course of a particular growing season) can make a big difference. And maize can produce more or less in the presence of various kinds of plants — the closer the maize seeds are planted the more plants per unit of land, but if they’re too close they can be smaller or less productive per plant; weeds might produce soil toxins or shade maize sprouts that need lots of sunlight. There’s also the question of whether we’re looking at monocultured maize — big fields of nearly-genetically identical plants — or polycultures — which typically involve smaller fields and growing several different kinds of plants together.\nThese ecological contexts can be managed by various agricultural practices, which is the third set of issues. Indeed, from a certain perspective, farming just is managing ecology. At this point, the dependence on value judgments starts to become obvious. If we’re very concerned about the environmental impacts of agriculture — things like fertilizer runoff, the effects of pesticides on non-targeted insects and other animals, and the use of fossil fuels to grow and transport food — we might value organic over conventional, industrialized agriculture. Or, if we’re employees of a company like Monsanto, we might have a strong values-based preference for the conventional, industrial status quo. Locavores and traditionalists value an agricultural system with many small-scale farmers using inexpensive but labor-intensive methods; while people who are most concerned about economic efficiency would value a system with fewer, large-scale farmers using advanced (and expensive) technology to minimize labor expenses. There are also relevant differences between the highly-industrialized, commercial system in core countries like Canada and the US; semi-industrialized commercial agriculture in periphery and semi-periphery regions; and subsistence agriculture in periphery and maginal regions. For example, access to capital — and so access to advanced technology — varies widely between (and within) these three different systems.\nIt might seem like, since GM maize is going to be grown in a highly-industrialized commercial system, these questions are irrelevant. But the productivity question is comparative: is GM maize more productive than the non-GM alternative? And so the way we answer this question depends on what the non-GM alternatives are. GM maize is going to be more productive than maize grown in subsistence agriculture, but because it’s grown in a more intensive agricultural system, not because it’s genetically modified. Once we raise the possibility of these kinds of comparisons — which I think we should — we should also raise the possibility that the productivity question isn’t even the right question to ask.\nReturning to our list, a closely-related fourth set of issues concerns political economy: Who owns the farmland, who decides how it’s farmed, and what happens to the maize after it’s harvested? For instance, is this maize being grown as a staple food for humans, or will it be processed into high fructose corn syrup and livestock feed? Again, while these issues might not seem directly relevant to the productivity question, they raise issues of what alternatives we’re comparing the GM maize against and what kind of broader food system we’re assuming.\nThe question of use is also closely related to the fifth and perhaps most scientifically fundamental set of issues, measurement conventions. We can take any ratio between outputs and inputs to be our measure of productivity. And there are a lot of different ways we might measure the outputs of GM maize: tons (weight), bushels (volume), farmer income (economic), human-edible calories of the raw maize, human-edible calories after processing, cattle-edible calories (cattle can digest parts of the plant that we can’t), and so on. Likewise, there are a lot of different ways we might measure inputs: per acre (land), per weight of chemical inputs, per farmer expenses (capital), per hour of worker labor. Bushels per acre or hectare are common measures, but if we’re interested in feeding the world and the environmental impacts of food production it would probably make more sense to look at calories produced using various land and chemical inputs.\nOnce we’ve decided how we’re going to measure productivity, we need to decide how to handle variation and uncertainty. Productivity will not be exactly the same in all places and times. Will we report an average, or some more complicated piece of statistics, such as a confidence interval or regression analysis? The more complicated statistics can give us a more sophisticated understanding of how productive GM maize is, but themselves rely on assumptions that bring in further value-judgments. For example, if we decide to use a confidence interval, we need to decide what confidence levels to use; and this decision requires taking various risks into account.3 We also need to take into account the fact that no measurement is perfect. Statistical techniques can help to reduce uncertainty but not completely eliminate it, and these techniques often involve tradeoffs among different kinds of uncertainty. We need to decide how much of each kind of uncertainty we can tolerate, which depends on our purposes — the reasons why we’re asking whether GM maize is more productive in the first place — and so depends on the value-judgments that are motivating the research.\nIn this post, I focused on a single, seemingly-simple, factual question: Is GM maize more productive than non-GM maize? I showed how answering this question depends on settling at least five different kinds of issues or assumptions: the kinds of GM traits, the range of ecological contexts and agricultural practices, the political economy of food production, and measurement conventions. For each of these, I argued that the complexity of the issue requires bringing in value judgments before we can answer the productivity question. Thus an objective answer to the question — in the sense of disinterestedness or value-freedom — is impossible. Objectivity is as mythical as a unicorn. Consequently, we need to find alternative ways to manage the relationship between science and policy.\n\n\n\n\nFor philosophers: I’m writing this post for a non-philosophical audience, so I’m not going to spell out “value judgments” in more detail, or distinguish epistemic, internal, or constitutive from their usual constrasts. As someone with one appendange in ethics, I actually think the way philosophers of science talk about “values” is problematic. ↩︎\n\n\nBy no means is this argument original. One nice version that’s probably accessible for non-academics is here. ↩︎\n\n\nFor more on risk, values, and the relationship between science and policy, check out the work of philosopher Heather Douglas. ↩︎\n\n\n\n\nSeptember 6th, 2013 1:20pm (philosophy of) science food gmo controversy"
  },
  {
    "objectID": "posts/2023-04-07-pluralism-problems.html",
    "href": "posts/2023-04-07-pluralism-problems.html",
    "title": "Pluralism about problematic situations",
    "section": "",
    "text": "Note\n\n\n\nMatt Brown has some commentary here.\n\n\nI’ve been invited to give a keynote on Matt Brown’s work at the UT Dallas conference in May. Brown (2013) fits with my current project (as seen in the recent blog posts), so this post is maybe the first of a sub-series of posts on his paper. All headless citations are to Brown (2013).\nBrown first argues that there is a tension between the autonomy (lack of accountability) of science and its social and political authority in a democratic society. He quotes Douglas:\n\n[A]n autonomous and authoritative science is intolerable …. A fully autonomous and authoritative science is too powerful, with no attendant responsibility (Douglas 2009, 7–8, as quoted by Brown, 481)\n\nScholars and activists who have been sensitive to this tension — he names Feyerabend and STS scholars; we could include many feminists, anti-racists, and Marxists — “have tended to challenge the … authority of science …. weakening or denying the existence of expertise in politics” (481). This is objectionable because it amounts to “giv[ing] up tools in policy-making that we cannot do without, … given the remarkable success of science” (481-2).\nHere, and elsewhere in the paper, Brown links the epistemic credibility or reliability of science to its political authority. Science should have authority in policymaking because it is an especially (the most?) reliable way of producing knowledge. But the kind or degree of political authority that Brown has in mind is vague. It’s one thing to say that scientific findings or methods can be useful in many policymaking situations, and so typically there should be a deliberate effort to solicit advice from scientists; it’s another to say that science should have some kind of priority or greater standing over other considerations.\nI argue that Brown’s own account of inquiry should lead us towards the former formulation, of science as frequently useful rather than science taking priority. On Brown’s account, inquiry (scientific or otherwise) is a systematic response to a problematic situation, a situation in which we (we’ll come back to this “we”) realize that our established habits fail to realize their aims. Inquiry is considered successful insofar as it establishes a better fit between habits and aims; this might involve developing completely new habits, tweaking old habits, and/or reconstituting the aims.\nA number of different kinds of controversies can emerge from this simple framework. In aim controversies, parties disagree over the aims that our habits are supposed to be realizing. Diagnostic controversies concern the explanation for why the habits are failing to realize their aims. We can even have problem controversies, in which parties disagree over whether we are in a problematic situation at all. Brown gives the example of US policy debates surrounding health care reform in 2009-2010 (483-4), which featured each of these kinds of controversies.\nWhen parties disagree with each other in these kinds of ways, they will disagree with each other about whether a line of inquiry should be considered relevant to a policy problem, and thus whether it should be counted as successful with respect to the problematic situation at hand. For example, consider an economic analysis that finds that a combination of employer and individual health insurance mandates and a means-tested subsidy for private insurance will tend to increase health insurance coverage. This line of inquiry might be considered successful by someone whose diagnosis of the problem with the US health care system is that private insurance is too expensive, but not by people who think the problem is a profit-driven healthcare industry or creeping government overreach. For the latter two groups of people, even if the economic analysis is highly reliable it is not relevant to developing a solution to the problem (Cartwright and Hardie 2012; Hicks 2015).\nIn other words, the “we” who find ourselves in a problematic situation can often be diverse in ways that lead “us” to different ways of understanding the nature of the problem. Giving “science” priority or greater standing over other considerations amounts to giving one group or perspective priority over the others (Feyerabend 1975; Longino 1987; Lacey 2014; Hicks 2017).\nThis analysis might seem to threaten to degenerate into anything-goes relativism, with partisans cherry-picking the science that fits their preferred account of the problem (Sarewitz 2004). But I think we can avoid relativism by viewing a policy controversy as itself a problematic situation prompting inquiry. In other words, we can use scientific inquiry as a way to understand and thereby navigate political conflict, not as a cudgel that one side gets to use against the others.\nConsider climate change. Climate scientists have raised concerns in public about the impacts of carbon dioxide emissions for over 35 years, but policy responses have been faltering at best and public attitudes seem deeply divided. Why is this, and what could be done to get a better match between the apparent problem and a policy solution? Scholars in several fields of the social sciences and humanities have tackled this question for at least 20 years, working to understand the roles of fossil fuel companies (Oreskes and Conway 2011; Supran and Oreskes 2017); uncertainties in climate science, both real and manufactured (Lloyd 2015); public perceptions of and attitudes towards climate change (Lewandowsky, Cook, and Lloyd 2016; Leiserowitz et al. 2021); and how well political representatives understand the views and preferences of their constituents (Bolsen, Druckman, and Cook 2015; Hertel-Fernandez, Mildenberger, and Stokes 2019). Taken together, this body of work supports a model on which there is strong public support for climate policy, even among Republican/conservative voters, but that the fossil fuel industry’s propaganda efforts have succeeded in creating the misleading appearance of scientific and public controversy, what Sparkman, Geiger, and Weber (2022) call a “false social reality: a near universal perception of public opinion that is the opposite of true public sentiment.”\nMuch like using physical climate models to model future flood risks (Parker and Lusk 2019; Lusk 2020), this body of social scientific-humanistic climate research is useful without claiming priority over other concerns. By understanding key actors and constituencies in the climate controversy, policymakers can design policies that are more likely to gather the support of broad coalitions. For example, a distinct feature of Ocasio-Cortez and Markey’s 2019 Green New Deal resolution was its call for a “just transition,” enabling fossil fuel workers to gradually move into other industries or take early retirement. These ideas played a significant role in the 2021 American Jobs Plan (Sicotte 2021), and thereby influenced the Bipartisan Infrastructure Bill.\n\n\n\n\nReferences\n\nBolsen, Toby, James N. Druckman, and Fay Lomax Cook. 2015. “Citizens’, Scientists’, and Policy Advisors’ Beliefs about Global Warming.” The ANNALS of the American Academy of Political and Social Science 658 (1): 271–95. https://doi.org/10.1177/0002716214558393.\n\n\nBrown, Matthew J. 2013. “The Democratic Control of the Scientific Control of Politics.” In The European Philosophy of Science Association Proceedings, edited by Vassilios Karakostas and Dennis Dieks, 2:479–91. Springer International Publishing. http://dx.doi.org/10.1007/978-3-319-01306-0_39.\n\n\nCartwright, Nancy, and Jeremy Hardie. 2012. Evidence-Based Policy: A Practical Guide to Doing It Better. Oxford University Press. http://books.google.ca/books?id=kN3OdpkUyTcC&printsec=frontcover&dq=intitle:Evidence+Based+Policy+A+Practical+Guide+to+Doing+It+Better&hl=&cd=1&source=gbs_api.\n\n\nDouglas, Heather E. 2009. Science, Policy, and the Value-Free Ideal. Pittsburgh, Pa: University of Pittsburgh Press.\n\n\nFeyerabend, Paul. 1975. “How to Defend Society Against Science.” Radical Philosophy, no. 011 (Summer): 3–8.\n\n\nHertel-Fernandez, Alexander, Matto Mildenberger, and Leah C. Stokes. 2019. “Legislative Staff and Representation in Congress.” American Political Science Review 113 (1): 1–18. https://doi.org/10.1017/S0003055418000606.\n\n\nHicks, Daniel J. 2015. “Epistemological Depth in a GM Crops Controversy.” Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences 50 (April): 1–12. https://doi.org/10.1016/j.shpsc.2015.02.002.\n\n\n———. 2017. “Genetically Modified Crops, Inclusion, and Democracy.” Perspectives on Science 25 (4): 488–520. https://doi.org/10.1162/POSC_a_00251.\n\n\nLacey, Hugh. 2014. “Scientific Research, Technological Innovation and the Agenda of Social Justice, Democratic Participation and Sustainability.” Scientiae Studia 12 (SPE): 37–55. https://doi.org/10.1590/S1678-31662014000400003.\n\n\nLeiserowitz, Anthony, Connie Roser-Renouf, Jennifer Marlon, and Edward Maibach. 2021. “Global Warming’s Six Americas: A Review and Recommendations for Climate Change Communication.” Current Opinion in Behavioral Sciences, Human Response to Climate Change: From Neurons to Collective Action, 42 (December): 97–103. https://doi.org/10.1016/j.cobeha.2021.04.007.\n\n\nLewandowsky, Stephan, John Cook, and Elisabeth Lloyd. 2016. “The ‘Alice in Wonderland’ Mechanics of the Rejection of (Climate) Science: Simulating Coherence by Conspiracism.” Synthese, September, 1–22. https://doi.org/10.1007/s11229-016-1198-6.\n\n\nLloyd, Elisabeth A. 2015. “Model Robustness as a Confirmatory Virtue: The Case of Climate Science.” Studies in History and Philosophy of Science Part A 49 (February): 58–68. https://doi.org/10.1016/j.shpsa.2014.12.002.\n\n\nLongino, Helen E. 1987. “Can There Be A Feminist Science?” Hypatia 2 (3): 51–64. https://doi.org/10.1111/j.1527-2001.1987.tb01341.x.\n\n\nLusk, Greg. 2020. “Political Legitimacy in the Democratic View: The Case of Climate Services.” Philosophy of Science, July. https://doi.org/10.1086/710803.\n\n\nOreskes, Naomi, and Erik M. Conway. 2011. Merchants of Doubt: How a Handful of Scientists Obscured the Truth on Issues from Tobacco Smoke to Global Warming. New York, NY, USA: Bloomsbury.\n\n\nParker, Wendy S., and Greg Lusk. 2019. “Incorporating User Values into Climate Services.” Bulletin of the American Meteorological Society 100 (9): 1643–50. https://doi.org/10.1175/BAMS-D-17-0325.1.\n\n\nSarewitz, Daniel. 2004. “How Science Makes Environmental Controversies Worse.” Environmental Science and Policy 7 (5): 385–403. https://doi.org/10.1016/j.envsci.2004.06.001.\n\n\nSicotte, Diane. 2021. “Will the Green New Deal Bring About a ‘Just Transition,’ or Just Transition? - American Sociological Association.” Summer 2021. https://www.asanet.org/footnotes-article/will-green-new-deal-bring-about-just-transition-or-just-transition/.\n\n\nSparkman, Gregg, Nathan Geiger, and Elke U. Weber. 2022. “Americans Experience a False Social Reality by Underestimating Popular Climate Policy Support by Nearly Half.” Nature Communications 13 (1): 4779. https://doi.org/10.1038/s41467-022-32412-y.\n\n\nSupran, Geoffrey, and Naomi Oreskes. 2017. “Assessing ExxonMobil’s Climate Change Communications (1977–2014).” Environmental Research Letters 12 (8): 084019. https://doi.org/10.1088/1748-9326/aa815f."
  },
  {
    "objectID": "posts/2013-02-23-why-local-.html",
    "href": "posts/2013-02-23-why-local-.html",
    "title": "Why Local?",
    "section": "",
    "text": "[I feel like my blog posts tend to be pretty technical and academic. This post is an attempt to write for a broader audience, while still being philosophical.]\nWhy should we eat local food?\nTo answer the question, we need to start by clarifying it. “Local” is tricky, so we’re going to come back to it. Let’s start with the “should.”\nWe sometimes use “should” to name an absolute requirement or prohibition — what philosophers call a “categorical imperative.” For example, you shouldn’t go around murdering people. Not “unless you really want to” or “except in special circumstances X, Y, and Z”; murder is simply — absolutely, or categorically — something you shouldn’t do. Other times, we use “should” to give practical advice: if I’m cold and want to warm up, I should go put on a sweater.\nThe “should” in the question “Why should we eat local food?” is somewhere between these extremes. It doesn’t strictly prohibit coffee and chocolate — especially coffee and chocolate, that’s sustainably grown by fairly-paid workers — and so it’s not an absolute, exceptionless, categorical rule.\nBut it’s also not merely a piece of practical advice. If you want to count all the grains of sand between Michigan City and Beverly Shores, in the practical advice sense you should use cardboard partitions to divide the beach into 3’ by 3’ squares. However, in another sense, you should not waste your time counting the grains of sand in the first place.\nThe “should” in “we should eat local food,” like the “should” in “you shouldn’t waste your time counting grains of sand,” isn’t advice about how to satisfy some desire you happen to have, however bizarre (or not) that desire might be. Instead, these two “shoulds” give advice about how to live a good, worthwhile, and fulfilling life. You shouldn’t count the grains of sand because it’s a waste of time: you have better, more worthwhile, more fulfilling things to do. And, similarly, we should eat local food because, generally speaking, it’s better, more worthwhile, and more fulfilling than the alternative.\nNotice that we have part of the answer to our question already. But why is local food better, more worthwhile, and more fulfilling? To answer that, we need to clarify “local.”\n“Local” might mean something like “within four hundred miles” or “a truck can deliver it the day after it’s picked.” Some large retail stores define “local” in this way. But it’s hard to see why local food, in this sense, is better than the alternative. From my apartment in South Bend, it’s an easy bike ride to monocultured corn fields that get sprayed with pesticides and herbicides every few weeks, or “barns” filled with hundreds of hogs. Meanwhile, somewhere in China (though fewer and fewer places each year), there’s a peasant farmer raising a diverse group of plants and animals, using methods that have kept her land healthy and productive for a few thousand years. It’s true that the peasant’s farm is more labor intensive, and getting her food to my plate involves some significant food miles. But “labor intensive” is a dysphemism (that’s the opposite of a euphemism) for “employed farmers,” and the ecological costs of shipping food across the ocean are actually quite a bit less than the ecological costs of synthetic pesticides and fertilizers — cargo ships and trains are pretty fuel-efficient.\nBy “local,” I mean something more like “knowing the people who grow and produce your food.” Local, in this personal sense, generally requires local in the geographical sense: it’s going to be difficult to know the people who grow your coffee if you’re in Indiana and they’re in Colombia. My co-op defines “local” in terms of a 60-mile radius around South Bend. While this sounds like a geographical definition, the idea is that a producer who’s less than 60 miles away can personally come to distribution on Wednesdays, which gives them a chance to meet consumers. The transfer of food from grower to consumer is face-to-face, and often at the same time there’s a transfer of gossip, local news, and updates on kids and family. It’s still an economic transaction, but it becomes more than this.\nLocal food, in this community sense, is better, more worthwhile, and more fulfilling than the alternative in at least three ways. First, if you personally know the people who grow and produce your food, you’re in a better position to trust them. Many of the labels you see at the supermarket, like “all natural,” are basically unregulated. Others, like “organic,” are regulated, but the regulations might not cover everything you’re concerned about. For example, organic regulation doesn’t cover animal welfare or working conditions. Second, the money you spend goes directly to the food producer, who in turn can use it to support other local businesses. Rather than being siphoned off to corporate headquarters, wherever they might be, your food dollars maintain the local economy. And third, especially when it involves many small-scale producers, a local food system tends to spread the wealth rather than concentrating it. All together, local food is good, worthwhile, and fulfilling because it helps support a good, worthwhile, and fulfilling local community.\nEconomist Hiroko Shimizu and geographer Pierre Desrochers have criticized local food, arguing that our current global food production and trade system is and will be much better at securing productivity, efficiency, and affordability than local food, and that these will be especially important when world population peaks at about ten billion or so later this century. In short, they argue, global food, not local food, will feed the world.\nI agree that questions of productivity, efficiency, and feeding the world need to be asked and answered. But Shimizu and Desrochers’ mistakenly assume that they are the only relevant questions, or the most important questions. The most important question is: what makes life better, more worthwhile, and more fulfilling, for farmers and workers; plants, animals, and ecosystems; and consumers? If the answer is, living in a certain kind of social and biological community, and that community requires eating local food, then it seems we should be working to make local food systems more secure, efficient, and affordable, not maintaining a global food system that has certain virtues but profoundly lacks others.\n\nFebruary 23rd, 2013 9:56am food economics"
  },
  {
    "objectID": "posts/2014-11-26-feminism-and-science-frenemies-.html",
    "href": "posts/2014-11-26-feminism-and-science-frenemies-.html",
    "title": "Feminism and Science: Frenemies?",
    "section": "",
    "text": "A friend of mine sent me a message on Facebook this morning:\n\nI’ve heard a couple of perspectives that insist that there’s a fundamental antagonism between natural science and feminism, and that simply adopting feminist empiricism is not enough. I’m skeptical of such views, but I haven’t yet been able to articulate a coherent alternative that takes seriously the forms of oppression that more anti-science feminism emphasizes. So, I guess my big question is, How can one articulate a commitment to anti-oppression feminism … while also affirming the importance, relevance, value of natural science?\n\nI think this is a really wonderful question, because it goes right to the heart of not just why I do the work that I do, but also how. As a philosopher of science, I’m not only trying to understand science (both natural and social); I’m also trying to improve it, and in ways that make it more valuable for social and political practices that, I think, promote human flourishing, including feminism and other anti-oppression social movements.\nOf course, this project assumes that science is, or at least can be, valuable in this way. If natural science is fundamentally antagonist towards feminism, then I’m wasting my time. So I want to start answering my friend’s question by understanding the opposite position: why would someone think that science and feminism are mutually antagonistic?\nIn the second half of the twentieth century, many feminists (both scientists and non-scientists) criticized many aspects of natural science. Science often operated with misogynist or androcentric assumptions — that men or males are typical, superior, or important, while women or females are atypical, inferior, or unimportant. Science was often organized internally along gendered lines — men could be “hotshots” and primary investigators, while women scientists were generally relegated to teaching, lab/technical assistant positions, and second-tier professorships. And, when science interacted with the public, “the scientist” was a technocratic, patronizing figure: the doctor (whether physician, psychiatrist, home economist, or other expert) told the housewife what was wrong with her and what she ought to do, and she was expected to comply without question.\nAs feminism became a recognized scholarly approach, more abstract and radical critiques were developed. These critiques argued, roughly, that science neglected interpersonal relationships — especially care, cooperation, and dependence — and the roles of emotion and symbolism in human experience. Since these things are generally associated with women and femininity in our society, their neglect by science reflects a “masculine” mindset of pure, detached rationality.\nNow, if you think that all of these things — misogyny, androcentrism, technocracy, expertism, and an ideal of pure, detached rationality — are essential, necessary, or natural features of natural science, then it’s reasonable for you to conclude that natural science is essentially, necessarily, or naturally opposed to feminism. And I think this is in the background of the “feminist anti-science” views that my friend has encountered.\nHowever, I do not think that any of these anti-feminist features are essential, necessary, or natural to science, although I do think that removing these features from science has (and, in an unfortunately larger number of cases, still) required considerable political organization and effort. The anti-feminist features of science are many and often deeply institutionalized, but still contingent. Let me elaborate this in three ways: with a deep historical point, a more recent historical point, and a pair of philosophical points.\nThe deep historical point is that what we might call the “liberatory potential and oppressive actuality” of both natural and social science has long been recognized by the Marxist tradition. If you know anything about Marxism, you probably know that Marxists say themselves as “scientific socialists,” who drew on cutting-edge work in social science (or, what we today would call social science) to make predictions about the future development and eventual collapse of capitalism. At the same time, actual social science, developed by “bourgeois academics,” was used to provide ideological cover for capitalism. So science had liberatory potential, even if it was generally actually used as a tool of oppression.\nWhat’s less widely recognized is that Marxists thought this applied to other fields of science as well. Early twentieth century debates over whether “nature” or “nurture” had a greater influence on human behavior were often debates between conservative and Marxist scientists. As the Marxists saw it, “nature” explanations were frequently used to rationalize social hierarchies as natural, necessary, and indeed socially optimal; while “nurture” explanations indicated ways that more egalitarian, less oppressive societies could be built using “social engineering.” (While that phrase sounds rather totalitarian to us, it had a much more positive, optimistic connotation in the early twentieth century.) The study of human behavior, then, had liberatory potential, even as it was actually oppressive.\nFor better or for worse, there aren’t many Marxists around today. But I think one insight worth preserving from the Marxist tradition is that, while actual science is often used oppressively, it has deep liberatory potential.\nThe more recent historical point is that feminist science has had a deep, positive influence on certain scientific fields. Here let me simply point to the excellent work by historian of science Londa Schiebinger, and especially her Gendered Innovations project (and the two anthologies with the same title), Feminism in Twentieth-Century Science, Technology, and Medicine, and Has Feminism Changed Science? Each of these works includes several case studies of the positive influence of feminist scientists in a field of natural or social science or engineering.\nThe historical points seem to address feminist concerns about misogyny and androcentrism within science. They also provide a case for challenging the gender hierarchy within science: if feminists and women scientists have had this benefit that everyone can acknowledge (identifying and removing misogynist and androcentric assumptions), then everyone should be able to agree that feminists and women scientists should have better standing within scientific fields, both as a recognition for the good that they have done and in order to make it easier for them to do good in similar ways in the future.\nBut what about the ideal of pure, detached rationality, and the technocratic and patronizing relationship between scientists and non-scientists?\nHere is where philosophers can make our specific kinds of contributions. First, we can argue that rationality does not require being “pure,’’ socially detached, and unemotional — we can challenge, and offer alternatives to, the ideal of value-free science and non-cognitive views of emotion and ethical judgment. Second, we can challenge, and offer alternatives to, technocratic and patronizing accounts of expertise and the role of experts in a democratic society. For a single piece that addresses both of these points, I would recommend Elizabeth Anderson’s article “Use of Value Judgments in Science”.\n\nNovember 26th, 2014 10:45am science and values philosophy of science feminism feminism and science"
  },
  {
    "objectID": "posts/2012-04-21-the-agroecological-argument-for-eating-meat.html",
    "href": "posts/2012-04-21-the-agroecological-argument-for-eating-meat.html",
    "title": "The Agroecological Argument for Eating Meat",
    "section": "",
    "text": "The New York Times has announced the six finalists in its Tell Us Why It’s Ethical to Eat Meat essay contest. Readers have until April 24th to vote for their favorite. Notably, three of the six essays give what I’m going to call the agroecological argument for carnivorism.  In this post, I’m going to explain this argument and respond to it.  \nI think the final paragraph of the essay ‘We Require Balance. Balance Requires Meat.’ states the argument quite nicely:\n\nA farm animal is not a pet or a wild animal fending for itself. The farm animal and the small farmer must cooperate to build a stronger herd or flock; we literally cannot survive without each other. The eating of animals is paramount to the production of food in a system that embraces the whole of reality. This is why eating meat is ethical. To not consume meat means to turn off a whole part of the natural world and to force production of food to move away from regenerative systems and to turn toward a system that creates larger problems for our world.\n\nBasically, truly sustainable agriculture – agriculture based on ecology, thus 'agroecology’ – requires the cooperation of animals; and this in turn requires eating some of them. So, for an environmentalist – someone who is committed, among other things, to sustainable agriculture – it seems that some degree of carnivorism, not vegetarianism, is required.\nAs an environmentalist, I find the agroecological argument compelling. Not convincing, but at least compelling. Indeed, it’s the only argument for carnivorism that I find remotely compelling. Claims that humans are ethically superior to other animals ('Meat is Ethical. Meat is Bad.’) strike me as ignorant and odious. Nebulous appeals to holiday feasts and tradition ('For What Shall We Be Blamed – and Why?’) are hand waving at best and uncritical apologetics for atrocities at worst. On the other hand, I personally know farmers who are trying to create a sustainable agricultural system. For nearly three years, I’ve eaten their vegetables and, occasionally, the eggs from their chickens and ducks. The agroecological argument speaks to some commitments that I share with these farmers, and some biological and economic realities that they face in trying to realize those commitments. In terms of either economics or nutrient cycling, they can’t afford to keep laying hens around for years after their egg production falls off.\nBut the agroecological argument is limited, and in a way that I think is quite serious. The argument does not apply directly to our contemporary, highly industrialized, food production system. Almost none of our vegetables are fertilized with composted manure. Almost none of our dairy or beef cattle graze on grasses. Almost none of our chickens scratch in the dirt for maggots. Except for a small number of people – those who are either 'grass farmers’ themselves or know someone who is – the conclusion of the argument is hypothetical: carnivorism would be required ifwe produced our food agroecologically. Since this is not the way we produce our food, the argument is basically irrelevant to our lives.\nIn some old lefty jargon, a description of a perfect world that does not include any explanation of how we might bring it about is said to be 'utopian’. A vision of a perfectly egalitarian society that is miraculously free of envy and fear is utopian in precisely this sense. And, to be clear, the term is pejorative: a utopia is, at best, a daydream enjoyed during a break from the hard work of actually changing the world; more likely, it is a distraction that makes us forget the necessity of this hard work. Even if the utopia is what motivates us, we need to understand the problems of the world we live in. And the utopia gives us no way to understand the problems. It can tell us that our world is imperfect; but presumably we already knew that. To paraphrase one of my students, you don’t need to talk about environmentally perfect agriculture to show me that CAFOs are bad.\nThe agroecological argument, I think, is utopian. It describes an ideal system of food production – 'ideal’ in several senses. Insofar as our system deviates from this ideal, the argument provides a criticism of our system. But that doesn’t tell us why our system has failed to achieve this ideal. Why, in other words, do we have lots of CAFO-raised cattle, and almost no grass-raised cattle? Why, after four decades of the organic movement, does organic agriculture only make up less than one percent of US farmland? Or, how do we go about realizing the agroecological ideal? The argument, by itself, can’t answer these questions. Again, since this is not the way we produce our food, the argument is basically irrelevant to our lives.\nYou might object that I’m demanding too much from the argument. It’s intended to answer one question – Is it ethical to eat meat? – and I’m suddenly asking a very different question – What’s wrong with our food system? My criticism should be targeted at the New York Times, for posing (what I think is) the wrong question, not the argument given to answer that wrong question.\nBut the question is open to interpretation. Consider these two ways of clarifying it:\n\nIs it ethical to eat meat in some conceivable situation?\nIs it ethical to eat meat in a food system like ours?\n\nThe agroecological argument only answers the first question; it gives us nothing, one way or the other, in response to the second question. And only the second, I’m arguing, is relevant to the real ethical decisions we have to make here and now. So when someone answers the original question with the argument, they are reading that question in its first, 'some conceivable situation’ version, and avoiding the genuinely relevant question.\nWhat would a more adequate argument tell us? The most important things, I think, would be that industrial meat production is enormously profitable and that meat has enormous cultural importance, not just in the US but globally. Meat is the food of the wealthy and powerful – especially, Carol Adams would add, wealthy and powerful heterosexual men – and, so long as there are fossil fuels to fertilize the corn and transport it to the livestock, entrenched industries that dominate the marketplace, and political patronage to help prop the whole thing up legally, industrial meat production is the cheapest way to meet massive consumer demand. Proponents of the agroecological argument themselves typically recognize that, in their system, humans would eat much less meat than contemporary Americans do. But then this system cannot satisfy the demands of a meat-hungry population. This is even more true to the extent that an agroecological system would genuinely respect its nonhuman animal members by ensuring them opportunities to lead good and full lives and slaughtering them only when it was necessary to maintain the ecology of the farm. The economic incentives, coupled to the cultural value of meat that create consumer demand, lead us inevitably to CAFOs, not pastures. And, after all, we leave under the reign of the almighty dollar.\nSo this, I think, is the important limitation of the agroecological argument. The problem is not 'internal’ to the argument; I have not argued here that one of its premises is flawed. (For that debate, see this piece and this response.) Rather, my problem is 'external’. Whatever merits the argument has on its own terms, it deals with an imaginary system of food production, and does not help us diagnose the problems of our actual system. \n\nApril 21st, 2012 11:00am food"
  },
  {
    "objectID": "posts/2023-09-08-misinfo.html",
    "href": "posts/2023-09-08-misinfo.html",
    "title": "Misinformation and trustworthiness: Frenemies in the analysis of public scientific controversies",
    "section": "",
    "text": "Lewandowsky et al. (2022) examine public scientific controversies through two contrasting “lenses” or analytical frameworks, using Covid-19 as their primary case study. The first lens, “science denial,” is pretty explicitly scientistic: in cases such as “the link between AIDS and HIV, climate change, evolution, and other clearly established scientific facts,”\n\nabsent new evidence, dissent from the scientifically accepted position cannot be supported by legitimate evidence and theorizing but must necessarily—that is, in virtually all instances—involve misleading or flawed argumentation. (Lewandowsky et al. 2022, 31)\n\nThere are some qualifiers in this particular quotation — “absent new evidence” and “virtually all instances” — but in other places these are dropped: “The rules of scientific evidence formation and argumentation are inescapable and cannot be discarded or side-stepped for political expediency” (Lewandowsky et al. 2022, 32) This lens explains public scientific controversies by appealing to a combination of irrationality and disinformation/propaganda.\nLewandowsky et al. (2022) recognize the technocratic implications of the “science denial” lens:\n\nthis insistence on quality of argumentation may seemingly curtail the public’s involvement in any scientifically informed debate. After all, members of the public are often nonexperts on topics and issues whose outcomes affect their lives. (Lewandowsky et al. 2022, 32)\n\nTheir first “solution” to this problem is to communicate “scientific issues” using “stories or pictures” (Lewandowsky et al. 2022, 32). The second “solution” is a segue into the second lens.\n\nThe “trust” lens explains controversies by appealing to the ways that “people differ in how much trust they put on various information sources (e.g., scientists vs. their neighbor on social media)” (Lewandowsky et al. 2022, 33). Further, the lens recognizes that differences in trust can be due to differences in trustworthiness.\n\nEthnic minorities, for example, have historically been discriminated against in the health care system. Western countries, especially those with colonial histo- ries, have also damaged people’s trust in medical treatments through their previ- ous mistreatment of indigenous populations (Lowes and Montero 2021) and misuse of vaccination centers, for example, by the CIA in its hunt for Osama bin Laden (Reardon 2011). It is unsurprising that people would question scientific evidence communicated by the same institutions that caused them harm or deceived them in the past (Jamison, Quinn, and Freimuth 2019). (Lewandowsky et al. 2022, 33, my emphasis)\n\nTo elaborate this a little more, it might be helpful to make a three-way distinction, between (a) (occurrent) trust, (b) perceived trustworthiness, and (c) actual trustworthiness. A potential trustee might satisfy criteria for trustworthiness, but be incorrectly perceived to be untrustworthy by the potential trustor. For example, under the influence of politicization campaigns, many US conservatives might incorrectly believe climate scientists to be more interested in their own careers than the public interest, and thus incorrectly perceive climate scientists to be untrustworthy.\nThe “trust” lens implies that “appreciation of why evidence is mistrusted in these communities is essential” and the importance of “regain[ing] trust rather than dismiss[ing] beliefs based on lived experience as simple denialism” (Lewandowsky et al. 2022, 33).\nLewandowsky et al. (2022) attempt to bring these two lenses together. Understanding why a group’s “cultural background or lived experience” undermines the trustworthiness of mainstream institutions “can provide pointers as to why [they] engage[] in (or fall[] for)” irrationality and disinformation (Lewandowsky et al. 2022, 34). In addition (or perhaps “specifically”), understanding the causes of trustworthiness failures “can reveal shortcomings in the scientific process or evidence base …. analysis of those arguments can provide valuable pointers to underlying issues—such as lacking representation in medical research—that can be addressed by suitable policies or remedial research” (Lewandowsky et al. 2022, 34).\nI would add that the example of incorrectly perceiving climate scientists to be untrustworthy suggests that perceived untrustworthiness and misinformation can be co-producing. Climate propagandists have not only promulgated “first order” misinformation about climate change (“it’s all natural variation”) but also “second order” misinformation about climate scientists (“they’ll say whatever it takes to get published”).\n\nUnfortunately, in their final section, “Recommendations,” Lewandowsky et al. (2022) seem to revert back to the “science denial” lens alone. Their two primary recommendations are that “misleading and inappropriate argumentation must be identified” and that “when misleading arguments have been identified, they can be used to ‘inoculate’ the public against their ill effects” (Lewandowsky et al. 2022, 35). The “trust” lens’ emphasis on understanding why scientific institutions might not be (perceived to be) trustworthy has all but disappeared. The “trust” lens does get one paragraph, arguing that “Policies that take into account the reasons underlying misleading arguments can be more effective than those agnostic about these reasons” and that emphasizing “‘winning’ an argument” is unlikely to be successful” (Lewandowsky et al. 2022, 35). But the final sentence asserts that deliberation “can be achieved” “only when misleading arguments can be identified and rejected” (Lewandowsky et al. 2022, 35), that is, only when “science” wins the argument.\n\nI really like the way this piece contrasts two common frameworks for understanding public scientific controversies. The scientistic and technocratic implications of the “science denial”/misinformation framework are on full display, which is useful for illustrating why I typically dislike this framework and find it, at best, incomplete. And the attempt to synthesize the two frameworks is useful: as you can see, it prompted me to think about how I understand the role of propaganda and misinformation in my analysis of controversies.\nBut that “Recommendations” section. To me, the paper reads like there were two sets of authors here. One set working with the misinfo framework, the other with the trustworthiness framework. The trustworthiness folks made a reasonable effort to integrate trustworthiness and misinformation in their section. But the misinfo folks ignored this, relying just on their framework to write the concluding section.\n\n\n\n\nReferences\n\nLewandowsky, Stephan, Konstantinos Armaos, Hendrik Bruns, Philipp Schmid, Dawn Liu Holford, Ulrike Hahn, Ahmed Al-Rawi, Sunita Sah, and John Cook. 2022. “When Science Becomes Embroiled in Conflict: Recognizing the Public’s Need for Debate While Combating Conspiracies and Misinformation.” The ANNALS of the American Academy of Political and Social Science 700 (1): 26–40. https://doi.org/10.1177/00027162221084663."
  },
  {
    "objectID": "posts/2018-02-28-care-ethics-for-data-science.html",
    "href": "posts/2018-02-28-care-ethics-for-data-science.html",
    "title": "A care ethics for data science",
    "section": "",
    "text": "Care ethics is an approach to ethical theorizing and reflection that emphasizes personal relations rather than abstract principles or rules. For example, ethics codes and human subjects research regulations take a rule-based approach to identifying and mitigating ethical problems. On a care ethics approach, we start by thinking about whose lives are affected by our actions, and whether our actions make for health or damaged relationships to those people.\nCare ethics developed in the 1980s, after psychologist Carol Gilligan observed differences in the ways adolescent girls and boys reason about moral dilemmas. Boys tended to rely more on abstract principles or rules to decide what was right (“cheating on an exam is wrong”), while girls tended to focus more on how their actions affected the people in their lives (“my mother would be disappointed if I cheated on an exam”). While contemporary care ethicists do not think that all men or women think in these stereotypical ways, focusing only on abstract principles ignores an important alternative approach used by many women. For this reason, several feminist psychologists and ethicists have developed care ethics as a robust approach to thinking about ethics.\nIn this post, I want to begin to sketch a care ethics for data scientists. Specifically, I want to ask who are the people that data scientists relate to, in our work as data scientists? That is, whose lives do we affect by doing data science? Clarifying these different relationships doesn’t yet tell us what makes these relationships go well or badly. But, as a first step, it’s important to systematically think about the range of different relationships in which we stand.\n\nTwo Paradigms of Data Science\nThroughout this post, I’m going to assume two different paradigms for data science activity. In academic data science, the data scientist is a member of an academic research lab, and specifically is a person who works directly with the lab’s research data. The data scientist is usually not the experimentalist or data collector — they’re not the person who runs the experiments or sets up the sensor systems in the field. At least, they don’t do data collection in their role as a data scientist. In other words, data scientist and data collector are two different roles, even though they might be done by the same particular individual. Similarly, I’m going to assume that the data scientist role is different from the role of the principal investigator. Among other things, the principal investigator sets the overall aims and research agenda for the lab, and has ultimate authority over the way research findings are interpreted and presented.\nIn business data science, the data scientist is an employee or contractor at a for-profit company or other “business-like” organization. In contrast with an academic setting, the overall aim of this organization is not to produce new knowledge. The organization has some other aim — say, to sell ads — and data science is used to better achieve this overall aim. Further, the data scientist uses “found data.” These data might be produced internally by the organization — maybe from logs of user behavior on the organization’s social media platform — or externally — such as public records or stock ticker data. The data scientist also has a supervisor or manager, who assigns the data scientist analytical or modeling tasks and is nominally responsible for communicating the data scientist’s findings or products to other parts of the organization.\nIn both paradigms, the data scientist — again, in their role as data scientist — is not responsible for the way the data are collected. They are also not ultimately responsible for the way the data are interpreted and communicated. Instead, the data scientist occupies one stage of a pipeline or assemblyline that connects sources of data to “decisionmakers.” This pipeline metaphor suggests that we can think about the relationships in which a data scientist finds herself by thinking about who is found “upstream” and “downstream” from data scientists.\n\n\nUpstream\nStarting from the data scientist, the first people we find upstream are data collectors and data curators (also called “data engineers” and “informaticists”). Data collectors construct and utilize the “instruments” — the experimental apparatus, the SurveyMonkey forms, the user activity logs — that transform physical events into “inscriptions” or “raw data.” Data curators are responsible for creating and maintaining the computing infrastructure needed to preserve collected data for use by data scientists: servers, databases, computer clusters, and so on.\nLike data scientists, data collectors and data curators will often be professionals with a postsecondary formal education and certification. But they may have lower status than data scientists — they might have the title of “librarian” or “technician” rather than “scientist.” Their work will often be anonymous — we will see the database and API, but not the name of the software engineer who designed them.\nFurther upstream are the sources of data, the people — or animals, or places, or other things — whose actions are inscribed in the “raw data.” Informed consent is frequently discussed as an important rule governing our relationships with data sources: we must obtain their consent before collecting and utilizing their data. Informed consent can also be thought of as an important aspect of our personal relationships with data sources. Do they understand what we want to do with their data? Would they approve? Would they find our analysis helpful, valuable, important? A waste of time, annoying, a distraction? Or worse, exploitative or harmful?\n\n\nDownstream\nImmediately downstream from us — or perhaps circling with us in an eddy — are fellow data scientists. We may relate to particular data scientists as mentors and students, or collaborators. An important implication of open source software and open data is that we often use the tools and datasets of fellow data scientists, and they in turn make use of the tools and datasets that we create. Collaborations will often involve data scientists with complementary areas of expertise within data science, creating further ties of interdependence. What would they say about how I am using the products of their work? What would I say about how they are using the products of mine?\nNext we have people who have authority over the way our work is interpreted, communicated, and used: managers, PIs, “decisionmakers,” readers/audiences, “the public.” In some cases we can easily exercise significant influence over these interpretations, communications, and utilizations. But often this is difficult to do, and we may need to think carefully about how to pass on our findings to avoid misinterpretation or the malign use (or neglect) of the products of our work.\nIn some cases, as data scientists we make claims about patterns or findings that we have discovered in the data. But data scientists also engage in visual communication, constructing static plots and graphics that require audiences to do more active interpretation. Further, we might create “interactives,” “dashboards,” or “systems” that leave open to users aspects of the process of discovering patterns and drawing conclusions. More flexible data products give more autonomy to our audiences or users. But even the most flexible data products are based on assumptions that inform their design — about what variables are important, how they might be related, how these relations might be used by audiences or users, and so on. In this way, we are pointing them towards certain conclusions, and away from others. Users have autonomy; but we are still channeling that autonomy. In what directions? Will they resist moving in that direction, or follow our lead? If they go where we are pointing, will it be too reluctantly, or perhaps too quickly? What do we want them to do when they get there? And what will they think of those broader aims?\nI use the term data subjects to refer to people — and animals, and places, and other things — whose lives (or existence) are shaped, in important ways, by the interpretation and use of data products. Data subjects often include data sources; but perhaps more often data subjects are not data sources. Consider a criminology model built using data from Philadelphia but put into use in Chicago. The people who applied the data — residents of Philadelphia — are not the people whose lives are shaped by the use of the model. While human subjects research rules regulate the relationship between researchers and data sources, they say absolutely nothing about data subjects.\nAs data scientists, our relationship with data subjects is mediated by supervisors, systems users, or other “decisionmakers.” But because decisions that we make channel the decisions made by downstream systems users, our decisions indirectly affect data subjects. If data subjects argue that our model has harmed them, it is callous to respond that we were “just following orders” and “doing what the client wanted.” That is literally denying our responsibility to care about our relations to other people. Just as it is important to reflect on the people whose data we use, and consider what they would think of the ways we are utilizing their data, it is also important to reflect on the people whose lives are governed by the analysis that we conduct, and consider what they think of that analysis.\nThese relationships become more difficult to reflect upon as they become more distant. Suppose I develop a deidentified dataset that is linked with a second dataset by another data scientist; then the combined dataset is re-identified by a third data scientist, and used to construct a predictive model; which is then sold to an IT contractor and bundled into a software suite; which is then used by an insurance company to predict risk of disease; with the result that someone cannot afford health insurance and dies of a treatable disease. It is tempting to say that our responsibility to the patient at the end of this chain has been watered down to nothing. But care ethics leads us to consider even this tenuous connection. If I come across this person as he lays dying at home untreated, what will I say? To deny any responsibility for his suffering because “I couldn’t know” or “I just played the smallest part” is, again, callous.\n\n\nConclusion\nIn this post, I have identified a series of relationships surrounding a data scientist: data sources; data collectors; data curators; fellow data scientists; managers, systems users, and other decisionmakers; and data subjects. I have also suggested some questions that can be used to start reflection on these relationships. Ethicists and other theorists could use these initial questions to describe the features that make these different relationships health or damaged. Care ethics also encourages us to think about the social context in which our relationships are formed and either sustained or damaged. This perspective might be especially important for thinking through the indirect relationships that I have identified here, such as between data scientists and data subjects.\nPracticing data scientists — and students and others training into data science — could use this post as a starting point for reflection on their own practice. What are the particular relationships of your data science activity? How would you describe those relationships — as healthy, or damaged, or in some other terms? How would the people on the other side of those relationships describe them? How do you know how they would describe them? If you — or they — would describe the relationship as damaged, or harmful, or exploitative, what steps can you take to repair the relationship?"
  },
  {
    "objectID": "posts/2012-03-14-friedersdorf-the-sex-friendly-case-against-free-birth-control-part-ii.html",
    "href": "posts/2012-03-14-friedersdorf-the-sex-friendly-case-against-free-birth-control-part-ii.html",
    "title": "Friedersdorf: The Sex-Friendly Case Against Free Birth Control, part II",
    "section": "",
    "text": "Part I\nNow for Friedersdorf’s second argument.  \n\nIf we broadly agree that we value “treating disease” and “fixing the broken body,” in what category do we put taking birth control? … [P]rogressives want to put it in the category of “preventative care.” But it is at least different than less controversial sorts of preventative care: 1) A small number of Americans have a moral objection to it. 2) The condition being prevented, pregnancy, isn’t identical to a disease. 3) The condition being prevented, pregnancy, can be avoided through an alternative approach, abstinence, that is completely effective and free – or a very cheap method, condoms, that have various advantageous and disadvantages that the thoughtful reader can supply him or herself.\n\nLike a ‘good’ philosopher, he goes on to compare subsidies for birth control to subsidies for somewhat fantastical counterfactuals:  pills that enable straight folks to enjoy mind-altering substances without addition or harmful effects, or get orgasms from yoga.  The argument seems to be that birth control is a matter of consequence-free pleasure, not medicine, and thus not legitimately subsidized on medical grounds or as preventative medicine.  (I emphasize ‘seems’ because I’m not certain about this reading.  Still, I can’t come up with a better one.)  \nFriedersdorf’s account of medicine involves exactly three aims:  \n\nTreating disease; \nFixing bodies (whether damaged through accident or disease, presumably); and\nPreventing the causes that necessitate the first two.\n\nOr, more succinctly, trauma medicine, pathology, and preventative medicine.  The crucial claims, then, are that (PC) pregnancy is a matter of neither trauma medicine nor pathology and that hence (BC) birth control is not preventative medicine.  \nBut (PC) reveals – spectacularly, I think – the inadequacy of this account of medicine.  This claim, plus the account of medicine and the obvious claim that pregnancy is not a matter of preventative medicine, imply that pregnancy is not a matter of medicine.  Which, as any expectant mother will tell you, is absurd.  I suspect that pregnancy is the single most common reason for medical intervention in our society today, and I know that historically it’s one of the most life-threatening conditions a human being can be in.  \nIt’s true that for most women in our society pregnancy the risks of pregnancy are virtually non-existent.  In 2007, there 12.7 maternal deaths per 100,000 live births in the US; even among non-Hispanic Black women the rate was only 28.4.  For contrast, in Mexico in 2008 this rate was 85, and in Uganda it was 430.  I’m also sympathetic to feminist arguments that pregnancy has been excessively medicalized in our country, and that this excessive medicalization is used to dominate both pregnant and ‘potentially pregnant’ women in unjust ways.  \nBut the world-historical drop in maternal deaths is, of course, due to the  medicalization of pregnancy, the development of sanitation and antibiotics, and, yes, birth control.  Women who would have had a half-dozen pregnancies or more – at a increased risk of death from sheer physiological exhaustion – now have only a few, and without sacrificing physical and emotional intimacy with their sexual partners. This suggests that the right approach to pregnancy – at least, with respect to medicine – is one that attempts to balance the benefits of medicalization with respect for women’s sexuality and reproductive justice.  \nPerhaps this can be done in terms of Friedersdorf’s account of medicine:  the medicalization of pregnancy is, when not invasive or oppressive, preventative medicine. But then birth control is also preventative medicine.  Like both abstinence and regular checkups with an OB/GYN, birth control is a very reliable way to avoid pre-eclampsia. So, like regular checkups with an OB/GYN – for both pregnant and ‘potentially pregnant’ women – birth control seems to be good preventative medicine, and the subsidy seems justified.  \nNote that I am not arguing here that birth control is good preventative medicine for ovarian cysts or painful menstruation.  I think it’s good preventative medicine for those things as well.  But those arguments don’t support subsidizing birth control as birth control.  They support subsidizing it under some other description.  Here I’m arguing that birth control is good preventative medicine for all of the life-threatening complications surrounding pregnancy.  \nFriedersdorf might be trying to pre-empt this argument when he points out that ‘[t]he condition being prevented, pregnancy, can be avoided through an alternative approach, abstinence, that is completely effective and free’.  Maybe I don’t understand the argument he’s gesturing out – it’s not actually spelled out any more than this, I think – but it strikes me as utterly uncompelling.  In general, for a given end E, there are indefinitely many courses of action (means) M1, M2, M3, …, that can reliable realize E.  Some of these courses of action will be better in all respects than others.  But, I claim, there will still be indefinitely many courses of action such that, for any two M1 and M2, M1 is better in some respects than M2 and M2 is better in some respects than M1.  For example:  \n\nAbstinence is effective when it comes to voluntary intercourse; it is absolutely free; has no health side-effects; but for most people it is extraordinarily difficult to do consistently; and it is useless when it comes to rape.  \nNatural family planning is difficult to do well, which makes it stress-inducing in its own particular ways; is more-or-less free (one has to pay for the thermometers and calendars); has no health side-effects; but is not terribly reliable; and is also useless when it comes to rape.  \nCondoms are inexpensive; but slightly to modestly diminish the pleasure of intercourse for most people; can break; and are also useless when it comes to rape.  \nRegular birth control is as effective as any of the others, and indeed is the only one of these not useless when it comes to rape; but it requires taking a pill at the same time every day; can have some negative health side-effects; and does cost more than condoms.  \n\nAll four of these options have something to say in their favor, but none is clearly superior to all of the others.  This argument is like saying that, from South Bend, one can get to Chicago by plane, train, or automobile:  not a good reason to not subsidize one of the options.  \n\nMarch 14th, 2012 1:04pm gender/sex"
  },
  {
    "objectID": "posts/2018-10-23-poster-contributions-of-women.html",
    "href": "posts/2018-10-23-poster-contributions-of-women.html",
    "title": "Poster: Contributions of Women to 20th Century Philosophy of Science",
    "section": "",
    "text": "I’m going to be presenting in the poster session at the Philosophy of Science Association meeting in Seattle next month. You can get the poster here and I’m going to try embedding it below.\n\n\nThis is the inaugural project for the CompHOPOS dataset, which aims to be a comprehensive dataset of journal articles (and some book chapters!) in philosophy of science. This project has also let me develop my skills with topic models — especially giant topic models (20,000 documents x 10,000 terms in the vocabulary). And I figured out how to cross-apply a method from bibliometrics in order to identify “germinal papers,” papers written before a topic emerges as a recognizable subfield.\nWhen we were finalizing the poster this weekend, my coauthor Evelyn Brister asked me what I found “most surprising” in this analysis. We ended up cutting that out due to space, so I thought it would make a good blog post.\nIt’s not really surprising to me, but it’s notable that women have made early contributions across the range of subfields of philosophy of science. I think there’s a perception that women have mostly been “confined” to feminism and philosophy of biology. None of the women with the most germinal papers (Table 1 and Figure A in the poster) are known for their work in feminist philosophy (though some are feminists as well as philosophers). And several are notable for their work in philosophy of physics, math, or statistics. Similarly, it’s notable that the areas in which women have been the most prominent early contributors (Table 2 and Figure B in the poster) include both general philosophy of science (“continental drift” is more general than its label suggests) and non-biological sciences such as cognitive science. Another version of Table 2/Figure B, which focused on counts of women-authored papers rather than share, included some areas of philosophy of physics.\nAnother notable thing is that many historically important women are still alive today. The timeline makes it clear that the 1970s was an extremely important period in the history of philosophy of science, and one that — as far as I know — hasn’t been the subject of much historical work. We have a very limited amount of time to collect an oral history of the period. It would be great for philosophy of science’s professional societies — the PSA, the Women’s Caucus, HOPOS — to sponsor this research."
  },
  {
    "objectID": "posts/2013-12-02-sexism-philosophy-and-the-reciprocity-of-virtue.html",
    "href": "posts/2013-12-02-sexism-philosophy-and-the-reciprocity-of-virtue.html",
    "title": "Sexism, Philosophy, and the Reciprocity of Virtue",
    "section": "",
    "text": "Sexism in philosophy has been on my mind lately, between my colleague Kerry McKenzie’s review of a disastrous attempt at philosophy of physics by notorious sexist philosopher Colin McGinn and a visit to our department last week by Jenny Saul. I’ve also been thinking a lot about virtue ethics, in part because I get to teach it for the first time next term. It seems like virtue ethics has some valuable insights for the problem of sexism in philosophy. In this post, I want to develop one small insight, starting with something that seems to be a challenge to a virtue ethical discussion of sexism in philosophy.\nThe challenge targets a key notion for many virtue ethics, which is often called the unity of virtue. Julia Annas calls it the reciprocity of virtue; that’s the term I’ll use here. The idea is that each virtue requires other virtues, and so all of the virtues hang together in a way. Put a little more strongly, having one virtue requries having every virtue.\nThe challenge to the reciprocity of virtue runs something like this:\n\nMcGinn is just one of a depressingly large number of good philosophers who are horrible sexists. They have the virtues that go into being a good philosopher — call these the philosophical virtues — but they’re sexist, disrespectful, chauvinist, and objectifying. They lack the non-sexist virtues. The reciprocity of virtue seems to imply that they cannot have the philosophical virtues without also having the non-sexist virtues. So the reciprocity of virtue seems to be false.\n\nYou might respond to this by claiming that McGinn and his ilk are a rare exception — just a few bad apples. The reciprocity of virtue is meant to apply generally and for the most part, and so it can accommodate a small number of exceptional cases. However, if you follow the first link at the top of the post, you can read hundreds of horrible stories about sexist behavior, often attributed to people who have high standing as professional philosophers. Unfortunately, it seems like there are many people who are both good philosophers and horrible sexists.\nAnother response is to separate “ethical” virtues from the virtues of specific professions. Consider someone who’s a “good assassin” or a “good mercenary.” It seems perfectly intelligible to say that someone is good at assassinating people, or killing people from money. And we can identify the character traits that go along with being good at these professions: being merciless and not hesitating to kill, for example, and being skilled with deadly weapons. These character traits are the “assassination virtues,” and clearly they have nothing to do with ethical virtues like mercy and justice. Sexist philosophers are a less extreme version of this same kind of case. They have the philosophical virtues, and these philosophical virtues have nothing to do with ethical virtues.\nHowever, I think one of the major strengths of virtue ethics is that it does not separate morality or ethics from the rest of our lives. Ethics doesn’t occupy some special sphere or make use of distinct kinds of considerations. Instead, it emerges from all of our various activities, in large part because, when these activities are worth doing, they require the virtues to be successful. For example, doing philosophy well requires courage and justice, to defend an unpopular argument or to concede that your position has been shown to be flawed. As we reflect on the various activities that fill our lives, we come to recognize that certain ways of acting and thinking are valuable in every activity worth doing — being courageous, just, honest, and so on. These things are the virtues.\nThe idea of “assassination virtues” confuses excellence and efficiency. The “good assassin” is an efficient killer: given a target, they can be relied upon to kill them quickly, expending a minimum of resources. But killing someone isn’t a fine, worthwhile achievement that fits with all of our other activities to produce a full and edifying life. On the other hand, something like philosophy, done well, is this kind of achievement. In other words, when it’s done well, philosophy achieves an excellence that assassination can’t. Because of this, the “assassination virtues” reciprocate with the ethical virtues — they don’t hang together — while the philosophical virtues do reciprocate — doing philosophy well requires the virtues, and indeed beyond a certain level having the virtues requires philosophical reflection.\nSo how would I respond to the challenge to the reciprocity of virtue?\nPhilosophy is a complex social activity, and like many such activities, it has a complex institutional reward structure. Within this reward structure, certain kinds of behaviors are rewarded, and so encouraged, and philosophers come to recognize these behaviors as constituting “good philosophy.” For example, publishing articles in certain journals, or publishing books with certain university presses, are rewarded with job offers and promotions. Gradually the kinds of problems and solutions and the kind of writing style used in these journals and books come to be recognized as constituting “good philosophy.” This same reward structure punishes other behaviors — which come to be recognized as “bad philosophy” or “not philosophy.”\nIn contemporary philosophy, sexist behavior generally falls under a third category, where it’s neither rewarded nor punished. Consequently, it’s not recognized as “good philosophy,” but it’s also not recognized as “bad philosophy.” Instead, it’s regarded as “philosophically irrelevant.” In this way, McGinn and his ilk can be thought of as “good philosophers,” despite being horrible sexists. Their sexism is regarded as irrelevant to their philosophical activities. (It’s not hard to find blog comments that take this line.)\nAs I said above, good philosophy requires the virtues. This includes the anti-sexist virtues. A sexist philosopher doesn’t take his opponent’s arguments seriously, just because she happens to be a woman, and consequently fails to recognize a problem with his own views; this is a failure of both justice and courage. The problem is that the reward structure of contemporary philosophy generally doesn’t punish him for his sexist behavior. So long as he does an adequate job responding to his men opponents, he’ll be treated as though he were a good philosopher. He’ll get the publications and prestigious positions that a good philosopher deserves, despite the fact that he’s not actually that good.\nVirtue ethics call this kind of problem compartmentalization. When society is compartmentalized — divided into distinct spheres, with each sphere rewarding certain virtues and ignoring others — the reciprocity of virtue can seem to fall apart. It might seem as though being a good philosopher doesn’t require having the anti-sexist virtues. But in fact they are required. It’s just that philosophers generally aren’t rewarded for having the anti-sexist virtues, and so someone can become a highly promoted philosopher despite being a horrible sexist.\nThis response has several important implications. One is that sexism is an institutional problem, not an individual problem. Even when we’re talking about individual attitudes (rather than structural sexism, which is a whole other problem), the underlying cause is the institutional system of rewards and punishments. It’s this system that must be changed, not just the attitudes and beliefs of many individual sexists. A second important implication is that we can’t address sexism in philosophy without changing our understanding of what makes for good philosophy. At a minimum, we need to recognize that sexist philosophy is bad philosophy, rather than regarding sexism as irrelevant to the quality of the philosophy. The necessary changes may not go further than this. But they may go further. For example, a major point of Saul’s talk was that the focus on defining “sexual harassment,” and giving counterexamples to any proposed definition, leads us to fail to actually do anything about sexual harassment. Paradoxically, perhaps certain kinds of demand for rigor are leading to bad philosophy.\n\nDecember 2nd, 2013 2:43pm metaphilosophy higher ed"
  },
  {
    "objectID": "posts/2013-09-04-how-to-use-citations-to-create-ignorance.html",
    "href": "posts/2013-09-04-how-to-use-citations-to-create-ignorance.html",
    "title": "How to Use Citations to Create Ignorance",
    "section": "",
    "text": "I spent most of Monday working on some “deep literature analysis” for my research on genetically modified organisms for food, or GMOf. This meant, in practice, that I spent about two hours looking up the citations for a single article. It was quite dull work, but the results were very interesting from the perspective of agnotology, an emerging area of science studies that deals with the production of ignorance. In this post, I’m going to give you some background on the “feed the world argument” and agnotology, then present my findings.\nFirst, a few quick paragraphs of background. The “feed the world” argument is probably the most prominent argument used by supporters of GMOf. It’s shown up venues from The New York Times to Nature, and off the top of my head I can think of versions of it that date back to 2000. The argument, in brief, is that (1) world population will increase significantly over the next few decades, so (2) food production should be significantly increased; and (3) GMOf will increase food production, hence (4) we should develop GMOf.\nHugh Lacey has pointed out that this argument is invalid: the fact that GMOf will increase food production doesn’t imply that they’re the best way to do so; until we think that GMOf will work much better than methods based on organic agriculture, for example, we don’t have enough reason to develop GMOf (and not develop organics). (2) and (3) don’t provide enough support for (4).\nLacey’s critique grants premise (3), at least for the sake of argument. My work is more critical of premise (3) itself. It’s not that I think (3) is false. Rather, I think the ability of GMOf to increase food production depends a great deal on how the technology is developed, and that the way this research is funded tends to produce GMOf that don’t significantly increase food production. The “feed the world” argument holds up the promise of GMOf, but I suspect that this promise won’t be realized if we continue on our present course.\nNext, agnotology. The term was coinced by Stanford historian of science Robert Proctor a few years ago, to mean “the study of ignorance.” Proctor thinks that we science scholars shouldn’t just be looking at the way knowledge is produced, and we shouldn’t just assume that ignorance is not (yet) having knowledge. Based on his work on tobacco, Proctor argues that ignorance is often actively produced, and in some cases is even produced as a “strategic ploy”. Think of the way tobacco industry executives and scientists publicly denied that nicotine was addictive or caused cancer for decades after they had strong evidence showing otherwise.\nWork in agnotology would be especially valuable, I think, because it can be quite difficult to tell whether someone is producing knowledge or ignorance. Something that appears to be knowledge (like an article in a peer-reviewed journal, complete with numbers and citations) can actually be agnogenetic, creating ignorance. The agnogenetic strategy — the way of creating ignorance — that I’m looking at here works in this way. Specifically, scholarly citations might be being used to create a false appearance that the paper’s claims are trustworthy and well-supported.\nI need to emphasize what precisely I’m arguing. My findings undercut the trustworthiness of the paper I examined, and so should make us question how well-supported its claims are. But I’m not going to provide evidence against any of its claims. Furthermore, I’m not claiming that the authors deliberately used this strategy to create ignorance. Indeed, I’m not even claiming that this paper does in fact create ignorance. My point is that the citations strategies used by the authors of this paper would be a very effective way to create ignorance if someone were trying to do so.\nThe paper — Brookes and Barfoot, “The Global Income and Production Effects of Genetically Modified (GM) Crops 1996-2011, GM Crops and Food 4:1 (Jan/Feb/Mar 2013), 74-83 — gives evidence that GMOf has already increased the production of maize (corn), i.e., in support of premise (3), as part of an argument that GMOf has produced economic benefits for farmers. Brookes and Barfoot are the founders and directors of PG Economics Ltd., which describes itself as \"a specialist provider of advisory and consultancy services to agriculture and other natural resource-based industries.” PG Economics Ltd. has been hired by such agricultural biotech companies as Monsanto and Dow Agro Sciences in the past, but the paper I was looking at is a report that they publish annually on the economic benefits of GM crops (both food and cotton) for farmers. Earlier versions of this report have been cited hundreds of times by scholarly sources, as well as by Monsanto and other proponents of GMOf in non-scholarly contexts. (Note that, in this paper, Brookes and Barfoot claim no conflict of interest; this seems to be consistent with the letter of the journal’s conflict of interest policy.)\nThe key evidence supporting premise (3) is given in Table 5, “Average (%) yield gains GM IR [insect-resistant] cotton and maize 1996-2011,” page 80. Brookes and Barfoot report modest improvements in core countries — 7.0% and 5.0% for maize in the US, for example — but really spectacular improvements in semiperiphery countries — 18.6% for maize in the Philippines, 21.0% in Colombia, for example. In addition, Brookes and Barfoot provide a list of several citations for each country in the far-right column of the table.\nThe numbers are impressive, and the list of citations creates the impression that they are the product of good scholarship. But they’re significantly larger than numbers I’ve seen elsewhere, which is why I decided to actually look up their citations. Specifically, I checked on the 12 citations dated 2003 or later for the four countries with increases greater than 10% for maize. Here’s what I found:\n\nSouth Africa\n\nCitation number 31 was published by a small peer-reviewed journal; it combines surveys of a small (n=33) number of large farmers and mid-sized (n=368) number of small farmers. It reports “higher yields for both groups” of farmers, but I don’t have access to the actual text of the article through my university.\nCitation number 32 was published by the Journal of Development Perspectives, a publication of the Economic Society of South Africa. Its conclusions don’t really support (3): “A survey of 135 farms in 2003/4 is used in a stochastic frontier model to show that Bt [insect-resistant] seed did not increase yields per kg of seed.”\nCitation number 33 was published by AgBioForum, which is a small but recognized journal on agricultural biotechnology. It discussed a series of mid- to small-size surveys of farmers conducted between 2001/02 and 2003/04; GM maize was comparable or more productive than conventional maize.\nCitation number 34 is said to be published by the “South African Maize Trust,” which doesn’t appear to have any record of the report on its website.\nCitation number 37 is to the 2002 edition of an annual publication by ISAAA, a pro-biotech organization.\n\nPhilippines\n\nCitation number 44 is listed online — as an image — but not accessible: http://www.strivefoundation.com/publications.html. From the website, it’s hard to tell what exactly the publishing organization does.\nCitation number 45 is a conference presentation — the data don’t seem to be available online.\nCitation number 46 is by a pro-biotech organization and doesn’t appear to be available online.\n\nColombia\n\nCitation number 18 was published in GM Crops and Food (the same journal as the Brookes and Barfoot paper) and is a small study (20 farms, 10 conventional and 10 GM).\nCitation number 55 is another conference presentation.\n\nBrazil\n\nCitation numbers 10, 58, and 59 are publications by a Brazilian research firm — like PG Economics Ltd., its data have been used by Monsanto — and none of these particular papers seem to be available online.\nCitation number 60 is by Monsanto Brazil.\n\n\nAll together, Brookes and Barfoot provide some modest to weak evidence that GM maize was significantly more productive in South Africa, and some weak evidence that it was significantly more productive in Colombia. But many of their citations are inaccessible or have industry affiliations, making them untrustworthy. Furthermore, Brookes and Barfoot provide no real explanation of how they decided to use these particular studies. Their methodology section claims that “The report is based on extensive analysis of existing farm level impact data for GM crops, much of which can be found in peer reviewed literature.” (78) But only 4 of the 12 citations that I tracked down above were peer-reviewed, and it seems odd to say that the analysis is “extensive” when it looks at only a couple dozen of the thousands of GM productivity studies.\nAgain, none of this means that Brookes and Barfoot’s numbers are false or aren’t actually well-supported. My point is that this paper isn’t very trustworthy: we can’t check many of their numbers, and many of the ones that we can check come from sources we might not trust. And yet it appears trustworthy — the list of citations is right there next to the numbers in the table. Recognizing that the paper isn’t very trustworthy — recognizing the kinds of citations that the authors cited, whether they’re available, and so on — requires a very tedious exercise in tracking down citations.\nThese two features together — the appearance of trustworthiness and the difficulty of dismantling that appearance — make this kind of use of citations a powerful agnogenetic strategy. A typical non-scientist reader of the paper probably wouldn’t go to the trouble of looking carefully at the citations. A scientist might look more carefully; but, especially if they just need a quick citation for the literature review section of their own paper, instead they might assume that untrustworthy sources would be caught in the peer-review process before publication. Yet this assumption may not be warranted: peer reviewers are busy faculty who aren’t paid for their work as reviewers, and checking citations is boring and time-consuming. So untrustworthy citations could easily slip past everyone.\n\nSeptember 4th, 2013 11:02am (philosophy of) science food gmo controversy"
  },
  {
    "objectID": "posts/2018-10-25-autotag.html",
    "href": "posts/2018-10-25-autotag.html",
    "title": "Auto-tagging posts + the nDH metric",
    "section": "",
    "text": "This morning I added a new feature to the blog posts: an R script that automatically generates and applies tags, allowing users to find old posts on (potentially, roughly) similar topics. Because of some difficulties setting up a chronological archive, I was worried old posts would be lost down the memory hole. This new feature should help prevent that.\nMy previous blogs had tags; you can see some of the remains of Tumblr’s tagging system at the bottom of many migrated posts. But the tags had to be added manually. This added a step to the publication process, and made me worried about inconsistent tags. What if I recently adopted a new tag that applied to some old posts? I’d have to scroll back through the archive to find them, which was both very tedious and prone to human error.\nThe new script solves these problems by automatically selecting tags and applying them across all posts at the same time. After writing a post, all I need to do is run a one-line command in the terminal. As a side effect, the tags will change, highlighting different arrays of topics as I add posts to the blog.\nIn the rest of this post, I want to explain the logic behind the tag selection process by walking through the code of the autotagging script. The post also includes a discussion of a metric I use for “high-information” terms, which I call \\[n \\Delta H\\].\nThe script uses two R packages: tidyverse, a suite of packages of data wrangling; and tidytext, a text mining package designed for use with the tidyverse. After loading these packages, the rest of the following chunk loads the content of every post on the blog into a single dataframe structure.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tidytext)\n\n\npost_folder = getwd()  # Just `_posts/` in the actual script\npost_files = list.files(post_folder, pattern = '\\\\.q?md') %&gt;%\n    file.path(post_folder, .)\n\ndataf = tibble(path = post_files) %&gt;%\n    mutate(post_id = row_number()) %&gt;%\n    rowwise() %&gt;%\n    mutate(raw_text = read_file(path)) %&gt;%\n    ungroup()\n\nstr(dataf)\n\ntibble [71 × 3] (S3: tbl_df/tbl/data.frame)\n $ path    : chr [1:71] \"/Users/danhicks/Google Drive/Coding/website/posts/2012-03-14-friedersdorf-the-sex-friendly-case-against-free-bi\"| __truncated__ \"/Users/danhicks/Google Drive/Coding/website/posts/2012-03-14-friedersdorf-the-sex-friendly-case-against-free-bi\"| __truncated__ \"/Users/danhicks/Google Drive/Coding/website/posts/2012-04-21-the-agroecological-argument-for-eating-meat.md\" \"/Users/danhicks/Google Drive/Coding/website/posts/2012-05-23-fetishism-and-innate-differences.md\" ...\n $ post_id : int [1:71] 1 2 3 4 5 6 7 8 9 10 ...\n $ raw_text: chr [1:71] \"---\\ntitle: \\\"Friedersdorf: The Sex-Friendly Case Against Free Birth Control, part I\\\"\\ndate: \\\"2012-03-14\\\"\\nc\"| __truncated__ \"---\\nlayout: post\\ntitle: \\\"Friedersdorf: The Sex-Friendly Case Against Free Birth Control, part II\\\"\\ndate: \\\"\"| __truncated__ \"---\\nlayout: post\\ntitle: \\\"The Agroecological Argument for Eating Meat\\\"\\ndate: \\\"2012-04-21\\\"\\ncategories: [d\"| __truncated__ \"---\\nlayout: post\\ntitle: \\\"Fetishism and Innate Differences\\\"\\ndate: \\\"2012-05-23\\\"\\n---\\n\\n\\n(Going through m\"| __truncated__ ...\n\n\nNext we use tidytext to parse the text of each post into individual terms or tokens.\n\ntokens_df = unnest_tokens(dataf, token, raw_text)\ntokens_df\n\n# A tibble: 86,856 × 3\n   path                                                            post_id token\n   &lt;chr&gt;                                                             &lt;int&gt; &lt;chr&gt;\n 1 /Users/danhicks/Google Drive/Coding/website/posts/2012-03-14-f…       1 title\n 2 /Users/danhicks/Google Drive/Coding/website/posts/2012-03-14-f…       1 frie…\n 3 /Users/danhicks/Google Drive/Coding/website/posts/2012-03-14-f…       1 the  \n 4 /Users/danhicks/Google Drive/Coding/website/posts/2012-03-14-f…       1 sex  \n 5 /Users/danhicks/Google Drive/Coding/website/posts/2012-03-14-f…       1 frie…\n 6 /Users/danhicks/Google Drive/Coding/website/posts/2012-03-14-f…       1 case \n 7 /Users/danhicks/Google Drive/Coding/website/posts/2012-03-14-f…       1 agai…\n 8 /Users/danhicks/Google Drive/Coding/website/posts/2012-03-14-f…       1 free \n 9 /Users/danhicks/Google Drive/Coding/website/posts/2012-03-14-f…       1 birth\n10 /Users/danhicks/Google Drive/Coding/website/posts/2012-03-14-f…       1 cont…\n# ℹ 86,846 more rows\n\n\nTo develop tags, we want to identify “highly meaningful terms” across the cropus of parsed blog posts. In text mining, a measure called TF-IDF is frequently used for this task. TF-IDF is calculated for each term-document combination. The TF-IDF of term \\[i\\] in document \\[j\\] is - the number of times \\[i\\] occurs in \\[j\\] (the term frequency), divided by - the (log) number of documents in which \\[i\\] occurs at least once (the document frequency).\nIntuitively, TF-IDF tries to identify words that are common in a particular document (high term frequency) but also only occur in a few documents (low document frequency). In English, words like “the” and “and” tend to have high term frequencies but also very high document frequencies, so their TF-IDF scores tend to be low. Misspelled words like “mipelled” have low document frequency, but also low term frequency, so again their TF-IDF scores tend to be low. Terms with a high TF-IDF should, the thought goes, be distinctive to a given document.\nThe problem with TF-IDF is that it’s difficult to make this intuitive account more precise. It can also be difficult to interpret, because the units are occurrences per log documents. Is 3 occurrences per log documents high or low? That depends on things like the number and length of documents in the corpus. In addition, TF-IDF is calculated for term-document pairs, not for individual terms. A word will have the same IDF value across the entire corpus, but will have different TF values as it occurs a different number of times across different documents.\nFor these reasons, in my text mining work I’ve switched to an information-based measure. To understand this one, note first that we have \\[m=42\\] blog posts. Suppose we draw one of these blog posts at random. The probability of drawing any given post is \\[p(doc) = \\frac{1}{m} = \\frac{1}{42}\\]. The entropy of this probability distribution tells us the number of binary bits (0s and 1s) we need to give a unique ID for each blog post.\n\\[ H = - \\sum_{docs} p(doc) \\log_2 p(docs) = - \\sum_m \\frac{1}{m} \\log_2 \\frac{1}{m} = log_2 m. \\]\nIn the case of my blog before writing this post, the entropy is \\[log_2 42 = 5.4\\].\nNow suppose that, after drawing a post at random, we draw a single word from the post at random (without peeking at exactly which post we’ve drawn). The probability that we’ve drawn any given post is now \\[p(doc \\mid word)\\]. The information gain of the word is the difference between \\[H\\] up above and the entropy of this new distribution \\[p(doc \\mid word)\\]:\n\\[\\Delta H = H[p(doc)] - H[p(doc \\mid word)].\\]\nWe can think of \\[\\Delta H\\] as the number of bits of information, relative to the full post ID, that we’ve gained from knowing the word. \\[\\Delta H\\] corresponds to inverse document frequency (dividing by the document frequency) in TF-IDF: it tells us how unique that word is to each document. A term like “the” will tend to have \\[\\Delta H\\] near 0 (it doesn’t give us any new information); a more distinctive term like “virtue” (only some of my posts are about virtue ethics) will have a relatively high \\[\\Delta\\], perhaps 4 or even 5 (it tells us a lot about which particular posts we might have).\nTo calculate an overall score for each term, we multiply \\[\\Delta H\\] by the (log) total times the term occurs across the entire corpus. Suppose “virtue” occurs 100 times across all blog posts, and has an information gain of 3 bits. Then we have \\[\\log_{10} n \\times \\Delta H = 2 \\times 3 = 6\\]. The units here are orders of magnitude-bits. An increase of 1 of these units means that a word occurs \\[10\\times\\] as frequently or provides 1 added bit of information. I call the resulting measure \\[n\\Delta H\\] (not writing the log for readability).\nThe following block calculates \\[H\\] for the distribution \\[p(doc)\\], as baseline, then calculates the information gain and \\[n\\Delta H\\].\n\nbaseline = log2(nrow(dataf))\n\ninfo_df = tokens_df %&gt;%\n    count(token, post_id) %&gt;%\n    group_by(token) %&gt;%\n    arrange(token) %&gt;%\n    mutate(p = n / sum(n), \n           H_term = -p*log2(p)) %&gt;%\n    summarize(n = sum(n), \n              H = sum(H_term), \n              delta_H = baseline - H,\n              ndH = log10(n)*delta_H)\n\nstr(info_df)\n\ntibble [8,822 × 5] (S3: tbl_df/tbl/data.frame)\n $ token  : chr [1:8822] \"0\" \"0.0\" \"0.009\" \"0.037\" ...\n $ n      : int [1:8822] 17 1 1 1 1 1 1 1 4 1 ...\n $ H      : num [1:8822] 2.17 0 0 0 0 ...\n $ delta_H: num [1:8822] 3.98 6.15 6.15 6.15 6.15 ...\n $ ndH    : num [1:8822] 4.89 0 0 0 0 ...\n\n\nThis next block isn’t included in the script; it generates a plot showing the distribution of \\[n\\Delta H\\] scores for a sample of 500 terms. Each point represents a single term; the x-axis is the (log) number of total occurrences across the corpus, color indicates the information gain \\[\\Delta H\\], and the y-axis is the overall \\[n \\Delta H\\] score.\n\nset.seed(2018-10-25)\ninfo_df %&gt;%\n    sample_n(500) %&gt;%\n    ggplot(aes(log10(n), color = delta_H, \n                                ndH, label = token)) +\n    geom_text() +\n    theme_minimal()\n\n\n\n\nInformally, the \\[n \\Delta H\\] score does a good job of identifying characteristic words for different kinds of posts, such as “data,” “developing,” and “feminism.”\nThe overall shape of the plot also shows how \\[n \\Delta H\\] balances information gain and frequency. Some terms like “yourself” and “x’s” have a high information gain (more than 4 bits — remember that the maximum is about 5.4), but occur fewer than 10 times and so have a low \\[n \\Delta H\\]. Extremely common terms (many more than 100 occurrences across a set of 42 posts) have very little information. The highest-scoring term, “market,” has both high information (4.3) and occurs 71 times.\nAll together, the \\[n \\Delta H\\] score picks out a set of high-information, moderate-frequency terms in the middle of the plot, much like TF-IDF. In contrast with TF-IDF, it uses information theory for theoretical support and produces a relatively interpretable value.\nThe last step in the script is to select a vocabulary of terms to use as tags, identify them in the posts, and write the tags into the Markdown files.\nTo select the vocabulary, we simply take the terms with the highest \\[n \\Delta H\\] scores. Because the blog is fairly small, it’s not unreasonable to take as many terms as there are blog posts (42 tags \\[\\to\\] 42 tags). As the blog grows, I’ll probably want to trim this down to set a hard cap. Alternatively, I might want to select a number of tags such that each post has at least 1 tag and no post has more than, say, 5 tags. For now, though, I’ll just use the simpler logic of the top 42 terms.\n\nvocab =  top_n(info_df, nrow(dataf), ndH)\n\nThen we’ll filter the list of tokens down to these tags, collapse the tags into a list, and convert the list into the string that will go in the header of the Markdown files.\n\ntags_df = tokens_df %&gt;%\n    filter(token %in% vocab$token) %&gt;%\n    count(path, post_id, token) %&gt;%\n    group_by(path, post_id) %&gt;%\n    summarize(tags = list(token)) %&gt;%\n    mutate(tags_str = map_chr(tags, str_c, collapse = ','), \n           tags_str = str_c('categories: [', tags_str, ']')) %&gt;%\n    ungroup()\n\n`summarise()` has grouped output by 'path'. You can override using the\n`.groups` argument.\n\nstr(tags_df, max.level = 1)\n\ntibble [62 × 4] (S3: tbl_df/tbl/data.frame)\n\n\nFinally we’ll write these files back to disk. Each post has a header section that looks something like this:\n---\ntitle: \"Post title goes here\"\nstyle: post\ncategories: [monkey,zoo]\n---\nWe’ll separate this header section from the body by searching for the ---, add the tags, then put everything back together and write to disk. (Except we won’t actually write the results out to disk here.)\n\nright_join(dataf, tags_df) %&gt;%\n    separate(raw_text, into = c('null', 'header', 'body'), \n             sep = '---', extra = 'merge') %&gt;% \n    ## Remove old tags\n    mutate(header = str_replace(header, \n                                'categories: \\\\[[^\\\\]]+\\\\]', \n                                '')) %&gt;% \n    mutate(combined = str_c('---', \n                            header, \n                            tags_str, '\\n',\n                            '---', \n                            body)) %&gt;%\n    # pull(combined) %&gt;% {cat(.[[1]])}\n    rowwise() #%&gt;%\n\nJoining with `by = join_by(path, post_id)`\n\n\n# A tibble: 62 × 8\n# Rowwise: \n   path                       post_id null  header body  tags  tags_str combined\n   &lt;chr&gt;                        &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;lis&gt; &lt;chr&gt;    &lt;chr&gt;   \n 1 /Users/danhicks/Google Dr…       1 \"\"    \"\\nti… \"\\n\\… &lt;chr&gt; categor… \"---\\nt…\n 2 /Users/danhicks/Google Dr…       2 \"\"    \"\\nla… \"\\n\\… &lt;chr&gt; categor… \"---\\nl…\n 3 /Users/danhicks/Google Dr…       3 \"\"    \"\\nla… \"\\n\\… &lt;chr&gt; categor… \"---\\nl…\n 4 /Users/danhicks/Google Dr…       5 \"\"    \"\\nla… \"\\n\\… &lt;chr&gt; categor… \"---\\nl…\n 5 /Users/danhicks/Google Dr…       6 \"\"    \"\\nla… \"\\n\\… &lt;chr&gt; categor… \"---\\nl…\n 6 /Users/danhicks/Google Dr…       7 \"\"    \"\\nla… \"\\n\\… &lt;chr&gt; categor… \"---\\nl…\n 7 /Users/danhicks/Google Dr…       8 \"\"    \"\\nla… \"\\n\\… &lt;chr&gt; categor… \"---\\nl…\n 8 /Users/danhicks/Google Dr…       9 \"\"    \"\\nla… \"\\n\\… &lt;chr&gt; categor… \"---\\nl…\n 9 /Users/danhicks/Google Dr…      10 \"\"    \"\\nla… \"\\n\\… &lt;chr&gt; categor… \"---\\nl…\n10 /Users/danhicks/Google Dr…      11 \"\"    \"\\nla… \"\\n\\… &lt;chr&gt; categor… \"---\\nl…\n# ℹ 52 more rows\n\n\n    #mutate(written = map2(combined, path, write_lines))"
  },
  {
    "objectID": "posts/2016-05-26-no-psychologists-haven-t-shown-that-gmo-opponents-don-t-care-about-evidence.html",
    "href": "posts/2016-05-26-no-psychologists-haven-t-shown-that-gmo-opponents-don-t-care-about-evidence.html",
    "title": "No, psychologists haven’t shown that GMO opponents don’t care about evidence",
    "section": "",
    "text": "Earlier this week, the journal Perspectives on Psychological Science published a paper by a team of psychologists on “absolute moral opposition” to GM foods. The paper has attracted some attention on my social media networks, where it’s being interpreted as evidence that many GM opponents are evidence-insensitive, that is, that their opposition to GM crops has nothing to do with “the facts,” and that this explains why the debate is intractable.\nFor example, Jesse Singal concludes from the study that\n\nit’s going to be really, really hard to convince skeptical Americans that GMO foods are okay to eat, because people’s beliefs about them are so driven by disgust rather than by any real understanding of the issues.\n\nHowever, the actual study did not investigate whether GM opponents are evidence-insensitive.\nHere’s what the study actually did. Participants were first asked a series of general questions about their attitudes to GM foods:\n\n(a) “I do not oppose this,” (b) “This should be prohibited no matter how great the benefits and minor the risks from allowing it,” (c) “It is equally wrong to allow some of this to happen as to allow twice as much to happen. The amount doesn’t matter,” and (d) “This would be wrong even in a country where everyone thought it was not wrong.” (317)\n\nSomeone was classified as an “absolutist opponent” if they answered “no” to both (a) and (b).\nNext, the respondents were presented with one of eight different scenarios. In four of the scenarios, an individual knowingly consumed a GM food (tomatoes, apples, tuna, or milk); in the other four, they consumed one of the same GM foods without knowing about it. Here’s the example text in the paper:\n\n“Mary eats tomatoes that have been genetically modified. She knows [does not know] the tomatoes have been genetically modified. Scientists have inserted genes in them so that they stay fresh longer.”\n\nRespondents then had to describe their feelings about the scenario in two ways. First, they either picked a word or a facial expression. The only options were “anger” (either the word or an “angered face”) and “disgust” (again, either the word or the face). Second, then rated, on a scale from 1 to 9, “how disgusted and how angered they were when imagining the scenario” (317). Finally, the respondents filled out some questions about their policy views and a “25-item Disgust Scale” to measure their “overall disgust sensitivity” (319).\nThat’s basically it. I want to make five points about this study.\nFirst, the conceptual framework that the researchers use conflates several categories recognized by ethicists. Consider the way absolute moral values are explained in the introduction:\n\n[Absolute moral values’] defining characteristic is the unconditional proscription of certain actions (e.g., “Do not cause the extinction of a species” or “Do not kill another human being”). Absolute moral values are explicitly regarded as axiomatic, requiring no further justification, and are protected from trade-offs with nonmoral (secular) values — especially money. (316)\n\nSpecifically, question (b) in the study instrument is examining non-consequentialist reasoning: whether someone is opposed to GM crops for reasons that are independent of the harms and benefits. This is not the same as an unconditional proscription; for example, someone might think that GM foods should be labeled because consumers have a right to make informed decisions about what they’re eating, regardless of the risks and benefits. This reasoning is non-consequentialist, but the pro-labeling view is not unjustified (it’s justified by general principles about autonomy and bodily integrity), and it may not be an unconditional proscription (e.g., the view might be “no GM foods unless they’re labeled”). For another example, someone might be opposed to GM foods because they think the benefits go overwhelmingly to a few large multinational corporations and the risks are borne by consumers, small farmers, and the environment. For someone with this view, the key question is not how great the benefits are, or how small the risks, but instead whether those benefits and risks are distributed fairly, such as according to John Rawls’ difference principle (the greatest benefit to the least advantaged). Or, for a third alternative, someone might be opposed to GM foods because they think the process for making decisions about GM foods is controlled by industry, not democratically.\nSecond, the study design treats GM opponents and supporters asymmetrically. The researchers do not consider absolute, non-consequentialist, or evidence-insensitive moral support for GM foods (which, in my opinion, is pretty common in some “pro-science” circles), or the role of disgust or other strong emotional responses in GM support. In this way, the study design pathologizes GM opponents even before any data have been collected: behind the research questions is an assumption that GM opponents are deviants, and their deviance needs to be understood in order to manage and neutralize it.\nThird, the narrow study design means that we do not understand how the respondents understood the prompts. When someone read (b), what risks and benefits were they thinking about? Was anyone thinking about the views I suggested in the last paragraph? Were there other reasons that neither I nor the researchers have considered? Similarly, on the responses to the scenarios, we don’t know how people were interpreting the fact that Mary didn’t know the tomatoes were GM. At one point the researchers speculate that “participants were more disgusted by inferred deception on the part of the firms selling the food” (319); but they have literally no additional data to investigate that possibility.\nFourth, the study design funnels emotional responses into two simplistic measures of just two very broad emotions, disgust and anger. If someone were worried that Mary was being deceived, they might not consider themselves either disgusted or angered; instead, they might prefer to describe themselves as “suspicious” or “mistrustful.” But maybe “disgust” is the closest option, so they pick that word, then give it a rating somewhere in the middle of the scale.\nThis point is closely related to the second point. But it also raises issues of hermeneutical injustice. The respondents were forced by the study design to articulate their views and attitudes in the (very limited) set of categories used by the researchers; they did not have an opportunity to represent themselves in their own terms. More broadly, the respondents didn’t have an opportunity to point out flaws, limitations, and ambiguities in the study, which means that the researchers can ignore — indeed, might never even be aware of — these flaws, limitations, and ambiguities. Consequently, it’s much easier for researchers — and other readers — to impose their own interpretations on the respondents.\nFinally, the study simply didn’t ask anyone why they hold the beliefs that they do; nor did it examine how people reacted when presented with arguments from other side of the GM controversy. This is the biggest issue for the interpretation — as in the blog post I quoted above — that the study shows GM opponents are evidence-insensitive. If your study does not test whether people’s views are sensitive to evidence, you cannot conclude that their views are insensitive to evidence.\nIt’s one thing for some editor at New York magazine to make this mistake. But the study authors make it themselves. In the conclusion, their “first key finding” is that “absolutist” GM opponents “indicate that they would maintain their opposition for any balance of risks and benefits; that is, they profess to be evidence insensitive” (320). But, as we saw up above, the respondents professed to no such thing. All the “absolutists” indicated is that their opposition to GM foods is not based on consequentialist considerations of risks and benefits.\nThe researchers might be extrapolating from previous research on disgust responses. In the introduction, they refer to studies of how people respond to “disgusting but putatively harmless behaviors — such as consensual sex between siblings or a family consuming its deceased pet dog.” Unsurprisingly, in these studies, “People are extremely reluctant to abandon [their] moral condemnation even when any harmful consequences (e.g., the siblings might get pregnant, dog flesh might make you ill) are explicitly eliminated” (316). Then, for the “first key finding,” they might be inferring that, since people with “absolutist” views were evidence-insensitive in these earlier studies, then “absolutist” GM opponents are also evidence-insensitive.\nHowever, this extrapolation assumes that disgust responses to incest and eating pets are relevantly similar to the disgust responses measured in this study. And I would like to suggest that this assumption is at least not obviously true.\nIn the conclusion, the authors briefly report some preliminary findings from a “pilot study.” In this study, respondents “rate[d] the persuasiveness of 10 arguments in favor of GM” either before or after they answered the four “absolutism” questions (a)-(d). Because it does not consider arguments against GM foods, all four arguments concern risks and benefits, and the study design is still limits the ways respondents can answer and doesn’t test attitudes before and after presenting the arguments, this study design doesn’t actually address any of the concerns I’ve raised in this post. But it would, at least, provide an instrument that could actually test evidence insensitivity.\n\nMay 26th, 2016 11:52am gmo controversy science and values"
  },
  {
    "objectID": "posts/2019-05-30-luck-and-the-academic-job-market.html",
    "href": "posts/2019-05-30-luck-and-the-academic-job-market.html",
    "title": "Luck and the Academic Job Market",
    "section": "",
    "text": "First, some news: starting this fall, I will be an assistant professor in the Department of Cognitive and Information Sciences at the University of California, Merced. I wrote the bulk of this post on May 26, the day after the 9th annual Conference on Values in Medicine, Science, and Technology at UT Dallas. I have a great community at this conference, and I’ve been sharing the news with my friends. Everyone is thrilled for me, of course.\nBut certain kinds of congratulation, while well-intentioned, give me pause. These are “you really deserve this” and “things finally worked out.” To my ear, these kinds of congratulation accept the dominant meritocratic view of academia, which further tends to lead to the conclusion that academics struggling with precarity and contingency deserve their low pay, lack of stability, and marginalized status.\nIt’s not that I think that I don’t deserve a tenure-track job, in some sense or another. I check all the meritocratic boxes, with something like 12 or 14 peer-reviewed publications, the majority of which are in “top” philosophy of science journals; I have a good mix of single-authored disciplinary papers and interdisciplinary collaborations; my PhD is from a fairly prestigious program; over the past seven years I’ve held a series of postdocs and research positions at comparable institutions; and so on.\nBut luck has played an enormous role in getting me to the point where I could check all these boxes. If things had gone just slightly differently, I would be struggling like so many of my peers: teaching 7+ courses per year, trying to make ends meet on $30,000 per year or less, and desperately trying to find time to publish an occasional paper.\nIn the rest of this post, I want to highlight 3 moments where luck made an enormous difference to the trajectory of my career.\nThe first moment was my admission to Notre Dame. I was not a philosophy major in undergrad; I studied math and political science, and took two philosophy of science courses my senior year. I decided to pursue a career in philosophy after taking those two courses and sitting in on a few graduate courses at the University of Illinois, Chicago. For obvious reasons I didn’t have a lot of mentorship to help me articulate my interests clearly, identify appropriate grad programs, weigh the advantages of philosophy vs. HPS, or write a relevant writing sample. (My first attempt at a writing sample was on Wittgenstein, Foucault, and philosophy of math. The second attempt was a little better, on Kant’s philosophy of math.) My application was a mess.\nOfficially, I was waitlisted at every grad program to which I applied; but I doubt I was considered seriously anywhere. Except at Notre Dame. Two days before the deadline in 2005 I was offered a seat in the incoming cohort. No doubt ND was scraping the bottom of the waitlist when they got to me. I was just lucky that everyone else above me had declined the offer.\nOf course, once I was admitted, I was a Notre Dame grad student and later a Notre Dame PhD, with all of the prestige and alumni connections that are attached to that name.\nThe second moment where luck played a major role in my career was actually a series of moments: the positions I’ve held since finishing my PhD. Like a lot of other precarious scholars, I’ve had to scramble from temporary position to temporary position, moving long distances every few years in order to follow the next opportunity. But unlike a lot of other precarious scholars, I’ve had a series of postdocs and other research-focused positions. Except for a single course of 6 students, I haven’t had any teaching responsibilities since 2013.\nThis series of research positions is largely the reason why I’ve been able to write so much, averaging almost 2 peer-reviewed papers per year since I finished my dissertation. (A traditional standard for tenure in philosophy at research universities is 1 single-authored paper per year.) In addition, these positions have given me opportunities to work in science policy, learn an entirely distinct set of research skills as a data scientist, and collaborate with researchers in other disciplines. Finally, they have also provided me with a comfortable income and conference travel support, giving me enough time and peace of mind to focus on research, rather than continually worrying about finding my next position while grinding through grading and commuting for poverty wages.\nOver the last 15 years, I’ve regularly had to think about what my backup plan would be if I didn’t find a good next step. For a long time the best option would have been to live with my dad in the San Francisco Bay Area and adjunct while getting a certificate to teach high school math. In the last couple years, the best backup plan would have been to work as a data scientist in the tech industry. Data science would have paid much better than high school math, but both backups would have meant the end of my research career.\nIn each case, these research positions weren’t things that were planned out well in advance. Most of the offers came in April, which is relatively late in the annual academic job market cycle. The offer for my most recent position, at UC Davis, came in early August, about a month before my then-current position ended.\nThe third moment where luck made a huge difference in my career was the job ad for UC Merced. I’m currently finishing a postdoc position at UC Davis. Davis is about 15 miles west of Sacramento; I grew up about 30 miles east of Sacramento. My entire immediate family is in the Sacramento area. I knew when I applied for the postdoc that my family couldn’t handle me moving home just to move away again two years later. I was committed to staying in Northern California.\nI recognized that this meant I was basically writing off a faculty position. (Unlike my grad school applications, I talked out the decision to accept this postdoc with a couple mentors before I had to make it.) There are several bachelor’s-granting colleges and universities in Northern California; but most are in the Bay Area, which is experiencing a severe housing crisis. Things are so bad that even UC Berkeley has lost faculty due to the cost of housing.\nDavis and the Sacramento area haven’t been affected (too much, yet) by the Bay Area housing crisis. And I thought there might be a chance that UC Davis would hire a philosopher of science during the two years of my postdoc. But really, when I took the postdoc, my plan was to use the time to figure out pathways into state government and the tech industry.\nThen, last August, a few people emailed me a job ad for a data ethics position (not at UC Merced, but another school). It was within my geographic range, and my work experience as a data scientist might compensate for the fact that I haven’t published anything on data ethics. I was hesitant to jump into the job market again, even just for one position. But once I decided to apply for that one, I went on PhilJobs and saw the UC Merced ad:\n\nWe are seeking candidates who take an interdisciplinary approach to ethics, applying ethical theories and principles to topics within the purview of [Cognitive and Information Sciences]. Sample topics might include neuroethics or the ethics of technology. Applied ethicists working on topics outside the scope of CIS who use scientific methods will also be considered (for example, someone who draws on large data sets).\n\nNot only was I a good topical fit; but also the department liked the interdisciplinary breadth of my work, my experience as a data scientist, and my connections to the data science community at UC Davis. Geographically, Merced is about 2 hours south of Sacramento, and about 2 hours and 30 minutes from my mom’s house. It’s a small city of about 80,000 people, and far enough from the Bay Area that it hasn’t really been affected by the current housing crisis yet. (Though Merced was one of the epicenters of the housing crisis in 2007-08.)\nIn other words, an interdisciplinary department at a university in an affordable location within my geographic range was looking to hire someone who does roughly what I do. During the two-year period between my moving back to California and my giving up on academia entirely to work in the tech industry. The fit in both directions could hardly have been better.\nIn all of these moments, things easily could have gone some other way. I might still have been an academic, but I probably wouldn’t be heading into a very nice tenure-track position at a research university. And there’s a very good chance I would have left academia for good, whether to teach high school math or to work in the tech industry. For all of the success I’ve had due to my hard work and “talent”, there are people who were equally “talented” and worked just as hard — if not harder — but haven’t been recognized or rewarded. Still other people would have been just as “talented” and worked just as hard, but were never given the chance. I’ll happily take your congratulations, but please keep in mind the people who weren’t lucky enough to get the kinds of opportunities that have come my way."
  },
  {
    "objectID": "posts/2017-12-27-problems-with-trolley-problems.html",
    "href": "posts/2017-12-27-problems-with-trolley-problems.html",
    "title": "Problems with Trolley Problems",
    "section": "",
    "text": "There is an enumerable set of available courses of action\nThis set of available courses of action is fixed\nThe consequences of any given course of action arise immediately\nThe consequences of each course of action can (apparently) be predicted with certainty\nThe value (goodness or badness) of any given consequence can (apparently) be determined with certainty\nThe value of any given consequence is (apparently) objective\nFor any pair of consequences, their values are (apparently) commensurable\nThere is a single agent involved\nThe agent is responsible for making exactly one choice among the available courses of action\nThis choice cannot be modified as consequences develop or more information is received\n(In the standard scenarios, the agent has no or little time to deliberate; at the same time, the scenario is presented as an object for extended deliberation)\n(In the standard scenarios, there is no specific relationship between the agent and potential crash victims)"
  },
  {
    "objectID": "posts/2017-12-27-problems-with-trolley-problems.html#oversimplified",
    "href": "posts/2017-12-27-problems-with-trolley-problems.html#oversimplified",
    "title": "Problems with Trolley Problems",
    "section": "",
    "text": "There is an enumerable set of available courses of action\nThis set of available courses of action is fixed\nThe consequences of any given course of action arise immediately\nThe consequences of each course of action can (apparently) be predicted with certainty\nThe value (goodness or badness) of any given consequence can (apparently) be determined with certainty\nThe value of any given consequence is (apparently) objective\nFor any pair of consequences, their values are (apparently) commensurable\nThere is a single agent involved\nThe agent is responsible for making exactly one choice among the available courses of action\nThis choice cannot be modified as consequences develop or more information is received\n(In the standard scenarios, the agent has no or little time to deliberate; at the same time, the scenario is presented as an object for extended deliberation)\n(In the standard scenarios, there is no specific relationship between the agent and potential crash victims)"
  },
  {
    "objectID": "posts/2017-12-27-problems-with-trolley-problems.html#misleading",
    "href": "posts/2017-12-27-problems-with-trolley-problems.html#misleading",
    "title": "Problems with Trolley Problems",
    "section": "Misleading",
    "text": "Misleading\n\n(In the standard scenarios, the two options appear to map on to utilitarianism and Kantianism, reinforcing the idea that these two theories are mutually exclusive and jointly exhaustive)\nTrolley problem scenarios (as distinct from accidental collisions) are likely to be rare (even exceedingly rare); while focus on them might give people the impression that they are likely to be frequent\n\nCompare: we aren’t really worrying in the same way about what happens if the car is struck by lightning and the AI starts to behave erratically, or how it will communicate with drivers of non-autonomous cars, cyclists, and pedestrians"
  },
  {
    "objectID": "posts/2017-12-27-problems-with-trolley-problems.html#technocratic",
    "href": "posts/2017-12-27-problems-with-trolley-problems.html#technocratic",
    "title": "Problems with Trolley Problems",
    "section": "Technocratic",
    "text": "Technocratic\n\nFocus on individual actions, not character or systems and institutions\nFits within, and so reinforces, and instrumentalist model of engineering practice. This model might itself fit within, and so reinforce, a broader isolationist model of engineering practice."
  },
  {
    "objectID": "posts/2017-12-27-problems-with-trolley-problems.html#infeasible",
    "href": "posts/2017-12-27-problems-with-trolley-problems.html#infeasible",
    "title": "Problems with Trolley Problems",
    "section": "Infeasible",
    "text": "Infeasible\n\nTrolley problem scenarios are non-monotonic: adding additional details can change the valence of the consequences\nThe space of trolley problem scenarios is non-surveyable. Designers cannot feasibly survey all (or even a large, “realistic” subset) of these problems in order to develop ex ante guidelines for autonomous vehicles’ behavior.\nCan autonomous cars reliably determine the number of pedestrians they might hit? Which ones are children? etc. Put positively, what information does an autonomous car actually have about its environment?\nDo autonomous cars deliberate like human beings? In line with either utilitarian or deontological models of practical reasoning?\n\n\nDecember 27th, 2017 1:56pm"
  },
  {
    "objectID": "posts/2020-05-12-covid-adaptive-management.html",
    "href": "posts/2020-05-12-covid-adaptive-management.html",
    "title": "COVID-19 needs adaptive management",
    "section": "",
    "text": "[I wrote this general-audience piece during the last week of April and first few days of May. I pitched it to a few popular outlets, but couldn’t find it a home.]\nWhat is the infection fatality rate for COVID-19? What groups of people are at greater risk, and what treatments are effective for lowering that risk? How many people have already been infected, and how many of them would be immune to reinfection? Which businesses are safe to reopen now, without triggering a second wave of infections, and under what guidelines? How can we conduct mass testing and contract tracing in a just and ethical way? What will the cultural and political fallout of this pandemic be for our society?\nIn the first days of May 2020, we know two things for certain about the answers to all of these questions. First, that almost nothing in certain. The data are incomplete, or unreliable, or simply won’t exist for months or years. Different models reach different conclusions, even when they use basically the same data and only subtly different methods. Preprints are available immediately, but haven’t been vetted by peer review, and may be retracted just as quickly. For any given study, it’s not hard to find a PhD on Twitter who has some scathing (and witty) criticism.\nThe other thing we know for certain is that we can’t wait until we’re certain. Policymakers at all levels — from Congress and FDA, through governors and state public health officials, down to mayors and ICU clinicians — need to make decisions now. Even inaction is a kind of action, as we’ve learned over the last three months from the contrasting cases of China and South Korea, New York and California. Any decision we make relies on answers to these questions, either implicit or explicitly.\nA traditional view of the relationship between science and policy has two distinct steps. First scientists produce knowledge. Then policymakers pick up that knowledge and use it to make decisions. This view is simple and clean. Scientists have their job; policymakers have theirs. But it requires science to be finished before policymakers can act. We need an alternative, one that enables science to inform policy even as it tries to reduce uncertainty. Adaptive management provides exactly this alternative.\nAdaptive management was originally developed in natural resources management and conservation biology. Ecosystems, such as forests and fisheries, are extremely complex systems. Organisms interact with each other and their environments in complex ways, and data are often indirect and patchy. Consequently, one forest may not react the same as any other forest in response to the same policy, and these differences can be all but impossible to predict in advance.\nAdaptive management responds to this uncertainty by treat policies as science and science as policy. Like good experiments, good adaptive management policies compare interventions to control areas and systematically collect data. It’s important to identify the major outcomes of interest, while also being on the lookout for signs of unanticipated side-effects are “unknown unknowns.” Managers may even formulate precise hypotheses, and design policies that carefully test those hypotheses. Critically, as circumstances change and understanding improves, the knowledge produced by this scientific research is used to update and adapt policies. This requires ongoing, close collaboration between policymakers, researchers, and people on the ground in the area of interest, such as workers and local residents.\nThis does not mean that adaptive management is easy. It challenges deep-seated assumptions about both good policy and good science. On the policy side, stakeholders — often relevant businesses — push for “regulatory certainty,” meaning a stable policy landscape without economic surprises. In part for this reason, Fischman and Ruhl found that US federal agencies often practice “adaptive management lite,” which includes data collection but does not have any way for policies to adapt to improved scientific understanding. On the science side, some standards of “good science” — such as the slow process of peer review — might need to be adjusted or even sacrificed for the sake of rapidly informing policy. And the close collaboration with policymakers on controversial issues risks the accusation that science has been “politicized.” These challenges may explain why adaptive management has not been widely discussed outside of natural resources management policy. Nonetheless, DeFries and Nagendra include control of infectious disease as an area where adaptive management and related approaches can be especially valuable.\nWhat would an adaptive management approach to lifting stay-at-home orders look like? First, data collection is essential. Communities must be able to know immediately when covid-19 appears in their area, and be able to respond rapidly using measures such as contact tracing. Second, policy changes might be patchy and staggered. Restrictions might be eased in one area a week or two before other areas. Some lessons learned in the early-open area might change the way opening is done elsewhere. Third, research ethics must be respected. Individual residents can’t give their consent to participating in policy experiments. But individuals can be informed about why policies are patchy, or might suddenly change. And individuals can also be given a chance to participate in making democratic policy-science. For example, town halls might be used to give residents an opportunity to share their experiences (data), interpret the most recent findings (science), and recommend courses of action (democracy). Similarly, policies must respect justice, recognizing for example that the burdens of the pandemic have fallen heavily on low-income communities of color.\nUncertainty does not need to be debilitating. But it does require thinking about policy as science and science as policy. And this, in turn, requires policymakers, researchers, and the public to work together.\n\nSuggested Readings\nDeFries, Ruth, and Harini Nagendra. 2017. “Ecosystem Management as a Wicked Problem.” Science 356 (6335): 265–70. https://doi.org/10.1126/science.aal1950.\nFischman, Robert L., and J.b. Ruhl. 2015. “Judging Adaptive Management Practices of U.S. Agencies.” Conservation Biology 30 (2): 268–75. https://doi.org/10.1111/cobi.12616.\nMitchell, Sandra D. 2009. Unsimple Truths. Science, Complexity, and Policy. University of Chicago Press.\nNorton, Bryan G. 2005. Sustainability: A Philosophy of Adaptive Ecosystem Management. Chicago: University of Chicago Press.\nWestgate, Martin J., Gene E. Likens, and David B. Lindenmayer. 2013. “Adaptive Management of Biological Systems: A Review.” Biological Conservation 158 (February): 128–39. https://doi.org/10.1016/j.biocon.2012.08.016."
  },
  {
    "objectID": "posts/2013-05-28-the-adversarial-method-where-s-moulton-.html",
    "href": "posts/2013-05-28-the-adversarial-method-where-s-moulton-.html",
    "title": "The Adversarial Method: Where’s Moulton?",
    "section": "",
    "text": "Yesterday 3:AM Magazine posted an interview with Rebecca Kukla. Kukla said some things about the adversarial method in philosophy, with which Jenny Saul disagreed. You can find a response by Eric Schliesser here, and from there links to another response or two.\nWhat I find remarkable in all of these discussion threads is the complete absence of references to Janice Moulton, and especially to her paper “Duelism in Philosophy.” Back in 1980, Moulton had already identified the Adversarial Method and made a number of sophisticated points and arguments that are being missed in the recent discussion since yesterday. Here are some highlights.\nOn the first page, Moulton makes a pluralist concession: “If it were merely one procedure among many for philosophers to employ, there might be nothing to object to. But when it dominates the evaluation, doing, and teaching of philosophy, it restricts and misrepresents what philosophy is.” (419) So her primary criticism isn’t that the Adversarial Method is somehow antithetical to women in philosophy, or that it’s essentially flawed in any way. The problem is with its exclusive (or near-exclusive) use.\nHer basic criticism, roughly, is that the Adversarial Method is only appropriate to certain contexts, namely, ones in which we actually have two adversarial positions and one needs to defeat or eliminate the other. This has two harmful downstream effects. First, the method is inappropriate to most actual contexts of practical deliberation, in which we are faced with things like moral dilemmas or the need to act under uncertainty. Thus the stereotypical practical irrelevance of much philosophy: what we do has nothing to do with what people actually need from us in their ordinary activities. As Moulton puts it,\n\n\nThe Adversary Paradigm requires only the kind of reasoning whose goal is to convince an opponent, and ignores reasoning that might be used in other circumstances: to figure something out for oneself, to discuss something with like-minded thinkers, to convince the indifferent or the uncommitted. (427)\n\n\nLater, discussing the specific emphasis on counterexamples that the Adversarial Method encourages, she points out that\n\n\nCounterexample reasoning can be used to rule out certain alternatives, or at least to show that the current arguments supporting them are inadequate, but not to construct alternatives or to figure out what principles do apply in certain situations. (429)\n\n\nSecond, the exclusive use of the Adversarial Methods tends to cause us to treat all situations as though they were adversarial situations. For example, we construct arguments designed to defeat adversarial boogeymen that exist pretty much exclusively in our own imaginations — egoists and external-world skeptics — and conceptualize the data of practical deliberation as logically exclusive alternatives — inviolable individual rights vs. utility-maximization.\nNote that this critique goes deeper than Saul’s virtue argument (the Adversarial Method tends to make philosophers uncharitable) and circumvents the worries about gender essentialism and critiques of the Adversarial Method from Louise Antony (I think that’s the paper where Antony raises those worries) and Kukla. Moulton also anticipates the historical defense of the Adversary Method, viz., that this method was used by Socrates at the “birth of Western philosophy.” She spends a few pages of her paper (423-6) carefully looking at the Socratic method of elenchus. She concludes that he was “a playful and helpful teacher” rather than “an ironic and insincere debater,” and that his aims were more to nurture reflection than defeat adversaries: “His aim is not to rebut, it is to show people how to think for themselves by examining what they think they know and seeing if it is consistent with the other things they believe.” Thus, she distinguishes a dialogue between friends who disagree and the exchange of arguments and critiques from the Adversarial Method as such. This distinction seems to be missing from the contemporary discussion.\n\nMay 28th, 2013 11:34am metaphilosophy"
  },
  {
    "objectID": "posts/2013-12-07-diversity-and-the-philosophical-tradition.html",
    "href": "posts/2013-12-07-diversity-and-the-philosophical-tradition.html",
    "title": "Diversity and “the” Philosophical Tradition",
    "section": "",
    "text": "A typical problem for introductory philosophy courses is that the list of readings is dominated by privileged authors — white men, members or adjuncts of the ruling classes of their respective societies, many of them able-bodied, often lifelong bachelors who have almost no experience interacting with children or the infirm, and so on. A typical proposed response — which fields like Literature adopted around 30 years ago — is to make the list of authors more diverse. But sometimes philosophers are hesitant to make this move:\n\nSaid colleague teaches her introductory ethics course using the historical approach … [B]ut on the historical approach, you are going to teach the historical figures: probably a couple [readings — I assume, DH] out of Plato, Hobbes, Rousseau, Nietzsche, Marx and Rawls and definitely all of Aristotle, Hume, Kant and Mill. No women. You can force women in – both times I taught it that way I used Mary Wollstonecraft, and my colleague ended her course with Philippa Foot. But neither of these figures has the stature of any of the men I’ve mentioned, and my guess is that the students can pretty much see through the token gesture (and if you teach it chronologically, the women come along pretty late anyway).\n\nThat’s a very dense response, and I could probably pull out three or four different arguments. I’m going to focus on this one here:\n\nThe philosophical tradition is dominated by privileged authors.\nThe reading list for intro philosophy courses should reflect the philosophical tradition.\nHence, the reading list for intro philosophy courses should be dominated by privileged authors.\n\nPremise 2 is too strong — there are lots of ways to introduce students to philosophy, and following the tradition is only one way. But capturing that would make premise 2 much more complicated. And anyway I’m really interested in premise 1 here.\nPremise 1 seems to rely on a certain way of thinking about traditions. On this way of thinking, traditions are strictly accruing. New things are added to the tradition — Rawls’ Theory of Justice was added just a few decades ago — but only rarely — a few additions per century. Once added, nothing is removed. In this sense, the tradition is fixed. Furthermore, additions aren’t intentional — nothing is added because someone set out to add them — and things can only be added for a limited period of time — we can’t go back and add something that was written centuries ago. Because of all this, traditions are inherently conservative.\nThis way of thinking about traditions is reflected in the remarks above that, if women are included in the reading list, it’s by “force” and as “token gestures.” Alasdair MacIntyre often attributes this view of traditions to Edmund Burke, and it was also held by the radical Enlightenment anti-traditionalists who were Burke’s main opponents. The difference was just that Burke thought that traditions were good and ought to be respected, while the Enlightenment figures thought traditions were oppressive and ought to be destroyed. (Ironically, nearly all of the authors listed in the quotation were anti-traditionalists in exactly this way.)\nMacIntyre thinks that Burke was wrong about traditions — disastrously wrong, tragically wrong — and several of his books involve reworking the idea of a tradition. Roughly speaking, on MacIntyre’s view traditions are ways of interpreting the life of our community over time — where we came from, where we are now, and where we’re headed. Since they’re interpretations, traditions are actively — even intentionally — made, maintained, and remade. They’re not a pile of accreted history, pressing down on us with the weight of centuries, but instead the way in which we understand the activities of our forerunners (including who counts as our forerunners) and how they relate to our activities and the activities of those who will come after us. Not only does this understanding change over time, but when things are going well it improves over time.\nConsider again the canonical authors in the quotation. We don’t read them uncritically, and indeed today we recognize the privileged perspective of these authors, and the way in which their privilege distorted their philosophy. We also recognize the ways in which structural oppression — of women, of persons of color and non-Europeans, of working-class persons, of persons with physical and mental impairments — ensured that only privileged perspectives were allowed to contribute to the tradition; all other perspectives were excluded, one way or another.\nIn short, we now recognize that our tradition wasn’t created by the accretion of great works of philosophy, but instead that what counted as great works was partly constituted by structures of privilege and oppression. Our tradition was made, and so we may be able to re-make it. Specifically, insofar as the philosophical tradition continues to be dominated by privileged authors, it’s because we allow it to continue in this way. Indeed, we’ve already started to re-make our interpretation of our history, by recognizing the way it’s been shaped by structural oppression and privilege. We just need to translate this process into our syllabi.\nI can see at least three means for doing this. First, we can recover the work of oppressed thinkers who were excluded from the tradition in the past. Kristin Waters’s anthology Women and Men Political Theorists (Blackwell 2000, ISBN 978-0631209805) and Karen Warren’s An Unconventional History of Western Philosophy (Rowman and Littlefield 2009, ISBN 978-0742559240) both juxtapose readings by men and women, and there are numerous anthologies for intro to philosophy courses that include the perspectives of oppressed persons in substantial ways. (If you think that it’s hard to find good readings for introductory courses by women and persons of color, at best it’s because you’re lazy.)\nSecond, we can incorporate ideological critiques of the tradition into our course content. I’m doing this in my intro to ethics course next term: after reading about utilitarianism, deontology, and virtue ethics, we’ll read a feminist critique of ethical theory and theorizing by Hilde Lindemann, then take a look at care ethics. The last time I taught intro to philosophy, Rawls and Nozick were followed by ideological critiques from Susan Okin, Anita Silvers, G.A. Cohen, and Charles Mills.\nThird, and even if without doing the first two, we can incorporate contemporary readings into our presentation of privileged authors. Penn State’s Re-Reading the Canon series, edited by Nancy Tuana, is an excellent resource here. In general, the aim of these volumes is to sort through the work of “great philosophers” from a broadly feminist perspective, asking what’s still valuable in this work, what is objectionable or must be discarded, and how these two sets are related. Many of the contributions will be too demanding for introductory students, but they can certainly inform our lectures in class.\n\nDecember 7th, 2013 11:03am metaphilosophy teaching"
  },
  {
    "objectID": "posts/2013-06-04-moocs-do-not-make-for-successful-math-classes.html",
    "href": "posts/2013-06-04-moocs-do-not-make-for-successful-math-classes.html",
    "title": "MOOCs do not Make for Successful Math Classes",
    "section": "",
    "text": "In this post, I’m going to discuss MOOCs, based on my recent experience in a Coursera course on Social and Economic Networks. I’m going to start with a brief explanation of my reasons for taking the course, give a quick overview of the structure of the course, then explain my criticism. I’ll end up arguing that, perhaps surprisingly, MOOCs are not effective, even for the math courses that, it would seem, they’re perfect for.\nI decided to take this course for two primary reasons:\n\nMOOCs are the subject of a lot of discussion in higher education right now. I’m deeply skeptical of them, but so far my worries haven’t really been empirically grounded.\nI’m quite interested in the new field of network science, and this seemed like a good opportunity to learn a bit about it.\n\nLet’s start with a brief, purely descriptive overview of the course. It was an introduction to network theory, with an emphasis on economics-style model building and analysis. (The lectures were given by Matthew Jackson, who’s in the economics department at Stanford.) While we did cover some empirical tools, and the lectures occasionally gestured in the direction of data analysis, for the most part this was essentially a math class, and the lectures relied heavily on advanced undergraduate or master’s level linear algebra and probability theory.\nAs you probably know if you’re already familiar with MOOCs, the lectures for each week were short videos, with the instructor greenscreened onto PowerPoint slides. Our videos were significantly longer than I had been led to expect, with individual videos typically 15-20 minutes long and a total of 90-120 minutes of video each week. The videos were interrupted every few minutes with a multiple-choice quiz question. The question could be skipped, and wasn’t graded. The assigned work for each week was a problem set of multiple-choice questions. There was also a final exam. “Passing” the class required completing earning at least 70% of the total possible credit. Note that, because I spent last week moving and then went out of town for a conference, I didn’t take the final exam; but I assume it was also multiple-choice questions. Also, since I ended up at 65% of the total possible points, I didn’t “pass” the class.\nInteraction between the instructor and students was basically nonexistent; this is going to be the core of my criticism below. While there was a text-based asynchronous discussion board, I only saw the instructor post anything there once. There was also a Google Hangouts chat (similar to Skype) near the end of the 8-week session, in which a handful of students got to ask the instructor questions live, for about half an hour. That session is available on YouTube. There was also a TA, who apparently is one of the instructor’s graduate students at Stanford. The TA interacted a bit more with the students on the discussion board, but seemed to primarily handle technical issues (e.g., mid-video discussion questions that had been coded incorrectly) and writing the quiz and problem set questions, as well as the explanations for the correct answers. As far as I know, he never provided additional explanations of lectures or answers to questions. This was pretty much left to other students.\nI found that this course worked well for me. I found most of the material interesting, never had problems with any of the assigned work that I couldn’t puzzle out in a few minutes (note again that I didn’t take the final exam), and learned a lot. However, based in part on my own background teaching math and my interactions with the other students in the discussion boards, I think my experience was exceptional. I have a master’s degree in math and lots of practice learning new areas of math on my own. (For example, I took graduate courses in mathematical logic without ever taking undergraduate logic.) I also taught myself the programming language Python this past winter, and I learned a lot by writing Python routines to implement various ideas presented in the lectures.\nBy contrast, my fellow students had a lot of questions, and early on one of the most active discussion threads comprised complaints about the difficulty of the course. The lectures were given at a high level of abstraction, and often both the definitions of new terminology and the examples used to illustrate those ideas were shortchanged. In a few cases, upon closer examination (by the students), it wasn’t clear how the example was supposed to illustrate the definition or principle, or the example appeared to involve a typo or miscalculation. I’m going to come back to these points in a few paragraphs.\nMy critique of this course — and, by extension, MOOCs in general — can be summarized as follows:\n\nA successful math course requires communication from the students to the instructor.\nMOOCs effectively all-but-close the channels for this communication.\nHence, math MOOCs cannot be successful.\n\nI’m going to speak specifically about math courses here. I’m doing this for two reasons. First, while my course was taught by an economist, it was basically a math course, and I think it’s safe to generalize from it to other math courses. Second, it seems that, if MOOCs are successful in any field, they would be successful in math. Humanities courses obviously require too much moderated discussion and their writing assignments can’t be graded automatically. (Despite edX’s efforts, they simply aren’t going to be able to develop a computer program that can judge how accurately a student has reconstructed Descartes’ argument for mind-body dualism, or how insightful their criticism of this argument is.) There’s no replacement for a physical lab in science courses. On the other hand, insofar as students in math classes are learning how to solve certain kinds of problems, it seems like there’s no difference between a live lecture and a MOOC. Thus my conclusion — that math MOOCs cannot be successful — is especially surprising.\nI could offer some general reasons to support my first premise, i.e., reasons why any successful course requires student-instructor communication. But instead I’m going to give a math-specific reason.\nIn a live math class — as well as a sufficiently small online math class — the instructor can interact with the students regularly, and consequently can develop a good understanding of what math teachers (especially at the primary and secondary level) call the mathematical maturity of the students, both as a whole class and individually. Mathematical maturity includes both the mathematical knowledge that students already have — whether or not they know what eigenvectors are, say — but also their ability to think abstractly. For example, junior high algebra students think of functions as operations that are done to numbers. By the time they’re high school seniors, they’re starting to think of functions as things, namely curves on a plane, and they’re ready to think of derivatives and integrals as operations that are done to these curves. It’s a few more years before they’re ready to think of both functions and derivatives as algebraic objects, in the way that studying differential forms requires.\nLearning to gauge mathematical maturity has been crucial in my summer work, teaching math and logic to gifted and talented teenagers at Johns Hopkins’ Center for Talented Youth. These kids are certainly bright, but generally aren’t very far beyond their age level in terms of mathematically maturity. I was initially hired to teach a class on chaos theory and fractals. This class failed pretty much completely, all three times that I taught it, because (for example) students weren’t sufficiently mathematically mature to understand the concept of phase space, and this concept is crucial to understand chaos theory. By contrast, because one can teach formal logic as pushing around symbols — like high school algebra — it works beautifully with these teenagers.\nIn short, any successful math class requires that the instructor accurately gauge the students’ mathematical maturity. And, obviously, this requires communication from the students to the instructor.\nFor the second premise, start by considering the channels for student-instructor communication in a traditional live lecture course. In this setting, students can interrupt the lecture — they can ask for additional clarification, an example (or an additional example) to illustrate the point under discussion, or point out possible mistakes. (Math teachers make mistakes regularly; it’s simply unavoidably easy to copy numbers incorrectly out of one’s notes or make a mental arithmetic error.)\nIn any asynchronous online course — whether it’s a 30-person online section or a MOOC — the lectures will be prerecorded, and students won’t be able to interrupt the lecture for any of the things I listed in the last paragraph. But in both a live course and a smaller online course students can also talk to the instructor outside of class, whether in person or using email or an asynchronous online discussion board. This isn’t quite as good as a question asked during a live lecture — typically more students will be confused about something than just the one who asks the question — but it still creates a channel by which the instructor can gauge the students’ mathematical maturity.\nA third channel is through graded work. Students in math courses hate being told to “show their work” — especially the students with the most “natural” understanding of the problems — but this often gives instructors valuable insight into how students are understanding the problems, and by extension the concepts relevant to the problems. Students who try to solve an algebra problem by “guess and check” — plugging in values until they get one that works — are probably at a lower level of mathematical maturity than students who use symbolic manipulations.\nSo, we have three main channels for student-instructor communication: questions during lecture, questions outside of lecture, and the work done on graded assignments. MOOCs, as per my second premise, close off all three of these channels.\nIt’s clear that MOOCs close off the first channel: the lectures are prerecorded videos. The students can pause and rewind, but can’t ask a question or point out a problem. As I mentioned above, it seemed to me and a few other students that there were a few typos and bad examples in the lectures, and these were never corrected or given more adequate explanations. Because this channel has, in some sense, a significantly higher bitrate than the other two, and both MOOCs and smaller asynchronous online courses close it off, it seems to me that this is a devastating problem for both kinds of online courses.\nI’ll come back to the second channel, questions asked outside of class time. The third channel is through graded work. At least in my course, graded work was exclusively answering multiple-choice questions. There was no way to “show your work,” and indeed it seems to me that computer evaluation of student efforts to solve math problems would be about as difficult to implement as computer evaluation of written assignments for a humanities MOOC. Furthermore, the logistics of a MOOC require several intermediaries between the instructor and the students’ submitted work, including the automated grading system and the TA. Perhaps student submissions could be randomly sampled, but of course it would be an enormous amount of person-hours to review a genuinely representative sample. Hence, MOOCs close off this third channel even more severely than smaller online asynchronous courses, at least in principle.\nFinally we have questions asked outside of class time, which in the case of a MOOC means the discussion board. As with the second channel, the logistics of MOOCs prevent the instructor from being heavily involved in the boards. They simply can’t respond to every question from the thousands of students in the course. Even a handful of TAs couldn’t keep up if the course is really larger than a couple hundred students; and then there’s the problem of effectively feeding these into the lectures.\nNote that an smaller online asynchronous course does not close off this channel. In a 30-person course, the instructor can still take questions from the students on the discussion board. For this reason, while I think that MOOCs as such cannot be successful, smaller online courses might still be. As I said above, I think this channel has a much lower bitrate than live questions, and so I remain skeptical about smaller online asynchronous courses. But I’m willing to give them the benefit of the doubt for the purposes of this post.\nSo, all together, MOOCs close the three channels of student-instructor communication found in a traditional live math course. They do not, as far as I can tell, open any alternative channels. And hence, combined with the first premise, it follows that MOOCs cannot be successful. MOOCs prevent instructors in math courses from accurately gauging the mathematical maturity of the students, and hence prevent these courses from succeeding.\nIt does not seem to me that there is any way for MOOCs to avoid this problem. If it were possible for them to create a new channel for student-instructor communication, then perhaps they could. But, by definition, we’re talking about a student-teacher ratio in the thousands. There’s no way for the instructor to keep up with the deluge of student questions.\n\nJune 4th, 2013 12:37pm higher education teaching mooc"
  },
  {
    "objectID": "posts/2013-09-27-the-climate-debate-ignorance-and-or-complexity-.html",
    "href": "posts/2013-09-27-the-climate-debate-ignorance-and-or-complexity-.html",
    "title": "The Climate Debate: Ignorance, and/or Complexity?",
    "section": "",
    "text": "Why is the climate change debate so interminable? From the perspective of many scientists, we’ve had compelling data since the 1970s and more than enough reason to reduce greenhouse gas emissions since the 1980s. Today the IPCC will release the first part of their fifth Assessment Report. But no one really expects this document to settle the debate.\nOne very common explanation for the endlessness of the debate is that the public are ignorant. This might be because they haven’t learned much of anything at all about climate change; the historian of science Robert Proctor calls this “native state” ignorance. Or the public might be ignorant because a more-or-less organized group of people, the “climate skeptics” or “climate denialists,” are deliberately feeding them misinformation to protect fossil fuel interests; Proctor calls this “ignorance as a strategic ploy.”1\nI agree that both kinds of ignorance play a role in drawing out the climate debate. But I don’t think it’s the complete explanation. So let me sketch a complementary one.\nTo begin, we need to understand how climate science works. There’s something that I call the “popular image” of climate science, according to which it’s supposed to work something like this:\n\nScientists measure the temperature and notice that things have been getting warmer since the industrial revolution.\nScientists infer that human greenhouse gas emissions are causing the warming trend.\nScientists make predictions about the future, concluding that temperature will increase by a certain amount by the year 2100 if humans continue to emit greenhouse gases.\n\nThe popular image portrays climate science as nice, neat, and fairly easy-to-understand. The problem is that it’s radically false. A more accurate image is below. If you get a bit cross-eyed trying to understand how it works, don’t worry, that’s kind of my point. You can skip to the next paragraph.\n\nScientists use statistical techniques to combine thermometer measurements (which only go back to about the mid-19th century) with physical measurements that indicate but don’t directly measure temperature (like the thickness of tree rings and pockets of air trapped in glaciers thousands of years ago).\nScientists build thousands of computer simulations to model the interactions of various factors, from human greenhouse gas emissions to increased solar activity to the chemical composition of the oceans. All of these simulations involve assumptions and simplifications that make it possible for computers to actually produce results in a reasonable amount of time.\nSome of these simulations compare combinations of major influences to the temperature data produced in step 1. It’s relatively easy to get the simulations that include human greenhouse gas emissions to statistically match the general trend of the temperature data. It’s much harder to get the simulations that don’t include human greenhouse gas emissions to match this general trend. So scientists infer that human greenhouse gas emissions are one of the major causes of the warming trend. And scientists don’t even try to get the simulations to match the temperature data exactly.\nOnce scientists have simulations that do a reasonably good job of matching the general trend in the past, they run the simulations forward to about the year 2100. Since the simulations don’t usually agree on these future projections, they’re aggregated using more statistical techniques.\n\nIn short, climate science relies on simplifying assumptions and complex statistical techniques. The conclusion that humans are responsible for climate change is based on evidence, but it’s not the easy inference that the public image suggests. Indeed, that last point is more general: climate science is not nice, neat, and easy-to-understand, as the popular image presents it.\nThis mismatch between the popular image and the complex reality gives sophisticated climate skeptics two kinds of crucial openings. First, skeptics can point to particular complicated and messy elements — weird assumptions in the statistics or computer simulations, or the complicated relationship between temperature data and the design of computer simulations. Second, skeptics can mimic parts of the climate science process in ways that will seem, to many non-scientists, to be about the same as what climate scientists are doing, while getting radically different conclusions.2\nThese skeptical arguments work on two levels. On the technical level, they assert that there are problems deep within the complexities of climate science. On the popular level, by pointing out that climate science isn’t living up to what it’s supposed to be — according to the popular image — they assert that the whole enterprise of climate science is a sham. It’s supposed to be nice and simple, but — skeptics suggest — that’s all just smoke and mirrors.\nClimate scientists and activists give good responses to these arguments on the technical level. But they don’t deal well with the popular level — they don’t take on the popular image of climate science as nice, neat, and easy-to-understand. Indeed, their simplified explanations for non-scientific audiences often reinforce this image.3 And this, I think, is one major reason why skeptical arguments are so durable, and so why the climate debate continues with no end in sight.\nCross-posted at the Rotman Institute of Philosophy blog.\n\n\n\n\nProctor explains this distinction in the opening essay of the collection Agnotology, edited with Londa Schiebinger. That book also includes an essay on the use of ignorance as a strategic ploy in the climate debate by historians Naomi Oreskes and Eric Conway. ↩︎\n\n\nFor an example of the first, see here. For an example of the second, see here. ↩︎\n\n\nFor example, compare the “Basic” and “Intermediate” responses to climate skeptics here. ↩︎\n\n\n\n\nSeptember 27th, 2013 10:59am (philosophy of) science climate science"
  },
  {
    "objectID": "posts/2020-01-19-radical-immanent-critique.html",
    "href": "posts/2020-01-19-radical-immanent-critique.html",
    "title": "Radical Immanent Critique",
    "section": "",
    "text": "My general approach to philosophy might be characterized as radical immanent critique. This might seem paradoxical. The goal of this blog post is to resolve this apparent paradox.\nLet’s start at the end. My approach is critical. By this I mean that my approach is evaluative, not purely descriptive; and that my evaluations are frequently negative. Things are going wrong; things aren’t working. This might be because the means aren’t appropriate to the ends, or it might be because the ends are bad ones.\nMy critique is radical. By this I mean that it targets deep assumptions of the scientific practices I study (and critique). The value-free ideal, and related concepts such as objectivity and partisan neutrality, is a good example here. The value-free ideal (for science) is widely assumed by scientists, policymakers, and members of the general public. It is also rarely questioned or put up for debate. This ideal has a profound influence on the way we organize scientific institutions and integrate them into our systems of public decisionmaking. In these ways, the value-free ideal is a deep assumption of science in our society. And it is an assumption I am sharply critical of. I don’t just think it’s mistaken; I think it’s pernicious. By criticizing this deep assumption, my work becomes radical.\nBut my critique is also immanent. This means that it is addressed to the social practices that I think should change — scientific research and policymaking — framed in terms that they can understand, and argued by appealing to premises that they accept. (This is also why I’ve published so much of my work in venues like Environmental Health Perspectives or Risk Analysis.) My basic argument against the value-free ideal is that it prevents scientists from doing the very things that they claim to be trying to do. For example, environmental health scientists characteristically want to produce knowledge that will be useful for protecting human health and the environment. I didn’t make up the phrase “protect human health and the environment”; it’s from the mission statement of US EPA. The value-free ideal prevents them from pursuing this goal when it’s used to portray and dismiss their research as “biased” and to justify excessively skeptical standards of evidence.\nThe sense of paradox in “radical immanent critique” comes from the fact that, on the one hand, I appeal to deep assumptions of scientific practices while also, on the other hand, criticizing deep assumptions of these same practices. How can I both appeal to and criticize an assumption?\nThe solution is that I’m appealing to some deep assumptions to criticize other ones. The aim of protecting human health and the environment is a deep assumption of environmental health science; my argument points out that this assumption is incompatible with another deep assumption, the value-free ideal. We might distinguish between radical critique and Cartesian critique. Radical critique targets deep assumptions of a social practice; but not necessarily every assumption. Only Cartesian critique attempts to target every assumption of a social practice at once. Cartesian immanent critique would be paradoxical; but there’s no logical problem with non-Cartesian radical immanent critique."
  },
  {
    "objectID": "posts/2023-01-30-democracy-defense.html",
    "href": "posts/2023-01-30-democracy-defense.html",
    "title": "Undue influence of epistemic values",
    "section": "",
    "text": "[We discussed this post on Facebook.]\nLusk (2021) examines the political legitimacy argument for the value free ideal; elsewhere collaborators and I have called this the “democracy defense” (Fernández Pinto and Hicks 2019; Hicks, Magnus, and Wright 2020). Here’s Lusk’s reconstruction of the argument (Lusk 2021, 104; edited down a bit further by me):\nLusk focuses on the Infiltration Premise, arguing that institutions for deliberative democracy can “generate [democratically] acceptable sets of values for use in scientific methodology” (Lusk 2021, 215); influences of these values would therefore not be undue, and so the Infiltration Premise would be false.\nI was actually more interested in the Legitimacy Premise. Lusk gives it a brief defense:\nThe Legitimacy Premise implicates — though does not logically entail — a difference in status between epistemic and non-epistemic values1. Specifically, the Legitimacy Premise picks out non-epistemic values as specifically problematic or worrisome if they play a role in political decisionmaking; epistemic values are not regarded as problematic or worrisome.\nBut epistemic values can have influences that undermine (descriptive) legitimacy. Consider Strengthening Transparency in Regulatory Science, a rule proposed and (very briefly) adopted by the US EPA under the Trump administration (Hicks 2022, 2023). The rule restricted the science that EPA could use to justify regulation, effectively imposing a strong open data requirement on environmental public health research. The rule was (publicly) justified by an appeal to epistemic values, namely, that open science practices will make the underlying science more reliable and reduce the rate/influence of false positive results. The rule received on the order of a million public comments, the vast majority of which were sharply negative. One of the most common criticisms was that the rule would undermine and delay regulation necessary to protect human health and the environment.\nIn other words, Strengthening Transparency was widely regarded as illegitimate because it prioritized an epistemic value (avoiding false positives) over a non-epistemic value (protecting human health and the environment).\nI suggest that the Legitimacy Premise relies on a pair of assumptions, namely, that epistemic values are politically neutral while non-epistemic values are politically controversial. Consider the block quotation from Lusk above. A principle of state neutrality only justifies excluding non-epistemic values insofar as they are controversial (ie, not neutral), and would also apply to controversial epistemic values. In the case of Strengthening Transparency, the non-epistemic value of protecting human health and the environment was, mostly, politically neutral; it was the epistemic value of eliminating false positives that was controversial.\nThe ideal of state neutrality is an ideal of depoliticization: that we can somehow get beyond the implacable chaos of power hierarchies, deep disagreement, sophistry, and naked (or artfully clothed) self-interest. The attraction of technocracy is that it purports to move us closer to this ideal, at least in the realm of the administrative state.\nSo it seems to me there is a deep irony within the “democracy defense” of the value-free ideal. Proponents of this argument present themselves as defending democracy from technocrats run amock. But the conceptual framework and institutional forms assumed by the argument function to carve out a depoliticized, technocratic realm that is protected from democratic accountability."
  },
  {
    "objectID": "posts/2023-01-30-democracy-defense.html#footnotes",
    "href": "posts/2023-01-30-democracy-defense.html#footnotes",
    "title": "Undue influence of epistemic values",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMany authors in the SVP literature use “non-epistemic” values as synonymous with social, ethical, and political values. However, if epistemic values are defined as features of scientific practice or its products that promote the attainment of truth, then paradigm “non-epistemic” values can be truth-promoting.↩︎"
  },
  {
    "objectID": "posts/2023-01-25-legitimacy-vfi.html",
    "href": "posts/2023-01-25-legitimacy-vfi.html",
    "title": "Technocratic legitimacy and the value-free ideal",
    "section": "",
    "text": "Holman and Wilholt (2022) argue that, as philosophers of science work to dismantle the value-free ideal, it is important to understand the function that the ideal played. Quoting Chesterton, they write\n\nonly when one understands how an institution arose and what purposes it was intended to serve, is one in a position to say “they were bad purposes, or they have since become bad purposes, or that they are purposes which are no longer being served.” (Holman and Wilholt 2022, 214)\n\nConsequently,\n\nif one rejects a purpose which motivated the adoption of the value-free ideal, then one must have a convincing answer as to why it should never have been endorsed, why it should no longer be endorsed, or at least show that the value-free ideal fails to serve the intended purpose. (Holman and Wilholt 2022, 215)\n\nHolman and Wilholt identify three functions for the value-free ideal:\n\nveracity\n\nscience pursues truth and avoids error\n\nuniversality\n\nscientific results are useable by anyone, whether they share scientists’ personal value-judgments or not\n\nauthority\n\nscience provides “a trustworthy body of knowledge that has broadly recognized social legitimacy” (Holman and Wilholt 2022, 214)\n\n\nIn the brief discussion that follows, Holman and Wilholt tie this sense of authority to questions of political legitimacy. In this post, I want to argue that the value-free ideal plays an important role in legitimizing the progressive, technocratic state, as illustrated in two historic moments where the value-free ideal was articulated.\nThe first moment was the Progressive Era of the late nineteenth and early twentieth centuries. The Progressive movement proposed that management of social-biological systems by credentialed, scientific experts would ensure prosperity and social peace, in areas including public health (Krieger 2011 ch. 4), the economy (Stapleford 2009), and natural resources (Gifford Pinchot), but also the human gene pool (eugenics). Versions of the value-free ideal were developed around the same time, most famously by Weber.\nThe second moment was the emergence of the risk management state in the 1970s-1980s (Beck 1992). Just as the Progressive movement was a technocratic response to the social and public health crises of the late nineteenth century, the risk management state was a technocratic response to the environmental and consumer safety crises of the 1950s-1960s. The primary methodology of the risk management state, risk assessment, institutionalized the value-free ideal, with (in principle) a strict demarcation between risk analysis — the scientific task of quantitatively determining the probability and magnitude of hazard — and risk management — the political task of designing and implementing policies to prevent and mitigate risk (Fernández Pinto and Hicks 2019).\nIn both moments, the value-free ideal enabled supporters of the technocratic state to de-politicize technocratic management, by distinguishing the realm of rational, expert decisionmaking — as “science” — from the realm of emotional, public “politics.” To reconcile technocracy with democracy — at least in the US — expert managers were placed under the authority of political appointees, who in turn were appointed and approved by elected officials. In other words, two forms of legitimacy were used to support the technocratic state. Democratic legitimacy was nominally top-down, from the voters electing politicians, who then delegate authority to experts in the executive branch agencies. While technocratic legitimacy was nominally bottom-up, depending on credentials and demonstrated competence as experts rose through the meritocratic hierarchy of those agencies. These two systems of legitimacy are not obviously compatible; consider today’s debates between epistocrats and democrats. The value-free ideal offers a solution, creating non-overlapping magisteria (Gould 2011): the systems do not come into conflict because potentially controversial values do not play a role in the experts’ work.\nUnfortunately, in this role the value-free ideal is self-defeating (Fernández Pinto and Hicks 2019). The problem is not so much that properly assessing the downstream risk of error requires appeal to potentially controversial political and ethical values, as per the argument from inductive risk (Elliott and Richards 2017). In the context of risk assessment, both scientists and policymakers have maintained the façade of value-freedom by, basically, pretending that all consideration of downstream consequences can be isolated on the political, risk management side of things.\nInstead, the value-free ideal is self-defeating because of the fact of reasonable scientific pluralism. In almost any case, it’s possible to construct a reasonable, technically sophisticated argument in favor of a different methodology, more and better data, an overlooked possibility, or an alternative explanation. Given resources and connections, parties with an interest in a certain policy outcome can pretty much always find at least one credentialed expert who will not only make an apparently reasonable, highly technical case for a preferred interpretation of “the science,” but also will castigate opposing experts as “biased” or following “an agenda.” The STS scholar Dan Sarewitz called this scenario “an excess of objectivity” (Sarewitz 2000, 2004) and I’ve referred to it as “Scientific Controversies as Proxy Politics” (Hicks 2017).\nGiven this analysis, what should a replacement for the value-free ideal attempt to do? One option would be something like “provide an alternative basis for the legitimacy of the technocratic state, more or less as it exists now.” I think many philosophers of science would support this sort of project. But to me this is not an attractive project — and I say this as someone who would probably be working for the US Environmental Protection Agency today if Clinton had won the 2016 election. Instead, I suggest that a replacement for the value-free ideal should help us democratize the technocratic state.\n\n\n\n\nReferences\n\nBeck, Ulrich. 1992. Risk society: towards a new modernity. Theory, culture & society. London ; Newbury Park, Calif: Sage Publications.\n\n\nElliott, Kevin C., and Ted Richards, eds. 2017. Exploring Inductive Risk: Case Studies of Values in Science. New York: Oxford University Press.\n\n\nFernández Pinto, Manuela, and Daniel J. Hicks. 2019. “Legitimizing Values in Regulatory Science.” Environmental Health Perspectives 127 (3): 035001. https://doi.org/10.1289/EHP3317.\n\n\nGould, Stephen Jay. 2011. Rocks of Ages: Science and Religion in the Fullness of Life. Random House Publishing Group.\n\n\nHicks, Daniel J. 2017. “Scientific Controversies as Proxy Politics.” Issues in Science and Technology, January 2017. https://www.jstor.org/stable/24891967.\n\n\nHolman, Bennett, and Torsten Wilholt. 2022. “The New Demarcation Problem.” Studies in History and Philosophy of Science 91 (February): 211–20. https://doi.org/10.1016/j.shpsa.2021.11.011.\n\n\nKrieger, Nancy. 2011. Epidemiology and the People’s Health: Theory and Context. New York: Oxford University Press.\n\n\nSarewitz, Daniel. 2000. “Science and Environmental Policy: An Excess of Objectivity.” In Earth Matters:  The Earth Sciences, Philosophy, and the Claims of Community, edited by Robert Frodeman, 79–98. Prentice Hall. http://www.cspo.org/_old_ourlibrary/ScienceandEnvironmentalPolicy.htm.\n\n\n———. 2004. “How Science Makes Environmental Controversies Worse.” Environmental Science and Policy 7 (5): 385–403. https://doi.org/10.1016/j.envsci.2004.06.001.\n\n\nStapleford, Thomas A. 2009. The Cost of Living in America: A Political History of Economic Statistics, 1880-2000. Cambridge University Press."
  },
  {
    "objectID": "posts/2019-03-14-legitimizing-values.html",
    "href": "posts/2019-03-14-legitimizing-values.html",
    "title": "Paper: Legitimizing Values in Regulatory Science",
    "section": "",
    "text": "A new paper by Manuela Fernández Pinto and me, “Legitimizing Values in Regulatory Science,” has just been published in Environmental Health Perspectives. This paper will be of interest to philosophers of science as well as the environmental public health community.\nAbstract:\nBackground: Over the last several decades, scientists and social groups have frequently raised concerns about politicization or political interference in regulatory science. Public actors (environmentalists and industry advocates, politically aligned public figures, scientists and political commentators, in the United States as well as in other countries) across major political-regulatory controversies have expressed concerns about the inappropriate politicization of science. Although we share concerns about the politicization of science, they are frequently framed in terms of an ideal of value-free science, according to which political and economic values have no legitimate role to play in science. For several decades, work in philosophy of science has identified serious conceptual and practical problems with the value-free ideal.\nObjectives: Our objectives are to discuss the literature regarding the conceptual and practical problems with the value-free ideal and offer a constructive alternative to the value-free ideal.\nDiscussion: We first discuss the prevalence of the value-free ideal in regulatory science, then argue that this ideal is self-undermining and has been exploited to delay protective regulation. To offer a constructive alternative, we analyze the relationship between the goals of regulatory science and the standards of good scientific activity. This analysis raises questions about the relationship between methodological and practical standards for good science, tensions among various important social goods, and tensions among various social interests. We argue that the aims of regulatory science help to legitimize value-laden choices regarding research methods and study designs. Finally, we discuss how public deliberation, adaptive management, and community-based participatory research can be used to improve the legitimacy of scientists as representatives of the general public on issues of environmental knowledge.\nConclusions: Reflecting on the aims of regulatory science—such as protecting human health and the environment, informing democratic deliberation, and promoting the capacities of environmental justice and Indigenous communities—can clarify when values have legitimate roles in regulatory science.\nhttps://doi.org/10.1289/EHP3317"
  },
  {
    "objectID": "posts/2014-03-12-is-motivated-reasoning-bad-reasoning-iii.html",
    "href": "posts/2014-03-12-is-motivated-reasoning-bad-reasoning-iii.html",
    "title": "Is Motivated Reasoning Bad Reasoning? III",
    "section": "",
    "text": "This is part III of a three-part series. This series will be posted simultaneously on Je Fais, Donc Je Suis, my personal blog, as well as the Rotman Institute Blog.\nIn part I of this series, I discussed motivated reasoning, reasoning in which emotions or values play a significant role. I looked at the work of Dan Kahan, which suggests that motivated reasoning is pervasive. This, in turn, suggested the gloomy thought that bad reasoning is pervasive. But, in part II, I argued that motivated reasoning is not necessarily bad reasoning. To make this case, I looked at two models of motivated reasoning (or, values in science) from STS scholar Daniel Sarewitz and philosopher of science Heather Douglas.\nAt this point, another gloomy thought seems to threaten. Motivated reasoning might not be bad reasoning, but it can still lead to intractable disagreement. If I — based in part on my values — accept the hypothesis that humans are causing climate change, and you — based in part on your values — reject this hypothesis, then how are we supposed to move forward? Now we recognize the role that emotions and values are playing in the disagreement; but disagreements over emotions and values seem to be as intractable as anything. In this final post in this series, I’ll address this gloomy thought.\nDisagreement — even intractable disagreement — is not necessarily a bad thing. Consider a simple scientific debate: A scientist says that some body of evidence supports her hypothesis. A critic points out, say, that there’s a crucial assumption linking the evidence and the hypothesis, and that the scientist hasn’t given any reason whatsoever to believe this assumption. The observers (other scientists) nod their heads, and ask the scientist why they should accept her assumption. The scientist conducts more research, and eventually produces an argument in favor of the assumption. The critic still isn’t satisfied, but the observers generally find the scientist’s argument convincing, and so they accept her hypothesis.1\nNow, in this simple scenario, the disagreement was intractable: the scientist never convinced the critic. But the disagreement was also quite productive. Without the critic, neither the scientist nor the observers would have realized that the hypothesis depended on the crucial assumption. Thanks to the critic, the scientist and the observers have a better understanding of the relationship between the evidence and the hypothesis, a better understanding of what all they’re accepting when they accept the hypothesis, and a better understanding of the ways in which the hypothesis could be undermined in the future.\nNext, suppose that the critic was critical because (and only because) of motivated reasoning — his values conflicted with the hypothesis, and so he made a special effort to identify the crucial assumption. Then motivated reasoning made for good reasoning and a productive disagreement. Indeed, without his motivated reasoning, it seems, no-one would have bothered to identify the crucial assumption.\nBut now suppose that the critic — who, remember, never accepted the hypothesis — refuses to go quietly. Maybe, despite the fact that everyone else thinks the scientist has answered his criticism, he keeps making the same point over and over again, in the published scientific literature, then on his blog, and eventually on, say, Fox News. Suppose that the hypothesis has some policy implications that certain powerful lobbyists and politicians don’t like. They promote the critic’s criticisms, despite the fact that every other scientist in the field accepts the hypothesis. Let’s call this the celebrity skeptic scenario.\nAt this point, we should worry that things could have gone wrong. I don’t want to assume that they have actually gone wrong.2 Instead, I want to ask: how could we tell whether they’ve gone wrong?\nIn part II, my argument that motivated reasoning can be good reasoning relied on this claim: “If motivated reasoning leads us to recognize when our best science is ambiguous and uncertain, and we respond to this ambiguity and uncertainty properly, then our reasoning can be good.” This suggests two ways in which motivated reasoning can be bad reasoning:\n\nThe science is no longer either ambiguous or uncertain.\nThe science is still ambiguous or uncertain, but we’re not responding to it properly.\n\nHow would these play out in the celebrity skeptic scenario? First, the scientist might respond to the critic by eliminating all of the ambiguities and providing overwhelmingly evidence. She might show that, whatever set of choices we make, we get to the same conclusion; and that there is no reason whatsoever to think that the hypothesis is wrong.\nIn this case, it seems that the critic is rejecting the hypothesis only because of his values; or that his values are simply overriding good evidence and reasoning, which clearly and unambiguously are pointing towards the hypothesis. To put it another way, the two places where Sarewitz and Douglas find room for values — ambiguity and uncertainty — have been filled in. So the critic’s motivated reasoning is, in this case, bad reasoning.\nI recognize that this scenario is a possibility. But I think eliminating all ambiguity and uncertainty will be practically impossible in many real-world cases. Consider what it would take to completely eliminate the ambiguities and uncertainties about climate change that we saw in part II.\nSecond, suppose that the scientist hasn’t eliminated all the ambiguity and uncertainty. But she fails to acknowledge this, and acts as though the science were unambiguous and certain. She could even claim that the critic is rejecting the hypothesis only because of his values. Or, in the other extreme, the critic exaggerates the ambiguity and uncertainty, and even claims that the scientist has accepted the hypothesis only because of her values. Indeed, perhaps both things happen, and each side accuses the other of thoroughgoing irrationality.\nIn these cases, the science is ambiguous and uncertain, but the participants in the debate are handling it badly. They are misrepresenting the extent of the ambiguity and uncertainty, and using this to mischaracterize their opponent in an effort to discredit him or her.\nI think that this kind of scenario is all too common in our society. Our scientific findings are frequently ambiguous and uncertain, and we fail to handle this properly. Instead, we wield the unattainable ideal of unambiguity and certainty as a rhetorical weapon (recall the discussion of “settled science” in part II).\nWhat would it look like if we handled ambiguity and uncertainty properly? Suppose the scientist — and the other members of the near-consensus — respond to the critic by saying something like this:\n\nYes, there are remaining uncertainties and ambiguities. We believe that these uncertainties and ambiguities have been sufficiently addressed, though we recognize that the critic doesn’t agree. We recognize that values and emotions play an important role in this disagreement.\nMore research could further address these issues. However, we do not think that any amount of research could address them completely. Our understanding of complex systems like these can never be totally unambiguous and totally certain.\nIn any case, if our hypothesis is correct, then we must take action now to prevent serious problems. We hope that policymakers can find policies that are acceptable to both believers and skeptics — that prevent the problems without creating other ones. But this is not something science can do.\n\nThere are still ambiguities and uncertainties. But the scientists acknowledge those ambiguities and uncertainties — they don’t try to pretend that they don’t exist — and they acknowledge that motivated reasoning is playing a role on both sides of the disagreement. This validates the critic’s criticisms, without completely conceding them.\nFurthermore — and this part is even more important — the scientists recognize that disagreements about values are best resolved by a political process, not a scientific one. Science — at least, not ambiguous and uncertain science, which is nearly all science — can’t compel us to adopt a certain policy on the basis of pure logic and evidence. Indeed, wielding logic and evidence as a political weapon will probably just make things worse. Handling ambiguity and uncertainty well — making disagreement productive — requires recognizing where science leaves off and politics must take over.\n\n\n\n\nMy discussion of this scenario is based on the account of non-value-free objectivity developed by philosopher of science Helen Longino. ↩︎\n\n\nSpecifically, I don’t want to conclude that things have gone wrong just because there’s a near-consensus and a single holdout. There have been plenty of episodes in the history of science in which the near-consensus position turned out to be wrong, and the critics (who turned out to be right) were ignored because they weren’t men, weren’t white, didn’t have positions at prestigious universities, didn’t have the support of wealthy and powerful people, and so on. ↩︎\n\n\n\n\nMarch 12th, 2014 10:00am (philosophy of) science science and values climate change"
  },
  {
    "objectID": "posts/2014-03-12-is-motivated-reasoning-bad-reasoning-iii.html#productive-disagreement-among-motivated-reasoners",
    "href": "posts/2014-03-12-is-motivated-reasoning-bad-reasoning-iii.html#productive-disagreement-among-motivated-reasoners",
    "title": "Is Motivated Reasoning Bad Reasoning? III",
    "section": "",
    "text": "This is part III of a three-part series. This series will be posted simultaneously on Je Fais, Donc Je Suis, my personal blog, as well as the Rotman Institute Blog.\nIn part I of this series, I discussed motivated reasoning, reasoning in which emotions or values play a significant role. I looked at the work of Dan Kahan, which suggests that motivated reasoning is pervasive. This, in turn, suggested the gloomy thought that bad reasoning is pervasive. But, in part II, I argued that motivated reasoning is not necessarily bad reasoning. To make this case, I looked at two models of motivated reasoning (or, values in science) from STS scholar Daniel Sarewitz and philosopher of science Heather Douglas.\nAt this point, another gloomy thought seems to threaten. Motivated reasoning might not be bad reasoning, but it can still lead to intractable disagreement. If I — based in part on my values — accept the hypothesis that humans are causing climate change, and you — based in part on your values — reject this hypothesis, then how are we supposed to move forward? Now we recognize the role that emotions and values are playing in the disagreement; but disagreements over emotions and values seem to be as intractable as anything. In this final post in this series, I’ll address this gloomy thought.\nDisagreement — even intractable disagreement — is not necessarily a bad thing. Consider a simple scientific debate: A scientist says that some body of evidence supports her hypothesis. A critic points out, say, that there’s a crucial assumption linking the evidence and the hypothesis, and that the scientist hasn’t given any reason whatsoever to believe this assumption. The observers (other scientists) nod their heads, and ask the scientist why they should accept her assumption. The scientist conducts more research, and eventually produces an argument in favor of the assumption. The critic still isn’t satisfied, but the observers generally find the scientist’s argument convincing, and so they accept her hypothesis.1\nNow, in this simple scenario, the disagreement was intractable: the scientist never convinced the critic. But the disagreement was also quite productive. Without the critic, neither the scientist nor the observers would have realized that the hypothesis depended on the crucial assumption. Thanks to the critic, the scientist and the observers have a better understanding of the relationship between the evidence and the hypothesis, a better understanding of what all they’re accepting when they accept the hypothesis, and a better understanding of the ways in which the hypothesis could be undermined in the future.\nNext, suppose that the critic was critical because (and only because) of motivated reasoning — his values conflicted with the hypothesis, and so he made a special effort to identify the crucial assumption. Then motivated reasoning made for good reasoning and a productive disagreement. Indeed, without his motivated reasoning, it seems, no-one would have bothered to identify the crucial assumption.\nBut now suppose that the critic — who, remember, never accepted the hypothesis — refuses to go quietly. Maybe, despite the fact that everyone else thinks the scientist has answered his criticism, he keeps making the same point over and over again, in the published scientific literature, then on his blog, and eventually on, say, Fox News. Suppose that the hypothesis has some policy implications that certain powerful lobbyists and politicians don’t like. They promote the critic’s criticisms, despite the fact that every other scientist in the field accepts the hypothesis. Let’s call this the celebrity skeptic scenario.\nAt this point, we should worry that things could have gone wrong. I don’t want to assume that they have actually gone wrong.2 Instead, I want to ask: how could we tell whether they’ve gone wrong?\nIn part II, my argument that motivated reasoning can be good reasoning relied on this claim: “If motivated reasoning leads us to recognize when our best science is ambiguous and uncertain, and we respond to this ambiguity and uncertainty properly, then our reasoning can be good.” This suggests two ways in which motivated reasoning can be bad reasoning:\n\nThe science is no longer either ambiguous or uncertain.\nThe science is still ambiguous or uncertain, but we’re not responding to it properly.\n\nHow would these play out in the celebrity skeptic scenario? First, the scientist might respond to the critic by eliminating all of the ambiguities and providing overwhelmingly evidence. She might show that, whatever set of choices we make, we get to the same conclusion; and that there is no reason whatsoever to think that the hypothesis is wrong.\nIn this case, it seems that the critic is rejecting the hypothesis only because of his values; or that his values are simply overriding good evidence and reasoning, which clearly and unambiguously are pointing towards the hypothesis. To put it another way, the two places where Sarewitz and Douglas find room for values — ambiguity and uncertainty — have been filled in. So the critic’s motivated reasoning is, in this case, bad reasoning.\nI recognize that this scenario is a possibility. But I think eliminating all ambiguity and uncertainty will be practically impossible in many real-world cases. Consider what it would take to completely eliminate the ambiguities and uncertainties about climate change that we saw in part II.\nSecond, suppose that the scientist hasn’t eliminated all the ambiguity and uncertainty. But she fails to acknowledge this, and acts as though the science were unambiguous and certain. She could even claim that the critic is rejecting the hypothesis only because of his values. Or, in the other extreme, the critic exaggerates the ambiguity and uncertainty, and even claims that the scientist has accepted the hypothesis only because of her values. Indeed, perhaps both things happen, and each side accuses the other of thoroughgoing irrationality.\nIn these cases, the science is ambiguous and uncertain, but the participants in the debate are handling it badly. They are misrepresenting the extent of the ambiguity and uncertainty, and using this to mischaracterize their opponent in an effort to discredit him or her.\nI think that this kind of scenario is all too common in our society. Our scientific findings are frequently ambiguous and uncertain, and we fail to handle this properly. Instead, we wield the unattainable ideal of unambiguity and certainty as a rhetorical weapon (recall the discussion of “settled science” in part II).\nWhat would it look like if we handled ambiguity and uncertainty properly? Suppose the scientist — and the other members of the near-consensus — respond to the critic by saying something like this:\n\nYes, there are remaining uncertainties and ambiguities. We believe that these uncertainties and ambiguities have been sufficiently addressed, though we recognize that the critic doesn’t agree. We recognize that values and emotions play an important role in this disagreement.\nMore research could further address these issues. However, we do not think that any amount of research could address them completely. Our understanding of complex systems like these can never be totally unambiguous and totally certain.\nIn any case, if our hypothesis is correct, then we must take action now to prevent serious problems. We hope that policymakers can find policies that are acceptable to both believers and skeptics — that prevent the problems without creating other ones. But this is not something science can do.\n\nThere are still ambiguities and uncertainties. But the scientists acknowledge those ambiguities and uncertainties — they don’t try to pretend that they don’t exist — and they acknowledge that motivated reasoning is playing a role on both sides of the disagreement. This validates the critic’s criticisms, without completely conceding them.\nFurthermore — and this part is even more important — the scientists recognize that disagreements about values are best resolved by a political process, not a scientific one. Science — at least, not ambiguous and uncertain science, which is nearly all science — can’t compel us to adopt a certain policy on the basis of pure logic and evidence. Indeed, wielding logic and evidence as a political weapon will probably just make things worse. Handling ambiguity and uncertainty well — making disagreement productive — requires recognizing where science leaves off and politics must take over.\n\n\n\n\nMy discussion of this scenario is based on the account of non-value-free objectivity developed by philosopher of science Helen Longino. ↩︎\n\n\nSpecifically, I don’t want to conclude that things have gone wrong just because there’s a near-consensus and a single holdout. There have been plenty of episodes in the history of science in which the near-consensus position turned out to be wrong, and the critics (who turned out to be right) were ignored because they weren’t men, weren’t white, didn’t have positions at prestigious universities, didn’t have the support of wealthy and powerful people, and so on. ↩︎\n\n\n\n\nMarch 12th, 2014 10:00am (philosophy of) science science and values climate change"
  },
  {
    "objectID": "posts/2014-12-08-when-feeding-the-world-doesn-t-mean-feeding-the-world-.html",
    "href": "posts/2014-12-08-when-feeding-the-world-doesn-t-mean-feeding-the-world-.html",
    "title": "When “Feeding the World” Doesn’t Mean “Feeding the World”",
    "section": "",
    "text": "A common lament in the GMO controversy is that both sides really want the same thing. As Ottoline Leyser puts it in a piece in PLoS Biology from this summer, “The most frustrating thing about this situation is that almost everyone wants the same outcome: a reliable, sustainable, equitable supply of nutritious food.” In other words, everyone wants to feed the world; given this, we should be able to reach something like a consensus on things like GM crops. Insofar as (specific) GM crops provide a safe and effective way to feed the world, everyone should find them acceptable.\nHowever, in this post I want to suggest that proponents and opponents of GM crops generally work with two rather different and incompatible conceptions of “feeding the world.” If that suggestion is right, then we would expect the two sides to have different standards for assessing GM crops. With one set of assumptions about “feeding the world,” GM crops seem quite promising. With the other set of assumptions, GM crops seem to be questionable at best. So the generic “feeding the world” rhetoric is misleading: everyone wants to “feed the world,” but people have incompatible ideas about what this actually means, and so they come to very different conclusions about the potential of GM crops.\nGM proponents — people in favor of using GM crops — generally assume what rural sociologist Philip McMichael calls the corporate food regime. Under the corporate food regime, food is treated like any other commodity — cars or clothes or iPhones — and as a commodity it is produced and traded in ways that generally serve to maximize wealth production. Since robust free trade and intellectual property harmonization serve to maximize wealth production, food is internationally regulated through more-or-less the same free trade and intellectual property regimes as cars, clothes, and iPhones. Insofar as GM crops are more economically efficient — producing more commodity food for lower input costs — they are a good way to feed the world under the corporate food regime. This is the status quo in our food system.\nGM opponents — people who don’t want us to use GM crops — generally advocate the food sovereignty regime. Put very broadly, under a food sovereignty regime, decisions about the production and allocation of food would be made through decentralized, participatory, democratic institutions, rather than harmonized global markets. Food would be valued primarily in terms of nutrition, cultural value, and sustainability (the flourishing of the biotic communities in which it was produced and consumed), and only secondarily (if at all) in economic terms.\nA food sovereignty perspective is not necessarily opposed to GM crops. It seems entirely possible that a local community somewhere might democratically decide that GM would be the best way to deal with, say, a nasty pathogen attacking an especially culturally valuable crop.\nHowever, a food sovereignty perspective is likely to be skeptical of both actual GM crops and the scientific institutions that produce them. In terms of area under cultivation, almost all GM crops actually available today are designed for the corporate food regime, and are developed to be more economically efficient than the alternatives, with little regard to nutrition, cultural value, or the flourishing of biotic communities. GM proponents suggest that other kinds of GM crops (for things like drought or flood tolerance) are under development, but this has turned out to be much more difficult than originally anticipated. At the same time, scientists — even at public research universities — find themselves ensnared in a web of intellectual property restrictions and institutional mandates to produce patents and secure industry funding. Thus, even if some scientists would like to develop new crops for the food sovereignty regime, chances are their work will be appropriated and evaluated by the corporate food regime.\nIn short, by the lights of the food sovereignty regime, GM crops are produced by and for the corporate food regime. At the very least, this places the burden of proof on GM proponents: show us (say the opponents) how GM can be used to promote food sovereignty, and not just reinforce the corporate food regime. What are the live possibilities for using GM crops in, say, polycultures of indigenous plants and animals? What about labor-intensive “peasant’’ farming systems, where many farmers have limited or no access to irrigation (much less synthetic fertilizers or pesticides)? Is it legally possible to develop new GM crops in the public domain?\nNote that all of these questions concern the effectiveness of GM crops — but only as seen from the perspective of food sovereignty. From the perspective of the corporate food regime, effectiveness is measured exclusively in terms of market productivity. Thus, insofar as proponents and opponents of GM crops are working with different food regime assumptions, they are working with very different ideas about effectiveness. That is, insofar as they don’t mean the same thing by “feeding the world,” they won’t evaluate GM crops in the same way.\n\nDecember 8th, 2014 8:59am science and values food regimes (philosophy of) science gmo controversy"
  },
  {
    "objectID": "posts/2014-03-10-is-motivated-reasoning-bad-reasoning-i.html",
    "href": "posts/2014-03-10-is-motivated-reasoning-bad-reasoning-i.html",
    "title": "Is Motivated Reasoning Bad Reasoning? I",
    "section": "",
    "text": "This is part I of a three-part series. This series will be posted simultaneously on Je Fais, Donc Je Suis, my personal blog, as well as the Rotman Institute Blog.\nSocial and political values predict your views on climate change: if you’re an egalitarian-communitarian (think: liberal, on the political left), chances are you think humans are responsible for climate change; if you’re a hierarchical-individualist (think: conservative, on the political right), chances are you think climate change is a natural phenomenon, or isn’t happening at all.\nSocial psychologist Dan Kahan argues that this is due to motivated reasoning, “the unconscious tendency of individuals to process information in a manner that suits some end or goal extrinsic to the formation of accurate beliefs.” Specifically, in the case of climate change (though not in the case of vaccines or genetically modified foods), Kahan argues that cultural cognition is at work: you accept or reject the belief that humans are responsible for climate change because you identify yourself as a member of a group (“liberals,” “conservatives”) that is committed to accepting or rejecting this belief. In other words, you believe humans are responsible for climate change because you’re a liberal and liberals believe humans are responsible for climate change.\nValues and good reasoning are often assumed to be antagonistic. There’s a metaphor that goes back to Plato: we’re in a chariot, being pulled by two horses, reason and emotion. Reason tries to pull us towards truth; but emotion pulls us away from truth. If emotion isn’t restrained, it will ride roughshod over reason and truth. (That last bit muddles the metaphor, but you get the idea.) Kahan’s defintion of motivated reasoning seems to suggest this antagonism. The end or goal is, as he puts it, “extrinsic to accurate belief”; it’s external to, irrelevant to, perhaps even opposed to the truth.\nOn this antagonistic picture, motivated reasoning seems to be bad reasoning. Consider two cases: motivated reasoning leads us to accept a false claim; or it leads us to accept a true claim. In the first case, things have clearly gone wrong: we believe something that’s false. In the second case, we’ve gotten to the right conclusion (we accept a true claim), but in the wrong way (following emotion and values rather than evidence and logic). In philosophy-ese, motivated reasoning seems to lead to beliefs that are false, unjustified, or both.\nWorking within this antagonistic picture, you might think that we can avoid motivated reasoning by improving science literacy — how much people know about science — and numeracy — “not just mathematical ability but also [the] disposition to engage quantitative information in a reflective and systematic way and use it to support valid inferences” (6). To go with Plato’s metaphor: by making the reason horse strong and powerful, we will move towards truth, whichever direction the emotion horse happens to want to go. We will tend to get justified true beliefs by overwhelming the influence of emotion or values.\nKahan’s work suggests that this isn’t the case. In one line of research, he divides people into two groups: high science literacy/numeracy [high SLN] and low science literacy/numeracy [low SLN]. The antagonistic picture suggests that (a) people in high SLN group will tend to agree with each other — they’re all being moved towards truth by a relatively strong reason horse — while (b) people in the low SLN group will tend to disagree with each other — they’re being moved in all different directions by a relatively strong emotion horse.\nThis prediction gets things exactly backwards: polarization increases with science comprehension. Consider this image, from Kahan’s blog:\n\nOn the left is the prediction: as SLN increase (as we move from left to right in the graph) the two groups converge. On the right is actual survey data: as SLN increases, egalitarian-communitarians (liberals) become more worried about climate change while hierarchical-individualists (conservatives) become less worried. The two groups move further apart, not together!\nThese results suggest that motivated reasoning is pervasive. High science literacy and numeracy don’t help; indeed, they just seem to make things worse. In terms of Plato’s metaphor, it seems that we don’t have two horses, reason and emotion. It’s more like reasoning is the horse pulling the chariot, but emotion is the charioteer, the one who ultimately decides which direction reason is going to go. Kahan puts it less metaphorically:\n\nWhen the data, properly construed, supported an ideological noncongenial result, high numerate subjects latched onto the incorrect but ideologically satisfying heuristic alternative to the logical analysis required to solve the problem correctly.\n\nSo it seems that we’re doomed to bad reasoning. Motivated reasoning leads us to false or unjustified beliefs, and motivated reasoning is pervasive.\nI don’t think this is necessarily the case. Specifically, I don’t think that motivated reasoning necessarily leads us to false or unjustified beliefs. Certainly they do sometimes. But not in all cases. In other words, the antagonistic picture is wrong. And that’s what I’m going to argue in part II of this post.\n\nMarch 10th, 2014 10:01am (philosophy of) science science and values climate change"
  },
  {
    "objectID": "posts/2014-03-10-is-motivated-reasoning-bad-reasoning-i.html#the-pervasiveness-of-motivated-reasoning",
    "href": "posts/2014-03-10-is-motivated-reasoning-bad-reasoning-i.html#the-pervasiveness-of-motivated-reasoning",
    "title": "Is Motivated Reasoning Bad Reasoning? I",
    "section": "",
    "text": "This is part I of a three-part series. This series will be posted simultaneously on Je Fais, Donc Je Suis, my personal blog, as well as the Rotman Institute Blog.\nSocial and political values predict your views on climate change: if you’re an egalitarian-communitarian (think: liberal, on the political left), chances are you think humans are responsible for climate change; if you’re a hierarchical-individualist (think: conservative, on the political right), chances are you think climate change is a natural phenomenon, or isn’t happening at all.\nSocial psychologist Dan Kahan argues that this is due to motivated reasoning, “the unconscious tendency of individuals to process information in a manner that suits some end or goal extrinsic to the formation of accurate beliefs.” Specifically, in the case of climate change (though not in the case of vaccines or genetically modified foods), Kahan argues that cultural cognition is at work: you accept or reject the belief that humans are responsible for climate change because you identify yourself as a member of a group (“liberals,” “conservatives”) that is committed to accepting or rejecting this belief. In other words, you believe humans are responsible for climate change because you’re a liberal and liberals believe humans are responsible for climate change.\nValues and good reasoning are often assumed to be antagonistic. There’s a metaphor that goes back to Plato: we’re in a chariot, being pulled by two horses, reason and emotion. Reason tries to pull us towards truth; but emotion pulls us away from truth. If emotion isn’t restrained, it will ride roughshod over reason and truth. (That last bit muddles the metaphor, but you get the idea.) Kahan’s defintion of motivated reasoning seems to suggest this antagonism. The end or goal is, as he puts it, “extrinsic to accurate belief”; it’s external to, irrelevant to, perhaps even opposed to the truth.\nOn this antagonistic picture, motivated reasoning seems to be bad reasoning. Consider two cases: motivated reasoning leads us to accept a false claim; or it leads us to accept a true claim. In the first case, things have clearly gone wrong: we believe something that’s false. In the second case, we’ve gotten to the right conclusion (we accept a true claim), but in the wrong way (following emotion and values rather than evidence and logic). In philosophy-ese, motivated reasoning seems to lead to beliefs that are false, unjustified, or both.\nWorking within this antagonistic picture, you might think that we can avoid motivated reasoning by improving science literacy — how much people know about science — and numeracy — “not just mathematical ability but also [the] disposition to engage quantitative information in a reflective and systematic way and use it to support valid inferences” (6). To go with Plato’s metaphor: by making the reason horse strong and powerful, we will move towards truth, whichever direction the emotion horse happens to want to go. We will tend to get justified true beliefs by overwhelming the influence of emotion or values.\nKahan’s work suggests that this isn’t the case. In one line of research, he divides people into two groups: high science literacy/numeracy [high SLN] and low science literacy/numeracy [low SLN]. The antagonistic picture suggests that (a) people in high SLN group will tend to agree with each other — they’re all being moved towards truth by a relatively strong reason horse — while (b) people in the low SLN group will tend to disagree with each other — they’re being moved in all different directions by a relatively strong emotion horse.\nThis prediction gets things exactly backwards: polarization increases with science comprehension. Consider this image, from Kahan’s blog:\n\nOn the left is the prediction: as SLN increase (as we move from left to right in the graph) the two groups converge. On the right is actual survey data: as SLN increases, egalitarian-communitarians (liberals) become more worried about climate change while hierarchical-individualists (conservatives) become less worried. The two groups move further apart, not together!\nThese results suggest that motivated reasoning is pervasive. High science literacy and numeracy don’t help; indeed, they just seem to make things worse. In terms of Plato’s metaphor, it seems that we don’t have two horses, reason and emotion. It’s more like reasoning is the horse pulling the chariot, but emotion is the charioteer, the one who ultimately decides which direction reason is going to go. Kahan puts it less metaphorically:\n\nWhen the data, properly construed, supported an ideological noncongenial result, high numerate subjects latched onto the incorrect but ideologically satisfying heuristic alternative to the logical analysis required to solve the problem correctly.\n\nSo it seems that we’re doomed to bad reasoning. Motivated reasoning leads us to false or unjustified beliefs, and motivated reasoning is pervasive.\nI don’t think this is necessarily the case. Specifically, I don’t think that motivated reasoning necessarily leads us to false or unjustified beliefs. Certainly they do sometimes. But not in all cases. In other words, the antagonistic picture is wrong. And that’s what I’m going to argue in part II of this post.\n\nMarch 10th, 2014 10:01am (philosophy of) science science and values climate change"
  },
  {
    "objectID": "posts/2019-06-18-network-analysis-to-evaluate.html",
    "href": "posts/2019-06-18-network-analysis-to-evaluate.html",
    "title": "Paper: Network analysis to evaluate the impact of research funding on research community consolidation",
    "section": "",
    "text": "A new paper by myself and three UC Davis collaborators (David Coil, Carl Stahmer, and Jonathan Eisen), “Network analysis to evaluate the impact of research funding on research community consolidation,” has just been published in PLOS One. This paper will most be of interest to the science policy, science of team science, and bibliometrics communities.\nAbstract:\nIn 2004, the Alfred P. Sloan Foundation launched a new program focused on incubating a new field, “Microbiology of the Built Environment” (MoBE). By the end of 2017, the program had supported the publication of hundreds of scholarly works, but it was unclear to what extent it had stimulated the development of a new research community. We identified 307 works funded by the MoBE program, as well as a comparison set of 698 authors who published in the same journals during the same period of time but were not part of the Sloan Foundation-funded collaboration. Our analysis of collaboration networks for both groups of authors suggests that the Sloan Foundation’s program resulted in a more consolidated community of researchers, specifically in terms of number of components, diameter, density, and transitivity of the coauthor networks. In addition to highlighting the success of this particular program, our method could be applied to other fields to examine the impact of funding programs and other large-scale initiatives on the formation of research communities.\nhttps://doi.org/10.1371/journal.pone.0218273"
  },
  {
    "objectID": "posts/2014-10-13-work-and-hackwork.html",
    "href": "posts/2014-10-13-work-and-hackwork.html",
    "title": "Work and Hackwork",
    "section": "",
    "text": "Should you love what to do, or “live to work”? This post suggests not:\n\nUnder neoliberalism, Lordon argues, employers have enough leverage to insist that workers’ desires align completely with that of employers, so that all their life force essentially goes into enterprise.\n\nFor example,\n\nPret à Manger notoriously demands this kind of excitement from its fast-food workers (they are supposed to smile and make small talk with customers and rate other employees in terms of their team spirit and so on) — in Lordon’s words, such employers aim for “the ultimate behavioral performance in which the prescribed emotions are no longer merely outwardly enacted, but ‘authentically’ felt.” They have to smile and “really” mean it. They have to eradicate pretending, eradicate the gap demarcated by the concept of “service” and sell the pretense that customers and servants are equal, only the servant has graciously and eagerly volunteered to kiss the customer’s ass.\n\nIn other words, the notion that you should love what you do is just another way for your employer to control and exploit you. If you “should” love what you do but in fact don’t, you “deserve” to lose your job.\nOn the other hand, there are long traditions of love of work in both the crafts and professions. A skilled craftsperson takes appropriate pride in her or his technical achievements, which are the product of many long hours of apprenticeship and practice. Professionals choose their vocation (clergy, academic, physician) not as “just a job” — not for the sake of money — but in response to a “call.”\nBut skilled workers don’t love all of the work that they do. They love the most valuable and challenging aspects of their work, the things that can lead to great achievements. Call this edifying work. I’m inclined to agree that love of edifying work is a virtue. But there’s also hackwork, which is work done without skill, intelligence, or imagination. Hackwork isn’t edifying — it doesn’t cultivate our abilities or lead to any other improvements — and it’s not valuable for its own sake — it’s just a means to get by and survive another day. Consequently, love of hackwork is a vice. A virtuous worker loves edifying work — lives to do edifying work — but does hackwork only grudgingly or with resignation and when it’s necessary.[1]\nPret à Manger is not trying to cultivate a love of edifying work among its employees — or, at least, I’m guessing that it’s not. This is because — again, I’m guessing — working at Pret à Manger generally does not involve much skill, intelligence, or imagination, at least for “team members.” The tasks of team members are routinized and highly regulated. In other words, the kind of work done at Pret à Manger is hackwork. And so the problem is that Pret à Manger is trying to cultivate vices among its employees.\nMy friend Charles shared the blog post above on Facebook, with some vague musings about its relevance to higher education. Traditionally, of course, education is one of the professions; and consequently educators are supposed to love what we do. This is in line with what I’ve said above: given that education is edifying work, it’s virtuous to love being an educator.\nHowever, in the names of “accountability” and “efficiency,” there are several prominent efforts to make education more routinized and regulated. Educators at all levels must regularly document their work in order to satisfy accountability requirements. MOOCs and other projects are attempting to replace live interactions and personalized feedback with prerecorded lectures and automated quizzes. These efforts are gradually making education more hackwork than edifying work. Consequently, the love of being an educator is becoming less of a virtue and more of a vice.\nThis does not imply that we should abandon our love of education. The alternative is to resist the efforts to transform education into hackwork.\n\n\n\n\nOne important exception: Sometimes hackwork can be done as a sort of break. For example, chopping wood and some kinds of cleaning (such as washing dishes or routine sweeping) are clearly hackwork. And many people enjoy doing these activities, as a way to “take their mind off things.” I would suggest that, in these cases, hackwork is enjoyed as recreation; that is, as a way to rest and refresh oneself, especially mentally and emotionally.  ↩︎\n\n\n\n\nOctober 13th, 2014 10:17am virtue ethics neoliberalism higher ed"
  },
  {
    "objectID": "posts/2023-05-04-tmfast.html",
    "href": "posts/2023-05-04-tmfast.html",
    "title": "New preprint: tmfast fits topic models fast",
    "section": "",
    "text": "Yesterday I published a new preprint on the arXiv: “tmfast fits topic models fast”. Here’s the abstract:\n\ntmfast is an R package for fitting topic models using a fast algorithm based on partial PCA and the varimax rotation. After providing mathematical background to the method, we present two examples, using a simulated corpus and aggregated works of a selection of authors from the long nineteenth century, and compare the quality of the fitted models to a standard topic modeling package.\n\nThe package itself is available on Github."
  },
  {
    "objectID": "posts/2013-02-20-the-tangled-food-web-we-weave.html",
    "href": "posts/2013-02-20-the-tangled-food-web-we-weave.html",
    "title": "The Tangled (Food) Web We Weave",
    "section": "",
    "text": "The basic reason I find the food system so fascinating is the complex and often surprising connections between issues that, at first glance, don’t seem to have much to do with each other.\nConsider the amount of processed food we eat, the ecological concerns we have about food production (pesticides, nitrogen runoffs, CAFOs), and the distribution of farmland ownership, specifically the fact that, both in the U.S. and globally, the total number of farms has sharply decreased at the same time as the total area farmed has slightly increased. The first two, at least, seem to be widely discussed in our society in recent years. But we don’t seem to discuss them together, as this Google Trends graph indicates:\n(Since 2006, the correlation between the two trends, as measured by R^2, is only 0.137, which is low.) And based on my interactions with students over the past year, many people seem to be vaguely aware of the consolidation of farmland. It’s regarded as bad, but also in a vague way; and it doesn’t seem to strike many people as being as important as the first two issues.\nIn fact, these three issues are quite closely connected. The mass consumption of processed food requires high economies of scale in food processing and agriculture. You don’t get cheap hamburgers unless they’re made in a factory using cheap meat; and you don’t get cheap meat without a fast-moving “disassembly line” in the slaughterhouse, cheap feed, and a minimal labor force. Similarly, you don’t high fructose corn syrup, xantham gum, and the other chemical constituents of cheap processed food (plus the cheap feed for the cheap meat) without cheap corn.\nEconomies of scale in food processing and production, in turn, require product uniformity and land consolidation. Product uniformity means that the unprocessed agricultural outputs – potatoes, corn, cattle, for example – are the same. The potatoes and corn are all the same size and have the same starch content (as the other potatoes and corn, respectively). The cattle all take the same amount of time and feed to go from birth to “mature” (i.e., ready-to-slaughter) weight, and after slaughter their meat has the same fat content. Product uniformity requires process uniformity: growing all of the potatoes, all of the corn, all of the cattle, in the same way everywhere, using the same varieties or breeds and the same chemical inputs (pesticides, fertilizers, feeding supplements, antibiotics, and so on).\nAnd it’s exactly this process uniformity that gives rise to the ecological problems. Pesticide-resistant insects and herbicide-resistant weeds have evolved so quickly because we’re using just a few different kinds of pesticides and herbicides in just a few different ways. Growing just a few varieties of corn and soybeans means we can’t build soil through more complex crop rotation and intercropping methods, so we have to use chemical fertilizers, which tend to run off and produce the “dead zones” in, for example, the Gulf of Mexico. Raising millions of closely-related cattle in the close, filthy, and extremely efficient conditions of the feedlot – and feeding them antibiotics to prevent disease outbreaks and make them grow faster – creates the ideal breeding ground for antibiotic-resistant bacteria.\nEconomies of scale also involve the use of agricultural machinery, such as large planters and harvesters. These machines are capital intensive – they require a large initial investment – as opposed to older, labor intensive technologies. The increase in capital intensity and the decrease in labor intensity both lead to consolidation. Say a single farming family can manage 200 acres with a horse-drawn plow. The 448,000 acres (700 square miles) of a typical county in Nebraska would be farmed by not quite 2,500 families, for a sustainable rural county population of 12-15 thousand people. But now a single farming family, using capital intensive machinery, can manage 1,000 acres or more. We only need 500 farming families, or about 2-3 thousand people, in the same typical Nebraska county. That’s not even enough for a consolidated high school. So economies of scale lead to farmland consolidation, and the devastating depopulation of farming communities.\nFinally, these connections depend on the particular cultural, economic, and technological context of our food system. Cultural and economical because farmers, in our society, have to be profitable more-or-less annually in order to keep their farms. Technological because future agricultural technologies – certain kinds of genetically modified crops – may or may not enable us to keep process uniformity without, for example, the chemical fertilizers. And so these surprising, complex connections are dynamic, likely to change over time as the cultural, economic, and technological context changes. What more could a scholar ask of his research subject?\n\nFebruary 20th, 2013 12:12pm economics food (philosophy of) science"
  },
  {
    "objectID": "posts/2018-11-06-why-baier.html",
    "href": "posts/2018-11-06-why-baier.html",
    "title": "Slides: Why Baier? Feminism, Trust, and Power",
    "section": "",
    "text": "Here are the slides for my talk at the Philosophy of Science Association meeting this past weekend in Seattle: Why Baier? Feminism, Trust, and Power\nHere’s the original abstract for this talk, written last fall."
  },
  {
    "objectID": "posts/2018-11-06-why-baier.html#why-baier-feminism-trust-and-political-critiques-of-science",
    "href": "posts/2018-11-06-why-baier.html#why-baier-feminism-trust-and-political-critiques-of-science",
    "title": "Slides: Why Baier? Feminism, Trust, and Power",
    "section": "Why Baier? Feminism, Trust, and Political Critiques of Science",
    "text": "Why Baier? Feminism, Trust, and Political Critiques of Science\nFeminist philosophers of science continue to draw on ideas about trust and trustworthiness developed by feminist ethicist Annette Baier more than thirty years ago (Baier 1986). Why does Baier remain popular among feminist philosophers? In this paper, I suggest that Baier’s account of trust supports a characteristic kind of feminist critique of science. This feature of Baier’s account means that it is also relevant, as a descriptive theory, to contemporary empirical work on conservative attacks on environmental science.\nFeminists have long recognized that science has the potential to be an invaluable tool for challenging sexism and dismantling patriarchal power structures. Thus many feminists are not interested in attacking science as such. However, feminists have also long recognized that science has often been used to defend sexism and prop up patriarchy (Harding 1991, Wylie and Nelson 2007). The feminist critique does not aim to destroy science, but instead to redirect it from oppressive to liberatory ends.\nBaier’s account of trustworthiness distinguishes “competence” from “good will,” and requires both for trustworthiness. This means that patriarchal science can still be conducted competently — it can still produce knowledge using reliable methods. But, by being used in sexist and patriarchal ways, patriarchal science can be understood as bearing ill will towards women. Patriarchal science is therefore untrustworthy, even when scientists are epistemically competent. By contrast, on other influential accounts, epistemic trustworthiness depends exclusively on epistemic competence. These accounts do not fit as well with the complex feminist stance towards science.\nBaier’s account also provides theoretical support for the “anti-reflexivity thesis,” developed by sociologist Aaron McCright and collaborators (McCright et al. 2013). McCright has found that political conservatives tend to display low trust in “impact science” — “science that identifies environmental and public health impacts of economic production” — but also high trust in “production science” associated with technological development. That is, conservatives do not distrust or reject science as such; they are not “anti-science.” Instead, they are anti-environmental regulation. Indeed, the loudest critics of EPA and other regulatory agencies often promote the benefits of practical scientific research and technological development.\nBaier’s account allows us to recognize that conservative distrust of impact science is not necessarily based on ignorance or irrationality. Rather, conservatives might believe that impact scientists bear them (or entities with which they identify, such as the fossil fuels or chemical industry) ill will. This ill will would be grounds to find impact scientist untrustworthy, independent of whether impact scientists are epistemically competent.\nReferences\nBaier, Annette. 1986. “Trust and Antitrust.” Ethics 96 (2):231–60.\nHarding, Sandra G. 1991. Whose Science? Whose Knowledge?: Thinking from Women’s Lives. Ithaca, N.Y: Cornell University Press.\nMcCright, Aaron M, et al. 2013. “The Influence of Political Ideology on Trust in Science.” Environmental Research Letters 8 (4):044029.\nWylie, Alison and Lynn Hankinson Nelson. 2007. “Coming to Terms with the Values of Science.” In Harold Kindcaid, John Dupré, and Alison Wylie, eds., Value-Fre Science? Oxford: Oxford University Press."
  },
  {
    "objectID": "posts/2020-05-18-dayton-talk.html",
    "href": "posts/2020-05-18-dayton-talk.html",
    "title": "Talk: “When Virtues are Vices: The Weaponization of Scientific Norms”",
    "section": "",
    "text": "Back in March, I was supposed to present at a colloquium at the University of Dayton on “Doing Science in a Pluralistic Society.” Due to the pandemic, the colloquium was turned into a virtual conference over two Fridays in April. The recording of my talk is now on Youtube and at the bottom of this post."
  },
  {
    "objectID": "posts/2020-05-18-dayton-talk.html#when-virtues-are-vices-the-weaponization-of-scientific-norms",
    "href": "posts/2020-05-18-dayton-talk.html#when-virtues-are-vices-the-weaponization-of-scientific-norms",
    "title": "Talk: “When Virtues are Vices: The Weaponization of Scientific Norms”",
    "section": "When Virtues are Vices: The Weaponization of Scientific Norms",
    "text": "When Virtues are Vices: The Weaponization of Scientific Norms\nConservative critics of mainstream climate science and environmental health science often “weaponize” traditional scientific virtues to manufacture doubt and slow the regulatory process. For example, climate skeptics appeal to Popperian falsifiability and Mertonian norms such as organized skepticism, arguing that mainstream climate science is unfalsifiable and propped up through closed scientific communities. The “Secret Science Reform Act,” which was passed by the US House of Representatives in both 2014 and 2015, would have prohibited US EPA from using “scientific technical information” unless it was “publicly available online in a manner that is sufficient for independent analysis and substantial reproduction of research results.”\nThese rhetorical appeal to traditional scientific virtues make it difficult to dismiss these anti-environmentalists as “anti-science.” Indeed, the effect of these appeals is to represent anti-environmentalists as defenders of science from the threats of politicization.\nI first argue that this kind of appeal to scientific virtues depends on a “narrowly epistemic” conception of the aims of scientific research. On this conception, the sole constitutive aim of science is to produce impartial knowledge. Other aims — such as protecting human health and the environment — are less important. The traditional scientific virtues reflect this strict separation of constitutive epistemic aims and practical uses of science.\nI go on to propose an alternative conception of the aims of scientific research, according to which epistemic and pragmatic aims of science can be equally important and mutually influencing. This view of the aims of science supports an alternative understanding of scientific virtues. On this understanding, when traditional scientific “virtues” are weaponized by conservative anti-environmentalists — when they are used to delay protective regulation, frustrating the constitutive pragmatic aims of a scientific field — they are actually vices rather than virtues."
  },
  {
    "objectID": "posts/2020-05-08-no-hug.html",
    "href": "posts/2020-05-08-no-hug.html",
    "title": "Why I won’t give you a hug",
    "section": "",
    "text": "Dear family member,\nI understand your perspective. The rural Californian county where you live — like mine — has had a few dozen confirmed cases of COVID-19, and no deaths. You talked to another member of our extended family, in an even more remote and rural county; he told you that he, his family, and his buddies all had something that sounds a lot like COVID-19, back in February. He couldn’t get tested at that point, of course, but they had all the symptoms: dry cough, fever, body aches. You’ve read a couple of news stories, saying that we know COVID-19 was in the state in February, and that a lot more people had antibodies for the disease than we first suspected (though, in our state, this is maybe as high as 4%, not 40%).\nMeanwhile, due to health issues in your household, you’ve been under quarantine since the first of March. You missed one family birthday gathering — the last family gathering of any kind — in the first week of March, not long before the whole state went into quarantine. That’s exhausting. Lots of small business owners in your community haven’t been eligible to apply for unemployment. Some of your neighbors have gotten their relief checks — a single check, that can cover some fraction of one month’s rent, their only income over the past six weeks. Others — especially older folks who don’t do their taxes online — are still waiting for their checks. You retired recently, and right now your primary income is rent from a small cottage on your property. But you’re not entirely sure whether the family living in the cottage is working right now. If they can’t pay their rent, you’re not quite sure how you’ll be able to pay the electric bill and get groceries.\nIt’s not that you want to open the doors wide across the state. The outbreak in the Bay Area, about two hours from both of us, is still simmering. And things are still quite bad in LA. But, you think, rural counties like ours, with just a few dozen confirmed cases and no deaths, we should be able to open things back up. For our communities, the treatment seems to be worse than the disease.\nThat’s your perspective, and I can understand and respect why you think that. It would be great for people to go back to work, for the state and national parks to reopen, for our family to get together again for food and hugs.\nBut I have a different perspective. In major epidemics this one, outbreaks and quarantines tend to come in cycles. Things start to get bad, quarantine is imposed until they simmer down again, things open up, and then things start to get bad again. And because COVID-19 is so sneaky, it’s incredibly easy for things to get bad again, incredibly quickly. In Chicago, in a series of family get-togethers, one infected person unwittingly spread the disease to 16 other people. Three of those people died, and several went on to infect further people, all within a few weeks. While things are quiet in our rural counties now, just fewer than 10 active, confirmed cases, I worry that opening things up too quickly will lead to 100 cases, then 500, and then our small rural hospitals will be overwhelmed.\nTo prevent another bad way of cases, we need to be able to do the two things that the CDC has recommended since January: lots of testing, and rapid contact tracing. In California, the latest recommendation is that we should be able to do 108 tests per 100,000 people. (This is actually lower than the recommendation of 152 tests per 100,000 people from mid-April.) But currently we’re only doing 74 tests per 100,000 people. We’ve expanded our testing capacity, but we’ve still got a ways to go. Things are even worse in most other states.\nI know you don’t like to talk about politics. But for me the crisis is unavoidably political, in terms of both the disease itself as well as the economic effects of the quarantine. As the epidemic unfolded in Asia in January and February, Trump was repeatedly warned that it would be here and that we needed to prepare. The Defense Production Act gives Trump the authority to order manufacturers to produce essential goods — like protective equipment for doctors and the swabs and chemicals needed for testing. He’s barely used it, which is part of the reason why our testing capacity is still so far below where it needs to be.\nOn the economic side, some of the effects of the quarantine are unavoidable. Shutting down the economy for 6 weeks or longer means that some economic pain is unavoidable. But we’re not distributing this pain fairly. Restaurant servers and kitchen staff are getting that one check, plus a little bump in unemployment benefits. The airline industry is getting tens of billions of dollars, and real estate investors — including Trump and his family — are getting $170 billion.\nWhen we talked the other day, I told you about Germany’s approach to the crisis, where the government covers workers’ salaries, avoiding the disastrous spike in unemployment that we’ve seen in the US. You asked me how we could possibly pay for such a program. For the past few years, Amazon has made billions of dollars of profits, but paid no federal income tax, and even received refunds of more than $100 million. In part, this is due to Trump’s 2017 corporate tax cut. Since March, Amazon’s stock price has increased about 25%, as customers shift from brick-and-mortar stores to shopping online during the pandemic. Even if you don’t agree with me that a substantial general increase in taxes on the wealthy and large corporations is a good idea, couldn’t we agree that a one-time tax on windfall profits would be fair?\nI hope that I’m wrong about what’s going to happen over the next few weeks. Like you, I think it would be wonderful if most people had already been exposed, that cases don’t rise as we reopen the economy, and that the emergency field hospitals and testing sites we’ve built over the last six weeks turn out to be unnecessary. I want to go backpacking, enjoy a latte on a Saturday afternoon before going to the movies, and most importantly get together with you and everyone else for a family dinner. But we’re far from certain that this best-case scenario is what’s going to happen. From my perspective, we need to move slowly, and be prepared for the worst case.\nI love you. But I won’t hug you. Not until June.\nDan"
  },
  {
    "objectID": "posts/2015-08-19-industry-funded-research-and-publishing-independence.html",
    "href": "posts/2015-08-19-industry-funded-research-and-publishing-independence.html",
    "title": "Industry-funded research and publishing independence",
    "section": "",
    "text": "Industry funding of scientific research was in the news last week. The most prominent story was that Coca-Cola is funding obesity research. But I had an interesting Twitter exchange about a different story: Clif Bar and Organic Valley are funding research on organic agriculture at UW-Madison.\nTo paraphrase the Tweet from Tamar Haspel at the top of the Twitter exchange, it’s hard to see why funding from Clif Bar and Organic Valley for organic research is substantively different funding from Monsanto or Syngenta for agricultural biotechnology research. If one is bad, then so is the other; or conversely, if one is perfectly acceptable, then so is the other. Or, more generally, how do we tell whether the values of funders are having an illegitimate influence on research?\nBrandon Horvath offered an interesting response to that question, in a tweet which seems to have disappeared. As I recall, his criterion was something like “being able to publish unfavorable findings.” That is, if a research group can publish findings that are unfavorable to their funders’ interests, then the influence of the funders’ values is legitimate.\nThere’s a lot that I could say about this proposal. For this post, I’m going to assume that Horvath’s proposal is a good one, and that this kind of independence in publishing does give reason to think that the influence of funders’ values is legitimate.\nWhat I want to consider are some epistemic challenges for this proposal. That is, I want to consider how people outside the research group can get evidence that Horvath’s condition is satisfied (or not) in any particular case.\nFirst, Horvath’s condition involves a counterfactual: it’s about what would happen if the research group decided to try to publish unfavorable findings. If a research group does, or has, published unfavorable findings, then it’s relatively easy to show that the condition is satisfied: “we found that this plant-incorporated pesticide didn’t work very well, even though Syngenta gave us millions of dollars to develop it.”\nBut what if the research group hasn’t published any unfavorable findings? This might be because values are having an illegitimate influence: maybe the researchers have produced some unfavorable findings, but the funders aren’t letting them publish; or maybe the researchers are illegitimately conducting their research in such a way that the research produces only favorable findings. On the other hand, maybe the influence of funders’ values is perfectly legitimate, and it just happens that the researchers have only produced favorable findings.\nIn other words, if the research group hasn’t published only favorable findings, then Horvath’s condition might be satisfied. But it also might be violated. In this case, we would have to look at other kinds of evidence to determine whether the condition is satisfied. For example, we might need to see some kind of legal document attesting that the researchers have funding independence.\nThe second challenge has to do with the public accessibility of this evidence. By “public” here I mean the general public, and especially people who don’t have access to paywalled scientific journals. My point here is that even if Horvath’s condition is satisfied and publications of unfavorable findings provide good evidence that the condition is satisfied, because members of the general public don’t have access to this evidence, they’re still not in a good position to conclude that the condition really is satisfied.\nThere’s a lot in that sentence, so let’s consider an example. We suppose that the research group has published unfavorable findings in several closed-access peer-reviewed scientific articles. Academics and other researchers generally have access to the journals that published these articles through institutional subscriptions. For example, university libraries will pay a subscription fee to the publisher, giving everyone affiliated with the university access to the journal. However, because the journals are closed-access, people without this kind of individual access can only access the publications if they pay the publisher. Access typically costs around 30 USD per individual article. That might seem relatively affordable for one or two articles, but it’s prohibitively expensive if you’re trying to comb through all of a research group’s publications looking for unfavorable findings.\nIn effect, members of the general public don’t have access to closed-access scientific publications. So if these publications are the only evidence that Horvath’s condition is satisfied, the general public is not in a good position to judge whether industry funding is legitimate or not.\nI see at least two ways for researchers to respond to this epistemic challenge. First, they can publish in open-access journals, or otherwise make their unfavorable findings freely available to the general public. (Many publishers allow authors to make pre-copyediting versions of their papers freely available in public depositories and personal websites.) Second, they can work with journalists and other science communicators. Though the exactly rules vary from publisher to publisher, authors are generally allowed to provide individuals with copies of the published versions of their papers. With these copies, a science communicator could prepare a publicly-accessible review and summary of the research, documenting the fact that the research group has published unfavorable findings. (Researchers could do this themselves, of course. But the summary might be better received if it were prepared by an independent author.)\nQuick note: Tumblr doesn’t have any native commenting facilities, and I haven’t gone through the trouble of setting up Disqus with my current theme. If you’d like to make a public comment, click here to send me an email. I’ll post comments or links to responses here or in future posts.\n\nAugust 19th, 2015 10:36am (philosophy of) science science and values"
  },
  {
    "objectID": "posts/2018-10-10-did.html",
    "href": "posts/2018-10-10-did.html",
    "title": "Difference-in-differences in R",
    "section": "",
    "text": "This post recreates this post with proper formatting, syntax highlighting, etc.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(skimr)\n\nFirst we’ll load the data (download if necessary). I just started playing around with skimr::skim() to generate nice summaries. (digits doesn’t seem to do anything, though?)\n\ndata_file = '../_data/did_data.dta'\nif (!file.exists(data_file)) {\n    download.file(url = 'https://drive.google.com/uc?authuser=0&id=0B0iAUHM7ljQ1cUZvRWxjUmpfVXM&export=download', \n                  destfile = data_file)\n}\n\ndataf = haven::read_dta(data_file)\nskim(dataf, work, year, children)\n\n\nData summary\n\n\nName\ndataf\n\n\nNumber of rows\n13746\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nwork\n0\n1\n0.51\n0.50\n0\n0\n1\n1\n1\n▇▁▁▁▇\n\n\nyear\n0\n1\n1993.35\n1.70\n1991\n1992\n1993\n1995\n1996\n▇▃▃▃▃\n\n\nchildren\n0\n1\n1.19\n1.38\n0\n0\n1\n2\n9\n▇▃▁▁▁\n\n\n\n\n\nFollowing the old post, we need to construct dummy variables for (a) before-and-after the EITC takes effect in 1994 and (b) the treatment group (1 or more children). We’ll keep these as logicals (rather than the numerics in the old post).\n\ndataf = dataf %&gt;%\n    mutate(post93 = year &gt;= 1994, \n           anykids = children &gt;= 1)\n\nThe regression equation is\n\\[ work = \\beta_0 + \\delta_0 post93 + \\beta_1 anykids + \\delta_1 (anykids \\times post93) + \\varepsilon. \\]\nThe “difference-in-differences coefficient” is \\[\\delta_1\\], which indicates how the effect of kids changed after the EITC went into effect. Let’s take a second to plot this.\n\nggplot(dataf, aes(post93, work, color = anykids)) +\n    geom_jitter() +\n    theme_minimal()\n\n\n\n\nOkay, so the scatterplot version, like, isn’t perspicuous. How about the un-dummied variables, and just the mean?\n\nggplot(dataf, aes(year, work, color = anykids)) +\n    stat_summary(geom = 'line') +\n    geom_vline(xintercept = 1994) +\n    theme_minimal()\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\nThe parallel trends assumption looks good, at least qualitatively. (Remember this only applies prior to the intervention.) However, the parallel trends are nonlinear, which maybe is why the example uses the dummied variable.\nAnyway, on to the regression. Which, hey, is a linear probability model because econometricians are funny like that.\n\nmodel = lm(work ~ anykids*post93, data = dataf)\nsummary(model)\n\n\nCall:\nlm(formula = work ~ anykids * post93, data = dataf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5755 -0.4908  0.4245  0.5092  0.5540 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.575460   0.008845  65.060  &lt; 2e-16 ***\nanykidsTRUE            -0.129498   0.011676 -11.091  &lt; 2e-16 ***\npost93TRUE             -0.002074   0.012931  -0.160  0.87261    \nanykidsTRUE:post93TRUE  0.046873   0.017158   2.732  0.00631 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4967 on 13742 degrees of freedom\nMultiple R-squared:  0.0126,    Adjusted R-squared:  0.01238 \nF-statistic: 58.45 on 3 and 13742 DF,  p-value: &lt; 2.2e-16\n\n\nThis indicates that the EITC increased work (workforce participation? whether someone was employed?) by 5% among families with at least 1 child. In the second plot, the blue line goes from about 45% prior to 1994 to about 50% afterwards."
  },
  {
    "objectID": "posts/2015-04-23-public-policy-what-philosophers-of-science-can-contribute.html",
    "href": "posts/2015-04-23-public-policy-what-philosophers-of-science-can-contribute.html",
    "title": "Public Policy:  What Philosophers of Science can Contribute",
    "section": "",
    "text": "I spent last week in Washington, DC, interviewing with about 10 different federal government offices as a finalist for a AAAS Science and Technology Policy Fellowship. As you’ll see from those slides, AAAS policy fellows are primarily natural scientists and engineers; while it’s common to have a handful of social scientists and historians, and a few fellows have had Master’s degrees in philosophy, as far as I can tell there’s not yet been a policy fellow with a Ph.D. in philosophy.1\nIn part because of this, going into the week I was nervous about being perceived as an irrelevant interloper. Notoriously, quite a few high-profile physicists think philosophy of science is useless or worse. Would I simply be laughed out of every office that interviewed me? Would I have to spend the week repeatedly justifying my existence?\nI think these kinds of questions and self-doubts are the flip side of an issue that’s been explored in several blog posts lately, most prominently by Subrena Smith, Roberta Millstein, and Chris Eliasmith. Many contemporary philosophers of science are deeply engaged in current science. (For example, here at the Rotman Institute, several of our grad students and postdocs are active members of research labs in such fields as neuroscience and entomology.) In James Woodward’s sense, many of us are methodologists rather than metaphysicians, primarily interested in understanding (and improving) actual scientific practice in its messy detail. But this means that our analytical tools and interests are rather different from those of many of other philosophers, and as a result we are sometimes perceived as not doing “real” philosophy.\nAt the same time, the methods and interests of philosophers of science are rather different from those of practicing scientists and science administrators (who are generally former practicing scientists). So this might mean that philosophers of science are caught: neither “real” philosophers nor “real” scientists, but some sort of incoherent hybrid.\nFortunately, my worries were completely misplaced. Again, I interviewed at about 10 different offices, speaking with people (primarily natural scientists by training) in the National Science Foundation, Environmental Protection Agency, and US Department of Agriculture.2 In almost every office, the people I met with already recognized that I could make a contribution to their work. In several cases they were downright excited at the prospect of hosting a philosopher of science and ethicist for a year or two. I had a number of exciting discussions about how my research and academic interests were directly relevant to some problem or project that an office was working on.\nIn short, the general perception was that I, as a philosopher of science, had a lot to offer. Thanks to my training in philosophy and my research work as a philosopher of science, I have a combination of skills and competencies that typical policy wonks — and even natural scientists and engineers — don’t.\nFirst, to speak about philosophers in general, we are highly trained in both careful, detailed analysis and clear communication. Natural scientists and engineers have a similar kind of emphasis on analysis in their training, but mostly write terse lab reports rather than 8,000-word argumentative essays.\nSecond, most philosophers of science have at least the same degree of technical competence in a scientific field as, say, someone with a master’s degree. We might not be able run a research lab, but we’re certainly capable of understanding a scientific journal article and having an informed discussion with actual research scientists. (Some STS scholars call this “interactional expertise.”)\nThird, thanks to these two points, philosophers of science are often in a good position to develop what I call “translational expertise.” This is skill in communicating technical ideas to a non-technical audience. In many of the offices where I interviewed, there was interest in having me work on improving communications between agency researchers (developing new toxicology assays, for example) and regulators (who need to negotiate decisions about exposure standards based on these new methods among various stakeholders).\nFourth, many of the issues that philosophers of science are grappling with today are closely related to issues faced by science policymakers: the strengths and limitations of novel methods, how to deal with uncertainty, how to integrate different kinds of evidence, the proper roles of interpretation and values, what we mean by “objectivity” or “policy-relevant but policy-neutral,” the internal social organization of science, ethical issues in new technologies, the relationship between experts, policymakers, and the general public, and so on. Even historically “technocratic,” “command and control” agencies like the EPA (those are the words of an EPA scientist who interviewed me) are beginning to recognize the need to incorporate qualitative social science and philosophy in their decisionmaking processes.\nOutlined like this, I’m actually rather surprised that no philosophers of science have preceded me as AAAS science policy fellows. Many of us care a great deal about making our work socially relevant and socially engaged. And it seems clearly that we have specific things to contribute to science policy. Hopefully my experience will serve as a model for other philosophers to follow.\n\n\n\n\nJust to be clear, AAAS S&T Policy fellows are different from AAAS fellows. The latter are distinguished senior researchers. In this post, I’m talking about the policy fellows program. AAAS policy fellows are required to have either a doctoral degree or a master’s in engineering and a few years’ work experience. ↩︎\n\n\nThe AAAS S&TP fellowships are organized into the three branches of the federal government. Nearly all fellows are in the executive branch, and there are four main executive branch programs: Diplomacy, Security, and Development; Energy, Environment, and Agriculture; Health, Education, and Human Services; and Big Data and Analytics. I’m a finalist in Energy, Environment, and Agriculture and Health, Education, and Human Services. ↩︎\n\n\n\n\nApril 23rd, 2015 11:59am philosophy of science science policy AAAS"
  },
  {
    "objectID": "posts/2014-06-18-virtue-ethics-for-robots.html",
    "href": "posts/2014-06-18-virtue-ethics-for-robots.html",
    "title": "Virtue Ethics for Robots",
    "section": "",
    "text": "Autonomous automated systems — machines capable of acting on their own for extended periods of time in complex environments — have been a major trope of science fiction for as long as the genre has existed. Over the last ten years or so, autonomous systems research has advanced dramatically, and it is generally recognized that autonomous automated systems will be common in warfare and everyday life (at least in wealthy countries) within the next ten years or so.\nIn light of these developments, “robot ethics” has developed as a serious scholarly (and popular) topic. Dipping into this literature, I get the impression that much of robot ethics takes what I’m going to call a principle-based approach to ethics. On such an approach, ethical judgment is a matter of, first, identifying the correct set of ethical principles, and second, correctly applying those principles in a given situation. (I have some thoughts on why robot ethics has taken the principle-based approach, but I’ll leave them out here. I’d be happy to elaborate in the comments.)\nIn this post, I’m going to argue that a principle-based approach tends to overlook two important features of ethical judgments in complex situations. In addition, I think that these two features correspond to certain worries that many members of the public have about automated ethical decisionmaking — that is, about turning over responsibility for ethical decisions to autonomous automated systems. So, insofar as robot ethics want to (try to) assuage public concerns about autonomous automated systems, they should at least broaden their range of ethical approaches.\nBefore going further, I should make it clear that I’m not actually very familiar with the current state of robot ethics. I’ve glanced at a few things, and read some things shared (and in a couple of cases, written) by my advisor, Don Howard, over social media, and I’ve had a few discussions with my friend and co-author Charles Pence. But I am, professionally speaking, a philosopher with expertise in ethics, and I generally work within an approach to ethics that is sharply critical of the limitations of a principle-based approach. So hopefully my arguments here will be valuable to robot ethicists even if they are built on a hasty generalization.\nMy argument is going to contrast principle-based approaches to ethics with an alternative approach called virtue ethics. Within philosophical work on ethics, principle-based approaches — especially Kantian deontology, utilitarianism, and appeals to human rights — have been dominant for the last several centuries. Virtue ethics has been recognized as a distinct approach for about 40 or 50 years, but virtue ethicists trace their approach back to such ancient philosophers as Aristotle and Confucius, as well as traditional ethe (ethos, plural) of agrarian and nomadic hunting societies in Africa and the Americas.\nThe principle-based approaches generally focus on decisions about particular actions, and aim to find the one right or best course of action to take in a given situation. By contrast, virtue ethics generally focuses on the character of agents over time. Since they’re less concerned about what to do in particular situations, virtue ethicists tend to be more attentive to complexity and uncertainty in decisionmaking than principle-based approaches. In addition, virtue ethicists tend to be concerned about more aspects of our ethical lives than just making decisions. Consequently, virtue ethicists have developed the conceptual tools that I’ll use below, and it’s hard to find room for these tools in the principle-based approaches. Thus, again, robot ethicists should expand their approaches, so that they can make use of these virtue ethics tools.\nTo make my discussion concrete, let’s consider two cases, both of which seem to be common in robot ethics.\n\nAn autonomous, automated soldier/weapons system is sent on a mission to engage with an enemy squad. The squad has barricaded themselves in a small building with a dozen children. Should the system engage the enemy — risking the lives of the children — or maintain its distance — risking the possibility that the enemy squad will escape?\nAn autonomous, automated car is transporting one passenger along a narrow street with parked cars on either side. A child suddenly runs out into the street from between two parked cars. The car does not have time to brake safely. Should it veer into the cars on the side of the street — risking the life of its passenger — or hit the child — risking its life?\n\nOn the principle-based approach, the task for robot ethics is to identify the correct ethical principles to be applied in these cases, and so to determine the ethically correct course of action for the automated system.\nHowever, I argue that there is no ethically correct course of action in either of these cases. Both cases are examples of moral dilemmas: situations in which every available course of action is, in some respect or another, seriously bad, wrong, or vicious. In the automated car case, both of the available options risk serious harm to moral innocents (people who do not deserve to be harmed). In the automated weapons system case, the choice is between serious harm to moral innocents now or serious harm later (after the enemy squad escapes).\nPrinciple-based approaches have trouble recognizing moral dilemmas to the extent that they are based on a single fundamental ethical principle, such as Kant’s categorical imperative or utilitarianism’s principle of greatest happiness. On pain of contradiction, the fundamental ethical principle cannot imply both the system should do X and the system should not do X. Pluralist principle-based approaches — like Beauchamp and Childress’ principlism for biomedical ethics — do better here. But the impression I have is that writers in robot ethics generally do not recognize the moral dilemmas is these cases; instead, they keep searching for (and arguing about) “the” right answer.\nBernard Williams, Rosalind Hursthouse, and Lisa Tessman — three major virtue ethicists — all emphasize regret as an appropriate response to finding oneself in a moral dilemma. Consider an ordinary human being, rather than an autonomous system, in either of our example cases. We expect that this person, whatever they do, will feel ethically responsible for bringing about a serious harm; they will feel regret. The fact that they have done the best that they could do in terrible, regretful circumstances could mean that we will not punish them for, say, the death of the children. But they will still feel responsible. Furthermore, this feeling of regret is an ethically appropriate response to a moral dilemma. Someone who does not feel bad in any way at all about the death of the children is callous, morally reprehensible, and a candidate for antisocial personality disorder according to the DSM IV criteria.\nThis brings us to the first concern that, I think, many people have about autonomous ethical decisionmaking. In the popular imagination, robots are cold, calculating, and callous; they simply “follow their programming” wherever it leads them. In other words, autonomous systems do not experience regret when confronted with moral dilemmas, and so respond to these situations in disturbing and ethically inappropriate ways. In this respect, concerns about automated ethical decisionmaking parallel Cold War-era concerns about scientific-military technocrats — Dr Strangelove and HAL are both morally reprehensible and disturbing characters because of their callous, calculating decisionmaking processes.\nI presented our two example cases dichotomously: the autonomous automated system must choose exactly one of two options, A or B. But of course real-world cases aren’t that simple; there are many different courses of action that could be taken, and many different options within each possible course. In the case of the automated soldier/weapons system, the system could rush into the building, relying on its superhuman reaction times to take out the enemy before any of the children are hurt; it could use smoke or some other means to drive everyone out of the building; or it could call in automated or human support.\nPrinciple-based approaches generally assume that the range of possible course of action is fixed and known in advance. The principles are used to evaluate each possible course, and the correct course of action is the one that comes out highest on this evaluation. Even Beauchamp and Childress’ approach seems to assume that this is how ethical decisionmaking works. But the observation in the last paragraph indicates that, in general, we don’t know the range of possible courses of action in advance.\nIndeed, in ethically complex and challenging situations, there is a crucial role for creativity and innovation. Sometimes we can appear to be in a moral dilemma because of serious problems with all of the simple, obvious options. But, by taking advantage of particular features of the situation in creative ways, we can find a way to avoid the dilemma. Because principle-based approaches generally assume that all of the possible courses of action are known in advance, they generally miss this role for creativity.\nI want to stress that this role for creativity is closely tied to the particular features of the situation. We might call this tactical creativity, to distinguish it from the strategic creativity that can develop novel approaches to general situations. Both strategic and tactical creativity are invaluable resources for resolving ethical dilemmas and novel situations; but tactical creativity, unlike strategic creativity, can only be exercised “on the ground,” in the particular situation at hand. Consequently, correctly resolving some apparent moral dilemmas requires tactical creativity.\nThis brings us to the second concern about automated ethical decisionmaking. Robots, in the popular imagination, are uncreative and unadaptable; as with the first concern, they simply “follow their programming,” rigidly and without deviation. In other words, while their designers might exercise strategic creativity, autonomous automated systems are incapable of tactical creativity. But, as I argued above, tactical creativity is required to resolve some apparent moral dilemmas. Thus, autonomous automated systems are more likely than humans to incorrectly resolve some apparent moral dilemmas. Because these systems are rigid and uncreative, they are incapable of seeing the creative way out of the dilemma. And so they are more likely than creative humans to do unnecessary ethical damage.\nIn this post, I’ve raised two concerns for robot ethicists, drawing on work in virtue ethics. First, because autonomous systems do not experience regret, they do not respond appropriately to finding themselves in moral dilemmas. And second, because these systems lack creativity, they are incapable of correctly resolving some moral dilemmas.\nMy arguments for these concerns draw on the popular image of robots and other autonomous automated systems as cold, callous, and calculating. Some readers might think that these concerns can be responded to by changing the popular image of robots — by encouraging people to think of Data from Star Trek or the robot in Robot and Frank, rather than HAL. But this would be superficial. What’s necessary, I think, is to think of ethical deliberation by autonomous automated systems not in terms of applying a set of principles, but instead as having and exercising a set of responsive capacities, including regret and creativity. This might require a radical change in the way roboticists approach system design, but (if successful) it would produce robots with genuinely virtuous character.\n\nJune 18th, 2014 11:46am robot ethics virtue ethics philosophy of technology"
  },
  {
    "objectID": "posts/2012-07-23-two-conceptions-of-human-nature.html",
    "href": "posts/2012-07-23-two-conceptions-of-human-nature.html",
    "title": "Two Conceptions of Human Nature",
    "section": "",
    "text": "In his essay “Two Philosophers Skeptical of Negative Liberty” in Libertarianism Defended, Tibor Machan comes close to a significant insight concerning the fundamental disagreements about human nature that motivate the high-level disagreements between (right-)libertarians, on the one hand, and egalitarian liberals and (democratic) socialists, on the other hand. In this post, I want to elucide the distinction that Machan draws, criticize it, and use the criticism to suggest a more accurate insight.\n\n\n\nMachan’s foil is Amartya Sen’s account of freedom in his Development as Freedom. In conventional terms, Sen’s account is an account of positive freedom – substantive freedom to carry out certain actions or realize certain ends. Machan, as a right-libertarian, advocates a strictly negative account of freedom – freedom from interference by other people. But this disagreement over conceptions of freedom is based on an underlying disagreement, over human nature. Machan endorses a conception of human nature that he attributes to the classical liberal tradition:\n\nIf one believes that, as a rule or for the most part, human beings who are not being interfered with by others have the capacity (with some help from intimates, of course) to secure for themselves what they need so as to flourish in their lives, then one is going to emphasize being free from interference because the central condition that an adult needs to flourish is not to be oppressed by other persons – that is to say, not have other constraint them. Once oppression stops, normal adults can get innumerable tasks accomplished, various goals achieved …. The major obstacle to our advancing in life, based on this idea of human nature, is other peoples’ interference …. Once that is fended off … people will have the chance to exercise their initiative – their capacity to make the necessary moves to improve upon their lives – and flourish in life. Thus they don’t need to have other conscripted to serve them – they will find mutually acceptable ways to attain their peaceful goals. (272) The idea of human nature underlying negative rights … is that, providing one isn’t interfered with by others who have the choice to abstain from such interference, one is going to be more or less able to secure for oneself what is necessary for a reasonably prosperous, healthy, flourishing life. (274)\n\nBut he attributes a very different conception of human nature to Sen:\n\nThose, however, who believe that human beings are ill-equipped to get ahead on their own – that they are either too ignorant, too weak, too poor, or in some other way deficient to pursue a fruitful life – will hold that being free from interference by others is not enough for human flourishing …. To make this possible, it is necessary to conscript others who are already well enough enabled to do work for those in need, ergo extensive systems of confiscatory taxation in systems of justice that characterize the welfare state or democratic socialism. (272)\nIn contrast, the positive rights … view tends to be supported by a passive conception of human nature. People are rather inert and helpless, even when nobody is intruding on them and nobody bothers them. They will basically remain poor even if the obstacles others’ intrusiveness creates for them are removed …. (274)\n\nFinally, to state the disagreement in brief,\n\n[T]he decisive issue is, ‘What conception of human nature is actually right?’ Are we self-movers, self-governors, and sovereign beings to at least a substantial enough degree that we can thrive in peace? Is our freedom from oppression sufficient enough to achieve a progressive forward-moving economic system? Or do we need aggressive support from above? (275)\n\nLet’s call the two conceptions of human nature – or, as I prefer, conceptions of personhood – the classical liberal and egalitarian conceptions. I think the following two lists capture the major features of each of these two conceptions (as Machan understands them), along with a few of their important (purported) implications.\n\n\n\nclassical liberal\negalitarian\n\n\n\n\nindependent\ndependent\n\n\nactive agents\npassive consumers\n\n\nindividual agency\ntop-down management\n\n\nnegative freedom\npositive freedom\n\n\nvoluntary association\nstate coercion\n\n\nprivate ownership\nstate ownership\n\n\n\nI don’t think Machan is the only right-libertarian who embraces the classical liberal conception of personhood, rejects the egalitarian conception, or attributes the negative implications of the egalitarian conception to their egalitarian liberal and socialist opponents. Jan Narveson seems to do several of these things throughout The Libertarian Idea, as does Robert Nozick in Anarchy, State, and Utopia. I also think something like a disdain for the egalitarian conception motivates right-libertarianism throughout our political culture. For one example, it’s prevalent in the work of Ayn Rand. For another, I’ve also heard this from my brother – a libertarian-learning IT systems engineer who majored in beer and partying in college.\n\n\n\nI believe that this distinction is an instance of a false dichotomy, which involves an “either/or” premise that ignores at least one relevant possibility or option. Machan is assuming that one holds either the classical liberal conception or the egalitarian conception, and that there are no other possibilities.\nBut the fact – and for the purposes of this post, I’m going to take it to be a fact – that human beings are dependent upon each other doesn’t imply that we’re not active agents, or that we do or must sit around, passively waiting for the state to distribute resources to us. It simply implies that our individual agency is quite limited. That is, there are relatively few things that we can accomplish on our own. We can and do accomplish much more when we collaborate with others, forming shared or collective agents. This is an important possibility that Machan has overlooked.\nMachan might try to claim that he’s accounted for collective agents, recognizing “voluntary organizations, service groups, and so on” as an alternative to government meddling in private affairs. But collective agents aren’t exactly voluntary. Consider families. We do not choose our parents and siblings; while we do generally choose our spouses and choose whether to have children, we have only limited information about what these people will be like in the distant future. And even divorce does not completely dissolve all family ties, when there are children involved. Similarly, when we join a career or a profession, we choose to join the organization with only limited information about what that work will be like, and we generally must conform to the standards of behavior established by our predecessors before we have any decisionmaking authority within the organization.\nAlso, the fact that negative freedom is inadequate for us to carry out our various activities – a simple lack of interference doesn’t guarantee that we have access to the materials and resources that these activities require, if all of those resources are owned by people who will not let use them – or that a laissez-faire private ownership system is inadequate for general flourishing doesn’t imply that flourishing requires a bureaucratic welfare system or government control of all property. Again, Machan has falsely assumed that there are only two options – individual ownership or state ownership – and neglected a third option – collective ownership, by democratically-organized collective agents.\nI believe that something like the positive view I’ve suggested in the last few paragraphs – one that recognizes both agency and interdependence, through semi-voluntary collective agents, and alternatives to both the free market and the state – is closer to the conception of personhood held by many egalitarians than the version presented by Machan. Consider this passage, from egalitarian liberal Martha Nussbaum’s Frontiers of Justice:\n\nThe ability to join with others to give one another laws is a fundamental aspect of human freedom. Being autonomous in this sense is no trivial matter: it is part of having the chance to live a fully human life. In our day \\ldots the fundamental unit through which people exercise this fundamental aspect of human freedom is the nation-state: it is the largest and most foundational unit that still has a chance of being decently accountable to the people who live there \\ldots. [T]he nation-state and its basic structure are \\ldots a key locus for persons’ exercise of their freedom. (257)\n\nNussbaum is speaking here of states rather than smaller-scale collective agents. But it is clear that she treats the state as a collective agent, a collaboration of citizens, rather than an alien force, imposing itself on citizens from above. More radical egalitarians might question whether the vast bureaucracy of the modern nation-state can really be a collaboration of all of its millions of citizens, but they would accept the underlying picture of human nature: neither utterly self-reliant John Galts nor utterly servile dependents of an alien state, but rather interdependent, social beings, who flourish by working together to achieve great things.\nIf this is right, the basic disagreement is best characterized as between an self-reliant conception, emphasizing individual agency, and a collaborative conception, emphasizing collective agency.\n\nJuly 23rd, 2012 11:47pm politics"
  },
  {
    "objectID": "posts/2019-02-01-explainable-machine-learning.html",
    "href": "posts/2019-02-01-explainable-machine-learning.html",
    "title": "Slides: Explainable Machine Learning: An Integrated Epistemic-Ethical Analysis",
    "section": "",
    "text": "Machine learning (ML) algorithms have become widely adopted over the past decade. Contemporary ML can achieve near-human levels of accuracy, but operates as an inscrutable “black box.” This has stimulated significant research in methods to explain the behavior of ML systems. However, many of the proposed methods violate a common assumption that explanations must be true. Arguably, such “explanations” of ML systems are not actually explanations. I draw on work in the philosophy of science and political philosophy to clarify the requirements for explainable ML. Recent work on idealization in science suggests that explanations can be false, but only if these falsehoods promote the goals of inquiry. I argue that, at least when ML is used in policy contexts, the goals of explainable ML ultimately trace back to democratic accountability, and conclude that ML can be explainable only when system development is participatory and democratic.\nPDF: https://drive.google.com/open?id=0B6oYmzobonqoM1lNZWdmMFEzTG8"
  },
  {
    "objectID": "posts/2013-04-14-local-food-global-justice.html",
    "href": "posts/2013-04-14-local-food-global-justice.html",
    "title": "Local Food, Global Justice",
    "section": "",
    "text": "I had the pleasure of attending the First Annual Workshop on Food Justice and Peace this past Friday and Saturday at Michigan State University, and at that workshop I had the pleasure of meeting Mark Navin of Oakland University. My talk offered a defense of local food from a radical participatory democracy perspective (this is often called communitarianism, though I dislike that label myself), while Mark offered a critique of local food from a (maybe) cosmopolitan, (certainly) global justice perspective. So we had plenty to talk about!\nIn this post, I’m going to focus on just one of Mark’s arguments, which he calls the argument from repair or the argument for compensation to developing societies. It runs as follows:\n\nDeveloped societies have harmed developing societies in ways that have contributed to their poverty.\nThe most (only?) reliable way to alleviate global poverty is to increase the developed world’s imports of the developing world’s (agricultural) products.\nA duty of repair provides a moral reason for the developed world to increase its imports of the developing world’s goods (and not to exercise ‘food sovereignty’).\nThe duty of repair may demand the sacrifice of valued personal or social projects. [Specifically, food sovereignty in developed countries, especially wealthy communities in such countries.]\nTherefore, the duty of repair may require the developed world to compromise its local food project.\n\nIn short, since developed societies got that way by exploiting societies that, today, are developing or “underdeveloped,” folks in developed societies have a strong obligation to support the development of developing societies. Insofar as this obligation conflicts with local food movements in developed societies, it seems that the local food movements must be sacrificed.\nI want to focus here on an aspect of this argument that I didn’t get to discuss with Mark in person. Specifically, how do we understand the harm inflicted on developed societies, or more generally how do we measure well-being? Overall, Mark seemed to assume that we measure well-being through wealth. Developed societies are doing well insofar as they are very wealthy; developing societies are not doing well insofar as they are very poor; and developed societies have historically harmed developing societies precisely insofar as the former have become wealthier by taking the wealth of the latter and keeping them poor.\nI agree that wealth is good, and so agree that, ceteris paribus it’s better to have more wealth than not and that the transfer of wealth from today-developing countries by today-developed countries was an egregious injustice.\nBut I also thinkthat some other goods are qualitatively more important than wealth, and that an exclusive focus on wealth is dangerous because it tends to lead to the sacrifice of those other goods for the sake of greater wealth. To be a little more precise, I’ll call a relevant set of non-wealth goods that I have in mind the goods of flourishing local communities. (A while back, I wrote this post defending local food in a non-technical way using these ideas.) In a future post (and perhaps even a paper), I plan to leverage this idea against Desrochers’ and Shimizu’s The Locavore’s Dilemma: their recommendations are overwhelmingly likely to lead to the sacrifice of the goods of flourishing local communities, in both developing and developed countries.\nTo be clear, when I say that the goods of a flourishing local community are qualitatively more important than wealth, I don’t mean that they are lexically more important than wealth, or in other words that it is always in all situations wrong to sacrifice the goods of a flourishing local community for the sake of wealth. For one thing, wealth is instrumentally valuable for maintaining a flourishing local community. So, when we are faced with a choice between (a) such desperate poverty that the local community is barely able to function and (b) a limited sacrifice of the goods of the local community, it seems like we have good reason to go with (b) over (a), at least for the short term. What really concerns me is the persistent and systemic sacrifice of the goods of a flourishing local community for the sake of wealth. This, I think, is where Desrochers and Shimizu would lead us.\nI suspect Mark is much more sympathetic to what I’ve said in the last few paragraphs than Desrochers and Shimizu, so here I’d like to try to make a more subtle point. Among the goods of flourishing local communities are what I’ll call cross-cultural friendship. By this I don’t mean friendship at the individual level, e.g., having a pen pal in another country, at least primarily. Instead, I mean relationships among local communities, and specifically the kinds of relationships that promote the flourishing of these communities. At the community level, this is analogous to Aristotle’s view that genuine friendship involves reciprocal good will: our friendship is valuable to us because and insofar as both (a) I care about your flourishing and you care about mine and (consequently) (b) my flourishing promotes your flourishing and yours promotes mine.\nIn various places, Alasdair MacIntyre discusses a specific kind of cross-cultural friendship, one that involves the exchange of philosophical criticism in a way that promotes the development of the ethical traditions of both cultures. In response to Mark, I want to make a more materialistic point. First, certain kinds of global trade can promote the economic well-being of developing countries, but not the goods of their local communities. That much is implied by my criticism of Desrochers and Shimizu. Second, among the harms inflicted by now-developed countries on now-developing countries is the destruction of cross-cultural friendship. This I take to be well established by the history of colonialism. And third, local food movements in developed countries, as flourishing local communities, have the potential to repair this harm by creating cross-cultural friendships with flourshing local communities in developing countries. This requires, for example, purchasing coffee following fair trade principles: from plantations owned by workers as cooperatives that don’t use child labor and do use sustainable growing methods, paying a premium to transfer wealth back to the developing country, and so on. But buying Fair Trade alone is insufficient. Cross-cultural friendship also requires a certain minimum degree of personal contact between the local food movement in the developed country and the coffee plantation in the developing country. (We might call this “direct trade”, except that term means something else, which falls short of what I’m proposing here.)\nPut another way, even transferring a significant amount of wealth from developed countries (back) to developing countries is insufficient to repair the qualitatively different kinds of harms inflicted on developing countries by developed countries. Repairing these harms requires promoting the local communities of developing countries in general and creating cross-cultural friendships specifically. Certification schemes, like Fair Trade USA, might go some way towards repairing these harms. But they’re primarily focused on economic repair. Community repair requires cross-cultural friendships between flourishing local communities in developed and developing countries. Local food movements, since they can stand on one side of these friendships, offer better prospects for a more thorough kind of repair.\nAll together, I think this offers a charitable challenge to Mark’s argument. The intellectual tradition of participatory democracy — especially by way of MacIntyre’s development of Aristotle’s ideas — is quite sympathetic to the argument from repair. But, compared to the egalitarian liberal framework within which Mark presents the argument, the more radical tradition offers a richer notion of the harms and goods involved and suggests better means for carrying out repair.\n\nApril 14th, 2013 11:02am food economics"
  },
  {
    "objectID": "posts/2020-05-04-chlorpyrifos-paper.html",
    "href": "posts/2020-05-04-chlorpyrifos-paper.html",
    "title": "Paper: Census Demographics and Chlorpyrifos Use in California’s Central Valley, 2011–15: A Distributional Environmental Justice Analysis",
    "section": "",
    "text": "In Spring 2018, I audited a spatial data analysis class taught by Noli Brazil at UC Davis. The pesticide chlorpyrifos had been in the news for the past year, as one of the first things Scott Pruitt did at EPA in 2017 was to deny a petition to revoke all tolerances for the pesticide (effectively banning it in the US). For my project for the class, I decided to apply some spatial regression techniques to understand where chlorpyrifos is used in California’s central valley. The initial results were quite compelling, indicating that Hispanic communities are potentially exposed to quite a bit more chlorpyrifos than White communities. I also developed what seems to be a novel way to account for measurement error in the American Community Survey’s public data.\nA few years later I finally found time to finish writing things up, and the paper was recently published in the International Journal of Environmental Research and Public Health. (Incidentally, I was impressed at the fact that the journal got a pair of pretty decent reviews back in just a few weeks. In philosophy of science, a fast turnaround time for reviews is 2-3 months.)\nHere’s figure 1 of the paper, showing the study area in terms of census tracts (blue) and places (yellow) and where chlorpyrifos is used (red dots; more saturation = more chlorpyrifos).\n{:width=“50%”}\nAbstract:\nChlorpyrifos is one of the most widely-used pesticides in the world, and is generally recognized to be a moderate neurotoxin. This paper reports a distributional environmental justice (dEJ) analysis of chlorpyrifos use in California’s Central Valley, examining the way distributions of environmental risks are associated with race, ethnicity, class, gender, and other systems of structural oppression. Spatial data on chlorpyrifos use were retrieved from California’s Department of Pesticide Registration (DPR) public pesticide use records (PUR) for 2011-2015. These data were combined with demographic data for the Central Valley from the American Community Survey (ACS). Spatial regression models were used to estimate effects of demographic covariates on local chlorpyrifos use. A novel bootstrap method was used to account for measurement error in the ACS estimates. There is consistent evidence that Hispanic population proportion is associated with increased local chlorpyrifos use. A 10-point increase in Hispanic proportion is associated with an estimated 1.05-1.4-fold increase in local chlorpyrifos use across Census tract models. By contrast, effects of agricultural employment and poverty on local chlorpyrifos use are ambiguous and inconsistent between Census tracts and Census-designated places.\nhttps://doi.org/10.3390/ijerph17072593"
  },
  {
    "objectID": "posts/2023-06-07-nazi-problem.html",
    "href": "posts/2023-06-07-nazi-problem.html",
    "title": "Revisiting the Nazi Problem",
    "section": "",
    "text": "In theory this will be a quick post. In theory.\nAt a conference a couple of weeks ago, I was talking (read: complaining) about how, as far as I’m aware, no one has ever really responded to the “Nazi Problem,” a concern about Longino’s conception of objectivity raised by Natalia Baeza in a grad seminar she and I took with Janet Kourany at Notre Dame circa 2007 and that I wrote up in my first publication a few years later (Hicks 2011). Yet, with the revitalization of race science over the last 8 years or so, the Nazi Problem is even more urgent today than it was in 2007-ish. Heather Douglas told me she had a brief solution in a recent paper of hers (Douglas 2023). In this post, after summarizing the Nazi Problem, I’ll present Douglas’ solution and my response.\nA key implication of Longino’s conception of objectivity is that it requires the active cultivation of a wide range of perspectives, in order to ensure the critical scrutiny of implicit assumptions made by the scientific mainstream. For Longino this provides an epistemic argument for the active cultivation of women (feminist) and POC (antiracist) scientists. The “Nazi Problem” is that this reasoning seems to apply equally to Nazi/sexist/racist scientists, and therefore it seems that Longino’s account of objectivity requires actively cultivating them as well.\nThe primary solution to the Nazi Problem that I consider in the paper1 was to point out an incompatibility between the political liberalism built into Longino’s account — ideas like formal (epistemic) equality — and Nazi anti-liberalism. A sexist can’t authentically comply with norms that require them to treat women as equals, and so we just qualify Longino’s account by saying that it doesn’t require the active cultivation of inauthentic perspectives. On the few occasions where people have responded to the Nazi Problem in print, this is the response they give (for example, Rolin 2017, 123–24).\nThe problem with this response, for me, is that it also rules out feminist standpoint theory. Because they think certain standpoints are epistemically advantaged, standpoint theorists also can’t authentically comply with norms that require them to treat disadvantaged standpoints as formal epistemic equals. I make a similar point about the commitment to neutrality between comprehensive conceptions of the good life and communitarian feminists.\nDouglas (2023)’s response to the Nazi Problem is raised in the context of identifying some key differences between inquiry (science) and democratic politics. A key contribution in this paper is the notion of inquirer façades, “faking one of the central norms of inquiry without following through on its demands.” Douglas proposes that “When inquirer facades are discovered, they can be legitimately ignored by those within the space of inquiry” (Douglas 2023, 133).2 Here’s Douglas on the Nazi Problem:\nI have four comments. My first point is that Douglas has reframed the Nazi Problem in terms of closed-mindedness or “dogmatism.”3 But, like Alcoff (2006) and Yap (2015), I don’t regard closed-mindedness as such a severe vice. As Yap puts it, “values such as respecting women’s autonomy and self-determination should be seen as basic commitments and not ‘up for grabs’ in the same kind of way” as our taste in music (Yap 2015, 5).\nSecond, rather than thinking the problem with Nazis is that they’re closed-minded, the problem with Nazis is that they’re Nazis. They hold odious racist, sexist, and anti-democratic beliefs, and often embrace offensive political violence as a means of achieving political power.4\nThird, the history of behavior genetics shows how a scientific community can protect racists and sexists by deflecting values-based critique while still responding to technical criticism (Panofsky 2014). Panofsky argues that, prior to Jensen’s 1969 “How Much Can We Boost IQ and Scholastic Achievement?” paper, behavior geneticists explicitly marginalized eugenics and race science, with the goal of avoiding controversy and having their work compared to Nazi race science (Panofsky 2014 ch. 2). Consequently many behavior geneticists were at least deeply uncomfortable with the controversy (and Nazi comparisons) that Jensen attracted (Panofsky 2014, 80). However, in reaction to broad criticisms of behavior genetics as such by Lewontin, Kamin, and other critics, behavior geneticists adopted a kind of fortress mentality — with the field under constant threat of repression by political radicals (Panofsky 2014, 114) — along with an “absolutist interpretation of intellectual freedom” that did not include any sense of social responsibility for, for example, racist appropriations of behavior genetics research (Panofsky 2014, 194–97). At the same time, behavior geneticists — including those who continue to study intelligence and race — have at least somewhat engaged with technical critiques, at least nominally rejecting a strong hereditarianism in favor of gene-environment interactions, and shifting from twin studies to GWAS (genome-wide association studies) and polygenic indices as genomics technology has developed (Panofsky 2014 ch. 6). These changes in the field are regarded as inadequate by many critics, both internal (Turkheimer 2022) and external (Downes and Kaplan 2022). But they do give behavior geneticists ground — however thin — to claim that race-and-intelligence researchers are responsive to criticism.\nMore generally, if Longino’s account can justify excluding Nazis who are closed-minded and/or not responsive to criticism, it would still seem to require actively cultivating Nazis, so long as they’re open-minded and engage in uptake. And, over the past decade or so, right-wing intellectuals and provocateurs — including proponents of race science — have adopted at least the posture of open-mindedness and the value of debate and free speech (Robin 2017; Reiheld 2018; Taylor 2018).\nFourth and finally, I didn’t develop my own solution to the Nazi Problem in Hicks (2011). A decade and change later, my own solution would align with Douglas’ recognition that “Both societal and epistemic responsibilities generate reasons for excluding some topics and methods from legitimate inquiry” (Douglas 2023, 129). Indeed, Douglas discusses the case of behavior genetics:\nI’ve emphasized the moral argument, which comes closest to the grounds on which I would exclude racists and sexists. But rather than the downstream social harms of research, I would point directly to the racism and sexism itself. These values — and acting in line with them — are odious and not to be tolerated (Schroeder 2022). Nearly 80 years after the formal demise of Nazism, there is good reason why it is almost universally reviled.\nThe problem with Nazis is that they are Nazis, and this is also the reason why we should not actively cultivate them, in science or elsewhere."
  },
  {
    "objectID": "posts/2023-06-07-nazi-problem.html#footnotes",
    "href": "posts/2023-06-07-nazi-problem.html#footnotes",
    "title": "Revisiting the Nazi Problem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwhich was my response to Natalia during seminar↩︎\nOne thing that’s a little unclear to me is the extent to which this definition depends on intentions or authenticity rather than behavior. “Faking” suggests something like inauthenticity, a mismatch between actual and voiced commitments, that I think can be difficult to get at empirically (Hicks 2022, 4n3). But norm compliance is often operationalized in behavior. For example, as I understand it liability in tort law generally doesn’t depend on intentions or anything else about mental states.↩︎\nI dislike this term, because I suspect etymologically it’s entangled with anti-Catholicism.↩︎\nI include “offensive” here to leave room for views, such as those of Malcolm X and Franz Fanon, that political violence might be necessary for community self-defense against a violently oppressive state.↩︎"
  },
  {
    "objectID": "posts/2020-07-10-critical-thinking.html",
    "href": "posts/2020-07-10-critical-thinking.html",
    "title": "Teaching critical thinking in 2020",
    "section": "",
    "text": "Back in May, I read a blog post by Cathy Davidson, an English professor at CUNY Grad Center, “The Single Most Essential Requirement in Designing a Fall Online Course.” It’s a really useful meditation for academics (and other teachers, and really anyone who works with young people) start to plan our courses for Fall 2020. Here’s the question that’s Davidson’s answer to the question-as-statement of her title:\n\nBefore we begin to design our fall syllabus, before we make clever instructional videos, we all need to think from a student’s point of view. We need to try to understand what it means to be studying for a future you don’t know that you will have. No one knows what lies ahead in the best of times. Now, all the predictions seem like some dystopian futuristic novel. Total social breakdown? Total economic collapse? A health emergency in which millions die over the next three or four years? How do you study to prepare for this future?\nWhat do our students need now? That is the essential question for going on line. Whether teaching algebraic geometry or sociology or literature or art or religion, we need to begin with the question of: what would I need if I were a student in this historic moment? [my emphasis]\n\nI’m teaching a critical thinking course this Fall. I immediately knew what the students in my course would need in the current moment: the virtues of engaging well in conversation across deep, emotionally-laden disagreement. If you’re an 19-year-old lefty or progressive, how do you talk with your Trump supporter parents about the election? Maybe you’re white and Black Lives Matter is challenging some of your deep-seated beliefs about policing, prisons, and criminals. Or you’re on DACA and your bio lab partner is talking about the pro-ICE rally that he went to last week. (CW: This link describes a rally that actually happened at UC Merced in 2018. https://www.mercedsunstar.com/news/local/education/uc-merced/article204022279.html) Our culture right now doesn’t provide many models of how (and whether) to have good conversations about these kinds of issues.\nThere’s significant demand for these models in our society right now, as witnessed by the success of books nominally about reason, logic, and deep disagreement by authors associated with the right-wing/anti-left “intellectual dark web.” (I’m thinking in particular of Stefan Molyneux’ The Art of the Argument, Boghossian and Lindsay’s How to Have Impossible Conversations, and to an extent Jordan Peterson’s 12 Rules for Life. These links are all to reviews or more general critical discussions.) These books have two problems. First, as textbooks in critical thinking they’re not very good. (Check the reviews.) Second, they serve as subtle entry points to the right-wing social media network. I’m not saying that students should never encounter and engage with right-wing ideas. But it should be done with care, and honesty, and none of these books are transparent about their political alignment.\nMany standard critical thinking textbooks take one of two approaches. The first approach, which I think is also the most common, focuses on the technicalities of logic: definitions, fallacies, often Aristotelean syllogism and a little sentence logic; maybe a little probability theory as (Bayesian) inductive logic. A good example of this approach is Hurley’s A Concise Introduction to Logic, currently in its twelfth edition and paradoxically running to more than 700 pages. If there’s anything in this book about interacting with other people in any way, it doesn’t appear in the table of contents.\nI think being familiar with more-or-less formalized logic can be useful, especially for upper-division philosophy majors. But it’s much less useful if what we want to do is talk with people with whom we disagree.\nThe second approach to critical thinking aims to cultivate what I think of as the “liberal conversational virtues”: open-mindedness, direct and accessible expression of one’s ideas, attentive listening, empathy, interpretive charity, and identifying common ground. Some good examples here are Maureen Linker’s Intellectual Empathy, Martin Fowler’s The Ethical Practice of Critical Thinking, and Anthony Weston’s Rulebook for Arguments. These books come closer to what I want to offer my students. (And that’s one of the main reasons I chose Morrow and Weston’s Workbook for Arguments, which includes the full text of Rulebook plus exercises and larger assignments.)\nBut these liberal conversational virtues can be — and historically have been — weaponized against cultural movements for social justice. I’m thinking of tactics such as sealioning, tone policing, concern trolling, “just asking questions,” and Alice McIntyre’s notion (via Alison Bailey) of “white talk” (especially assertions of one’s anti-racist open-mindedness and empathy as a way to avoid criticism). The liberal conversational virtues become vices when they make my students more susceptible to these tactics.\nAs a first pass, I might address this limitation of teaching the liberal conversational virtues by assigning some additional readings and activities, discussing these various tactics, helping my students spot them in the wild, and giving them tools to respond appropriately.\nI’ll probably do some of this. But it risks turning a chunk of my class into a laundry list of fallacies — just a different list from the standard one you find in Hurley’s textbook. So, as a second pass, I think we (including both teachers and students of critical thinking) need to think about what goes wrong when the liberal conversational virtues are weaponized. Why is the sealion’s endless call to open debate not just annoying but actually ethically problematic?\nI think the answer has to do with power. Characteristically, sealioning and similar tactics are directed downwards, from someone who is relatively high in our society’s power hierarchies — and interested in defending those hierarchies — and towards someone who is relatively low — and interesting in criticizing them. In other words, sealioning is a vice because it functions to protect an unjust status quo from criticism. (Linker discusses power dynamics in the introduction and second chapter of her book, though on a quick skim I can’t tell if she’s connecting power to my concern with the ways thinks like empathy and open-mindedness can be weaponized and do harm. I might assign some of her discussion in my course.)\nOn this analysis, critical thinking — and specifically whether certain approaches to conversation are, in a given case, virtuous or vicious — is loaded with substantive assumptions about power and justice. I can’t assume that students share my substantive assumptions about power and justice. (Though, in my experience so far, most UC Merced students are pretty sympathetic to my views.) But I can, at least, teach them that the deep disagreements in our society also apply to the meta-level; that our deep disagreements about Black Lives Matter or Trump are, at the same time, also deep disagreements about how to talk about our deep disagreements."
  },
  {
    "objectID": "posts/2013-01-11-satz-account-of-noxious-markets.html",
    "href": "posts/2013-01-11-satz-account-of-noxious-markets.html",
    "title": "Satz’ Account of Noxious Markets",
    "section": "",
    "text": "In this post, I’ll summarize Stanford political philosopher Debra Satz’ account of noxious markets. (Satz 2010; unsourced page number citations will be to this book.) This will serve as preparatory work for a post applying her account to commercialized science (probably, specifically, the “funder effect”).\n(Please note that this series is an experiment in publicly posting sections of a paper as I write them. I look them over briefly for typos and the like, but my ideas here are still in development.)\nSatz’ account is designed to apply to specific, contextually-embedded markets. “Specific” contrasts with (a) generic markets, the kind of thing you learn about in Intro to Microeconomics, (b) the unified global market comprising all transactions (insofar as this exists), and (c) market societies, that is, societies in which the market is the dominant social institution, that is, capitalist societies, both abstract and particular. As she puts it,\nFor example, she considers whether specific markets in women’s reproductive labor and sexual labor, child labor, voluntary slavery, and kidneys are noxious. Furthermore, these markets are treated as embedded in broader social contexts. For example, the examples listed two sentences ago are considered in the context of industrializing economies, the highly unequal global distribution of wealth, and the vulnerability of extremely poor individuals and households. Furthermore, her account is designed to apply even to perfect markets and Pareto-optimal distributions: a market that is working efficiently in the economists’ sense can still be noxious. (See Wikipedia for an overview) Specifically, the fact that both parties consented to the transaction does not imply that it was not noxious. (Satz seems to use the common simplifying assumption that any transaction directly involves exactly two agents.) In these three respects (and some details covered below), Satz’ work can seen as drawing on and broadening the scope of the literature on exploitation. (See the Stanford Encyclopedia entry) I would also classify her account as egalitarian liberal: egalitarian in that it considers substantive equality to be a primary criterion of justice, and liberal on the grounds that is it not (or is not intended to be) socialist (specifically, it is not or is not intended to be anti-capitalist or critical market societies).\nSatz’ account focuses on four “parameters” or “dimensions,” and provides rough guidelines for refining intuitions rather than strictly necessary or sufficient conditions: “High scores along one of these dimensions, or several of them together, can make any market appear ‘noxious’ to us.” (98) These four dimension are organized into two categories, one dealing with the consequences of markets and the other dealing with the sources or “underlying condition” of agents in markets:\nSo, a specific market that features all four of these to some extent is noxious. Or, a specific market that features only one but to an egregious extent – say, egregious exploitation – would also seem to qualify as noxious.\nI’ll discuss each of the four dimensions in turn."
  },
  {
    "objectID": "posts/2013-01-11-satz-account-of-noxious-markets.html#weak-agency",
    "href": "posts/2013-01-11-satz-account-of-noxious-markets.html#weak-agency",
    "title": "Satz’ Account of Noxious Markets",
    "section": "Weak agency",
    "text": "Weak agency\nSatz does not seem to give a clear definition of “weak agency” or “agency failure,” but instead gives various examples, primarily information asymmetries and externalities (“the market has serious indirect effects on people who are not involved in the market transactions,” 97). Both kinds of agency failures are related to market failures in the familiar economists’ sense.\nAsymmetric knowledge describes a situation in which one of the parties in the transaction has significantly less (relevant) information than the other. A cliche example would be a used car salesman who knows that the car he’s selling to the ignorant customer has a faulty transmission. Or, more noxious, an infant formula manufacturer who knows she’s selling products contaminated with melamine. In a perfect market, all agents have perfect information about all of the available goods.\nExternalities refer to costs and benefits that are not represented in the transaction (for example, in the cost paid by the buyer). Satz’ examples of externalities seem to primarily deal with agents whose interests are simply not represented in the transaction at all: children in labor markets, future generations, and “markets involving the production, purchase, and dissemination of information that fail to present relevant alternative points of view about a pressing political issue” (97). Externalities in this sense seem to amount to barriers to enter the market. Future generations, for example, have no way to enter contemporary negotiations that set the price of carbon emissions, and so no way to have the harms they suffer from climate change represented in the price paid today to emit carbon. (In this sense, climate change is a market failure!)\nOne way in which we might deal with barriers to market entry is to set up principal-agent schemes: the agents who cannot directly participate in the market (because of high barriers to entry – they are nonverbal or don’t exist yet, for instance) can be represented by other agents who can directly participate (because of lower barriers to entry). (The out-of-market agents are the principals and their representatives are the agents. The equivocal use of “agent” will only happen in this paragraph.) For example, parents are assumed to represent the interests of their children. However, “[t]his is less easy than one might think. Not only can it be costly to transmit information, but the principal can have opposing interests to the agent.” (77) For example, ineliminable uncertainty about the effects of climate change limit our ability to represent the interest of future generations in the contemporary carbon emissions market; and our interest in cheap energy now tends to frustrate attempts to represent their interests.\nSatz claims that weak agency cannot be reduced to harm:\n\nAlthough the majority of troubling markets characterized by weak agency involve extremely harmful outcomes, it is possible to be concerned by such markets even in the absence of harms. In this category would fall product markets that target young children; markets involving the production, purchase, and dissemination of information that fail to present relevant alternative points of view about a pressing political issue; and markets whose products are based on deception, even when there is no serious harms. (97)\n\nThis leads to a nice contrast between Satz’ account and welfare economics. From the latter perspective, a perfect market is good because it produces a Pareto-optimal distribution; weak agency is bad because, as a deviation from a perfect market, is tends not to produce a Pareto-optimal distribution. Thus the welfare economics perspective is thoroughly consequentialist. On Satz’ account, a market with a great deal of weak agency is noxious even if the distribution that resulted were Pareto-optimal. More generally, it will be noxious independent of any consequences. Hence her account is non-consequentialist. (Applied ethicists and non-philosophers writing in applied ethics often work with a dichotomy between consequentialist and deontic ethical theories. Strictly speaking, a deontic account would be one in which ethics is cast in terms of rights. Note that Satz’ account does not seem to be deontic in this sense: asymmetric information does not seem to violate anyone’s rights.)"
  },
  {
    "objectID": "posts/2013-01-11-satz-account-of-noxious-markets.html#vulnerability",
    "href": "posts/2013-01-11-satz-account-of-noxious-markets.html#vulnerability",
    "title": "Satz’ Account of Noxious Markets",
    "section": "Vulnerability",
    "text": "Vulnerability\nSatz explicitly relates vulnerability in the market to the risk of exploitation:\n\nWhen people come to the market with widely varying resources or widely different capacities to understand the terms of their transactions, they are unequally vulnerable to one another. In such circumstances the weaker party is at risk of being exploited. (97)\n\nAs Hallie Liberto points out in a current work-in-progress, exploitation is usually understood in terms of fairness: the impoverished kidney-seller is exploited because it’s not fair for him to only get $150 in return. Her proposed definition understands exploitation in terms of vulnerability:\n\nA wrongfully exploits B when and only when (1) A wrongfully extracts benefits from B at some cost to B, and (2) B would not participate in the extraction, with its specific terms, if it were not for some vulnerability experienced by B with respect to A, and (3) the particular vulnerability experienced by B contributes to the wrongness of A’s extraction. (Liberto, 20)\n\nSo the impoverished kidney-seller is exploited because he wouldn’t make the deal if he were impoverished and his impoverishment contributes to the wrongness of only paying him $150 for his kidney.\nIn the context of markets, this account of exploitation is somewhat like, but not strictly the same as, barriers to market exit (another familiar kind of failure to realize a perfect market). In traditional Marxist accounts of exploitation, proletarians are vulnerable to having surplus labor extracted from them precisely because they cannot exit the labor market: they will literally die of starvation and exposure if they do not sell their labor. G.A. Cohen pointed out that this is not quite right, especially as we move into the twentieth century: workers have the option of becoming entrepreneurs. (Cohen, part III) However, this escape from exploitation is by no means a sure thing; arguably still involves exploitation (in the form of paying back the small business loan); and, while it may be viable for individual proletarians, it is (probably) not viable for the proletariat as a class. Proletarians on the labor market are vulnerable, even though in the strict sense they are free to exit the market."
  },
  {
    "objectID": "posts/2013-01-11-satz-account-of-noxious-markets.html#harmful-outcomes-for-individuals",
    "href": "posts/2013-01-11-satz-account-of-noxious-markets.html#harmful-outcomes-for-individuals",
    "title": "Satz’ Account of Noxious Markets",
    "section": "Harmful outcomes for individuals",
    "text": "Harmful outcomes for individuals\nThe other two dimensions of noxious markets deal explicitly with consequences, especially harmful consequences. Unless I missed something, Satz does not do much to indicate how she understands harm (and benefit). However, she relates Adam Smith’s “interest[] in the role of markets in enabling substantive freedoms, … and not only in their role in satisfying preferences” to Amartya Sen’s work in the capabilities approach (50; also 95). Later, she speaks favorably of Elizabeth Anderson’s argument that “giving money, even a great deal of money, to a child who has not been educated will not compensate for her lack of education, even if cash is what she (as an adult) now herself prefers …. [I]t does not turn her into a citizen who can participate competently and meaningfully in democratic self-governance.” (101) Anderson has explicitly developed this as an argument for the capabilities approach. (Anderson)\nSo, while it may not be clear precisely which version of the capabilities approach Satz might endorse, it is clear that she disagrees with welfare economists’ use of subjective preferences and other economists’ use of wealth as exhaustive measures of well-being.\nFurthermore, Satz’ concern with harmful is not limited to either the immediate consequences of a single transaction or the distribution of harms and benefits in a timeless economic equilibrium. For example, in line with her favorable references to Anderson, one of Satz’ argument against many forms of child labor is the harmful it effects they are likely to have on these children when they are adults: “The costs of child labor can extend far into the future, having, for example, long-term adverse effects on the child’s health.” (158) “[C]hildren who work and do not go to school will likely lack the capacities that they need – literacy, numeracy, broad knowledge of personal and social alternatives, communication skills – to effectively exercise their agency as adults.” (165)"
  },
  {
    "objectID": "posts/2013-01-11-satz-account-of-noxious-markets.html#harmful-outcomes-for-society",
    "href": "posts/2013-01-11-satz-account-of-noxious-markets.html#harmful-outcomes-for-society",
    "title": "Satz’ Account of Noxious Markets",
    "section": "Harmful outcomes for society",
    "text": "Harmful outcomes for society\nThis dimension might be misread as drawing attention to some kind of supraindividual entity, society “as such” or “in itself.” But it’s clear that what Satz means by “society” is something more like “the relations among individuals.” Her initial gloss of this dimension, for example, says that “The operation of these markets can undermine the social framework needed for people to interact as equals, as individuals with equal standing.” In the following paragraph, she worries about “the equality of individuals as co-deliberants and co-participants in making laws that apply to themselves.” (95, her emphasis)\nThe child labor example illustrates one of the social harms that Satz considers most important, hierarchical attitudes:\n\nChild labor can undermine the possibility of a society of equals. Uneducated, illiterate adults will often form a servile social caste, excluded from participating in society’s main institutions …. [B]ecause most of Indiana’s labor force come from the lower classes and is involved in performing menial tasks, the upper-class elite has not thought that education for poor children was necessary. (162)\n\n\nBaldev, a bonded laborer who managed to free himself through a windfall inheritance from a relative …. places little value on his ability to make decisions, exhibition a condition that, following the philosopher Thomas Hill, I will call servility. A servile person not only refuses to press his rights in certain cases, but does not see himself as having rights in the first place. (185)\n\nSimilarly, in her discussions of markets for reproductive labor (that is, surrogate motherhood) and sexual labor (that is, prostitution), Satz worries about the possibility that such markets would reinforce patriarchal hierarchical attitudes:\n\nPregnancy contracts involve substantial control over women’s bodies …. On my view, what makes this control objectionable … is not the intrinsic feature of women’s reproductive labor, but rather the way such control specifically reinforces a long history of group-based inequality …. The fact that pregnancy contracts, like military contracts, give someone control over someone else’s body is not the main issue; rather the issue is that in contract pregnancy the body that is controlled belongs to a woman, in a society that historically has subordinated women’s interests to those of men, primarily through its control over women’s sexuality and reproduction. (128-9)\n\n\n[P]rostitution represents women as objects for male use. As I indicated earlier, prostitutes are far more likely to be victims of violence than other professions; they are also far more likely to be raped than other women. A prostitute’s “no” does not, to the male she services as well as to other mean, mean no. (149)\n\nMore generally, Satz recognizes and is concerned about the endogenous nature of preferences, attitudes, and expectations. Economists (and not a few political philosophers; see her discussion of Dworkin on 66-76) standardly treat individuals’ preferences as given outside or prior to the market, that is, as exogenous. But this is clearly false. First, speaking about the market as a whole for a moment, the entire raison d'être of the advertising industry is to manipulate – even create ex nihilo – consumer preferences.\nSecond, markets in specific goods can – I would say “tend to” – produce the expectation that all instances of these goods are available for a market transaction. Call this the market expectation effect. Satz glosses a striking instance of this argument, Scott Anderson’s argument against legalizing prostitution:\n\n[I]f prostitution was viewed as just another job analogous to other forms of employment, then presumably sex could be included as part of any number of jobs. Women who did not wish to have sex on demand might find that their employment options were limited and that they were less employable on the labor market. (149)\n\nIndeed, women might find that in many social situations they would be expected to be sexually available “for the right price”; and this might reinforce misogynistic attitudes that all (heterosexual) sexual relationships are essentially prostitutional – that married men pay their wives for sex through a house and clothes and so on.\nIn a recent paper, Paul Thompson has insightfully discussed another form of the market expectation effect. Thompson’s interest is when a market becomes technologically possible – by contrast, Satz takes the existence of a market to be primarily a legal question. He begins with an illustrative vignette:\n\nImagine yourself sitting on a hilltop, relaxing in the shade of a lone apple tree on a warm mid-autumn afternoon. You have collected all the apples that were lying on the ground and from branches that were within reach and placed them in a bag that you brought along with you. Now, you are simply enjoying the autumn breeze and the call of songbirds, when you see a stranger coming up the hill who is also carrying a bag. After exchanging greetings, the person tells you that they are in search of apples, but they notice that you have beaten them to the harvest. “Can I buy half a dozen from you?” the person asks. Evidently, the apples in your bag manifest at least some minimal characteristics of the commodity form….\n\n\nIf my thought experiment had the approaching stranger inquire about buying the view from the hilltop, the sound of the birds or the warmth of the breeze, the narrative arc would have taken an altogether different direction. We would be wanting to know more about the curious norms at work in this scenario. “Do you mind if I sit under your apple tree?” might have been a natural enough question, but “Can I buy the sound of those birds from you?” would not….\n\n\nOr maybe this question only seems unnatural to those of us whose imaginations are limited by the technological capabilities of a bygone era…. [A]s someone who still carries a device I refer to as a “cell phone” that I use almost exclusively to make and receive communications that I still anachronistically refer to as “telephone calls,” it does not occur to me that I might be carrying something that could record the sound of the birdsongs and that I might, for a fee, Email this recording to the stranger so that she could use them as a ringtone or as musical accompaniment for the photograph she is taking of the view from the hilltop on her own pocket device. Perhaps, the birdsongs do avail the commodity form when these possibilities are envisioned, and perhaps were I a bit more up to date, I would respond quite naturally to the query “Can I buy the sound of those birds from you?” with my own question: Is there an app for that? (Thompson, 88-90)\n\nWithout portable recording devices, the birdsong cannot be stored, transported, or distributed; it is ephemeral. This is not the same as calling it a public good, in the economists’ sense. To be a public good in this sense, a good must be nonrivalrous and nonexcludable. That is, use by one agent doesn’t prevent other agents from using or enjoying it (nonrivalrous) and it is difficult (or impossible) to prevent other agents from using or enjoying it (nonexcludable). The live birdsong is just as nonrivalrous and nonexcludable as it was before: Thompson’s enjoyment doesn’t prevent the stranger from enjoying it, and it’s difficult to prevent her from enjoying it. (He would have to fence off the entire hilltop, say.) And this doesn’t change depending on whether or not there are portable recorders around. The portable recording device makes it possible to store the birdsong indefinitely, transport it away from the hilltop, and distribute it among various consumers. Without this possibility, there’s no more of a market for birdsong than there is for dreams. The market expectation effect is then that, when it is possible to handle the birdsong in this way, there will be a market for it.\nThompson’s example may point out a specific-and-also-general form that the market expectation effect takes in a market society: when it is technologically feasible to offer something on a market, then in a market society it will be so offered. Perhaps this could even serve as a functional characterization of a market society, namely, as a society in which there’s a market for anything precisely insofar as it’s technologically possible to put it on the market.\nWe might generalize the market expectation effect, in a way that turns us back to vulnerability. Consider a situation in which child labor goes from illegal to legal, and suppose many but by no means all impoverished families put their children on the labor market. As a result of the increased labor supply and the lower wages paid to child laborers, many kinds of low-wage labor previously done by adults will be either unavailable – all that work is now done children – or available only at significantly lower wages. Thus, in order to maintain the same income, all impoverished families will have to put their children on the labor market. A small number of children on the labor market has the effect of putting all (impoverished) children on the labor market, not by virtue of a change in attitudes or subjective expectations but by virtue of the impersonal operations of supply and demand.\nThis concludes my summary of Satz’ account. In a future post, I plan to apply her account to commercialized science."
  },
  {
    "objectID": "posts/2013-01-11-satz-account-of-noxious-markets.html#references",
    "href": "posts/2013-01-11-satz-account-of-noxious-markets.html#references",
    "title": "Satz’ Account of Noxious Markets",
    "section": "References",
    "text": "References\n\nElizabeth Anderson, “Justifying the Capabilities Approach to Justice,” in Robeyns and Brighouse, eds., Measuring Justice: Primary Goods and Capabilities, Cambridge UP, ISBN 9780521884518\nG.A. Cohen, History, Labour, and Freedom, Clarendon Press, ISBN 0-19-824779-6\n“Exploitation,” Stanford Encyclopedia of Philosophy, http://plato.stanford.edu/entries/exploitation/\nHallie Liberto, “The Normative Source of Exploitation” (draft manuscript)\n“Perfect Market,” Wikipedia, http://en.wikipedia.org/wiki/Perfect_market\nDebra Satz, Why Some Things Should Not Be For Sale: The Moral Limits of Markets, Oxford UP, ISBN 978-0-19-989261-7\nPaul Thompson, “'There’s an App for That’: Technical Standards and Commodification by Technological Means,” Philosophy of Technology (2012) 25:87–103\n\n\nJanuary 11th, 2013 11:48am gender/sex economics"
  },
  {
    "objectID": "posts/2023-01-26-principle-agent.html",
    "href": "posts/2023-01-26-principle-agent.html",
    "title": "The presence-absence model and epistemic representation",
    "section": "",
    "text": "In 2015-16 I was working for1 the US Environmental Protection Agency. My work colleagues were technocratic managers, most of whom had PhDs and were professionalized as scientists (even when they were on the policymaking, risk management side of things). Because the concept of “legitimacy” in questions about “the legitimate roles for values in science” comes from political legitimacy — at least, it does for me — I started to ruminate on the idea of scientists as representatives, in close analogy to elected political representatives. Manuela Fernández-Pinto and I included a few of these ideas in Fernández Pinto and Hicks (2019). But I haven’t taken the time to develop the analogy further.\nAmong political theorists, a key work on political representation is Hanna Pitkin’s The Concept of Representation (Pitkin 1967). While I haven’t read it myself, my understanding is that Pitkin developed an account of representation in terms of presence and absence. As Mark Warren puts it, “representatives stand in, speak, or act for the represented in their absence: they are represented in the spaces and activities where they cannot be” (Warren 2018, 39, Warren’s emphasis). This way of thinking about representation goes back at least to Hobbes, where the relationship between the people and the sovereign is literally a principal-agent contract (Brown 2009 ch. 5). In this line of political thought, the represented are absent primarily for logistical reasons: it’s impractical to have thousands (or millions) of people in one place all talking to each other and coming to agreement.\nThis model is simplistic, and in later posts I’ll start to explore its limits and follow the intellectual trails constructed by political theorists. But, just as this is a useful first model for political representation, I want to argue that it’s a useful first model for scientists as epistemic representatives. Namely: scientists stand in, speak, or act epistemically for the general public in their absence. Scientists are often epistemic agents on behalf of the public, who are generally absent from the space of technocratic decisionmaking.2\nThe public are typically absent from these spaces for the same logistical reasons that they are typically absent from spaces of political decisionmaking. But also for two other, more interesting reasons. First, the public lacks the credentials of scientists, the diplomas and work history that are taken as evidence that someone has relevant knowledge and skill. And second, the public often lack relevant knowledge and skill.\nWhen I talk about expertise in my undergraduate Critical Reasoning course, I like to illustrate the difference between credentials and actual expertise with the examples of ACT UP and the environmental justice movement (Epstein 1996; Pauli 2019). [Say more here in the actual paper]\nHowever, on any given topic, most people have only very limited knowledge and skill. We are radically epistemically interdependent beings, relying on others with actual expertise to develop knowledge that promotes our epistemic interests (Wilholt 2022). I suggest that this point corresponds to political theorists’ distinction between preferences and interests (Warren 2018, 41). A good political representative acts to promote the considered interests of the constituency — the things they would want if they had the time and resources to engage in thoughtful deliberation — even if those interests don’t align with what they prefer in the moment. Similarly, scientists as epistemic representatives are responsible for producing the knowledge that we would produce if we had the time and resources to do so.\nFollowing Warren’s discussion, we run into a problem:\nThe use of “paternalism” here is notable, because feminist philosophy of science was born out of feminist critics of paternalistic, patriarchal science, especially (but not only) biomedical science. It’s therefore tempting to say that this same paternalism is a key problem for the legitimacy of the technocratic state today. Consider the backlash to lockdown approaches to Covid-19 (Harvard et al. 2021; Harvard and Winsberg 2021; Lee et al. 2021).\nBut let’s follow the political theorists for one more dialectical step. Warren fends off the spectre of Burke by offering “two loci of judgment [that] must be robust for democratic representation to occur:”\nIn other words, democratic representation requires continuous, reciprocal exchange between representatives and their constituents. Representatives might make decisions that contradict the constituents’ preferences. But they will need to be able to justify those decisions in terms that the constituents recognize as their interests. And the constituents will need to engage in quasi-public deliberation and remonstration, and ultimately decide whether or not to accept the claimed interests as theirs (Warren 2018, 45).\nOn this analysis, the problem with the technocratic management state is that it lacks institutions — or even informal effective processes — for the necessary continuous reciprocal exchange.3 And, per my last blog post, this reflects the value-free ideal.\nA responsive, reciprocal relationship between scientists as representatives and their constituents can also help to address the gap between credentials and actual expertise. In environmental justice contexts, one important role of credentialed expert allies is to translate the expertise of community members and activists into the idiom of the technocratic management state (Ottinger and Cohen 2011, 7–8). Activists might also be included more directly in policymaking, as with ACT-UP (Epstein 1996).\nAll together, while it is oversimplified, this first model of representation supports an argument for incorporating more substantive opportunities for direct interaction between publics and experts in technocratic policymaking. For decades, science policy scholars and political scientists have examined a variety of institutional forms for such interactions (Warren and Gastil 2015; Steel et al. 2020; Kaplan et al. 2021). However, it’s worth noting that some European studies have found that deliberative minipublics do not improve legitimacy (Jacobs and Kaufmann 2021; Goldberg and Bächtiger 2022)."
  },
  {
    "objectID": "posts/2023-01-26-principle-agent.html#footnotes",
    "href": "posts/2023-01-26-principle-agent.html#footnotes",
    "title": "The presence-absence model and epistemic representation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, I was working for AAAS, as a Science and Technology Policy Fellow, and “hosted by” EPA.↩︎\nNote that I intend to use “epistemic agent” in exactly the same way as feminist social epistemologists.↩︎\nThis is incorrect as stated. Agencies like EPA have at least two such processes. However, one is formal and ineffective, while the other is effective but fails criteria of democratic inclusion and justice. First, the formal notice-and-comment system solicits feedback from the general public on proposed regulation. But agency staff merely have to provide pro forma responses to these comments, giving the public very little effective political agency in this process (Richardson 1999; Simon F. Haeder and Yackee 2015; Simon F. Haeder and Yackee 2018; Costa, Desmarais, and Hird 2019). Second, regulatory agency staff often have informal interactions with various “stakeholders,” providing opportunities for much more effective reciprocal exchange. But typically the only stakeholders included in these interactions are regulated industries (eg, Gillam 2017). Environmental justice communities, for example, are unlikely to be included in these interactions, making these interactions both exclusionary and injust.↩︎"
  },
  {
    "objectID": "posts/2020-07-10-travis-lab-feedback.html",
    "href": "posts/2020-07-10-travis-lab-feedback.html",
    "title": "Configuring Github and Travis-CI for Automated Lab Feedback",
    "section": "",
    "text": "Last summer, anticipating teaching for the first time since 2013, I started reading about ungrading, and somewhere (maybe even in that piece, I didn’t check) read about a computer scientist who uses continuous integration (CI) to automatically give students feedback on their CS lab assignments. Each lab assignment has well-specified goals, and the CI automated tests evaluate the students’ solutions for correctness. Successful completion of the assignment can be counted automatically, specifications-grading style, or the instructor can review the code after it’s working for things like coding style and efficiency.\nThis fall I’ll be teaching a graduate methods course on data science. This seems like a great course for implementing this CI approach. But I haven’t used CI before, and the tutorials for using Travis-CI with R turned out to be unnecessarily complicated, not least because Travis-CI now has good support for R. The purpose of this post is to briefly review how to set up Github and Travis-CI for automated lab feedback.\nEach lab assignment is based on this template repo: https://github.com/data-science-methods/lab-test.\n\nBasic lab workflow\nThis setup assumes a workflow for lab assignments where students clone a Github repository with instructions, data, etc.; complete the assignment in a single R script; that a working solution has deterministic values for variables with set names; and that students submit their work using a pull request. A different programming language (or multiple programming languages) will require different infrastructure for running unit tests. Multiple R scripts (including more complex project structures) will require more articulated test files. Writing unit tests for non-deterministic values will be quite a bit more complicated than the example test in the template.\nThere are three phases for this approach: one-time setup with accounts, and then steps you and your students will do for each lab assignment.\n\n\nInstructor: Account setup\nYou’ll only need to do these steps once.\n\nYou’ll need a Github account.\n\nI assume you already have one of these and that you know basic git terminology and Github workflows.\n\nOptional: I went ahead and created a new organization for my course, so that the course website and lab repos all live together. But that’s strictly unnecessary.\n\n\nYou’ll need a Travis-CI account, which you create using a Github login. Travis-CI is free for working with public Github repositories.\n\nOptional: If you created a new organization for your course, make sure it shows up in your Travis-CI settings: Click on your profile picture (upper-right corner), then Settings. Look for the list of organizations on the left-hand column. If your organization isn’t there, then at the bottom of that column you should see a link to “Review and add your authorized organizations.”\n\nI think that’s basically it. Travis-CI should now be able to see your public repositories.\n\n\nInstructor: Repository setup\nYou’ll do these steps when you create each lab assignment.\n\nIn the template repo, click the green “Use this template” button (where the Clone button usually lives) to create a new repo for the lab.\n\nI’m going to use the naming scheme lab-01 where 01 indicates the week of the course.\n\n\nClone the new repo to the machine where you work.\n\nEdit lab.R with the assignment instructions.\n\nIf you add any packages to the setup, add them to DESCRIPTION as well.\n\nTravis-CI assumes that R repositories are packages. It will automatically install dependencies, but only if all of the dependencies (including testthat) are listed in the DESCRIPTION file. You don’t need any of the usual package metadata; all you need are the list of Imports.\n\n\nFor each problem in the assignment, write appropriate tests in tests/test_lab.R.\n\nYou write tests using testthat expectations: https://testthat.r-lib.org/reference/index.html.\n\n\nIf you changed any file names, make sure they’re consistent across lab.R, tests/test_lab.R (which source()es the assignment script), and .travis.yml (it needs to know where to point testthat::test_dir()).\n\nOptional: Create a solutions branch. Fill in solutions for each problem, and run testthat::test_dir('tests') to check that your instructions and tests work as expected. Cherry-pick any corrections back to master.\nPush master back up to Github.\n\nIn Travis-CI’s Settings, find the lab repo and flip the switch to turn on CI. (In my experience, it can take like 10 seconds for Travis-CI to see the new repo or a push/pull request to an active repo.)\nOptional: Push solutions up to ensure that Travis-CI is working as expected.\n\nNote that there doesn’t appear to be a way to have a private branch of a public repo.\n\n\n\n\nStudent: Lab workflow\nThe students will do these steps when they work on the lab assignment.\n\nFork the lab assignment to their own account, then clone the fork to their working machine.\n\nModify the yaml header for the lab assignment with their name and so on.\nWork in lab.R to complete the assignment, per instructions.\n\nIf any packages are added to the setup, add them to DESCRIPTION as well.\nAt any point, run testthat::test_dir('tests') to get immediate feedback on their progress.\n\nAt any point, submit a pull request to get automated feedback via Travis-CI.\n\nTravis-CI docs on building pull requests\n\nSubmit their work by submitting a final (passing) pull request.\n\nOptional: Use the RStudio knitr button (or rmarkdown::render('lab.R')) to generate a pretty HTML or PDF version of their completed assignment."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a philosopher of science, data scientist, and science policy researcher. My primary academic research focuses on the role of ethical and political values in science and public scientific controversies. I’m also interested in bibliometrics, the use of statistics in scientific practice, and data ethics. As a data scientist, much of my current work involves text and bibliometric data.\nMy pronouns are they/them/their. Since 2019 I’ve been a faculty member at UC Merced.\n\n\nMore details about my career before UC Merced\n\nIn September 2017 I joined the Data Science Initiative at UC Davis as a Postdoctoral Researcher. During my postdoc, I developed a portfolio of projects related to “academic institutional effectiveness.” These projects will draw on a variety of datasets — student enrollment data, faculty data such as grant applications, library or wifi use data, as well external data such as transcripts of government meetings and publication metadata — to address concrete questions about the inner workings and social impact of UC Davis.\nFrom 2015 to 2017 I was a AAAS Science and Technology Policy Fellow. I spent 2015-16 with the US Environmental Protection Agency’s Chemical Safety for Sustainability program; and 2016-17 with the National Science Foundation, where I worked primarily with the National Robotics Initiative. During my time in government, my major projects included portfolio analyses using bibliometrics data.\nFrom 2013 through 2015, I was a Postdoctoral Fellow at the Rotman Institue of Philosophy at Western University in London, Ontario. I finished my PhD in philosophy at the University of Notre Dame in South Bend, Indiana, in April 2012. Before coming to Notre Dame, I did some graduate work in the math and philosophy departments at the University of Illinois, Chicago, and majored in math and political science at the University of Puget Sound in Tacoma, Washington. While at Puget Sound, I was in the Honors Program (a Great Books program by another name) and participated in various debate, music, and undergraduate research activities. I was born and grew up in Northern California.\nI have a deep commitment to social justice work and community-based research. In 2013, I served on on the board of directors of Purple Porch Co-op, a local food co-operative in South Bend, Indiana. I was the board’s secretary and co-chaired two committees related to membership and community events. During 2017 I was involved with a deep canvassing project with the Northern Virginia and Washington, DC, chapters of Showing Up for Racial Justice [SURJ]. I wrote a theory of change for deep canvassing on racial justice and helped pilot the collection and analysis of quantitative and qualitative data. During 2018 I have been involved with the Sacramento chapter of Democratic Socialists of America. I have used my data science and research skills to support the chapter’s major initiatives, including local canvassing for Proposition 10, a rent control initiative."
  }
]