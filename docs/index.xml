<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Dan Hicks </title>
<link>https://dhicks.github.io/index.html</link>
<atom:link href="https://dhicks.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Website description</description>
<generator>quarto-1.3.433</generator>
<lastBuildDate>Sun, 12 Nov 2023 08:00:00 GMT</lastBuildDate>
<item>
  <title>Epistemic interdependence or epistemic hierarchy</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-11-12-epistemic-interdependence.html</link>
  <description><![CDATA[ 




<p>Given my intests in public scientific controversies and the politics of expertise, it might be a little surprising that I don’t really find much of interest in the social epistemology literature on expertise. I was reading a social epistemology paper yesterday that helped me articulate one reason for this: how we frame the need for expertise.</p>
<hr>
<p>I frame this need in terms of <em>epistemic interdependence</em>. For example, when we started a unit on appeal to authority <a href="https://fairy-shrimp.netlify.app/09-expertise.html#epistemic-interdependence">in Critical Reasoning this semester</a>, I first estimated that</p>
<blockquote class="blockquote">
<p>it would take you about 17 million minutes, or about 287,000 hours, to read the entirety of Wikipedia. Working 24/7/365.26, that’s a bit less than 33 years. Working full time (40 hours/week for 50 weeks per year) it’s 143 years.</p>
<p>And that’s just for the brief, nontechnical summary of collective human knowledge in a single language.</p>
<p>Humans depend on each other in a variety of ways. We need others to take care of us when we’re young, sick, and elderly. We distribute the work of making useful things across different job categories and industries, along with the work of getting useful things from the places they’re made to the places where they’re used. We’re also <strong>epistemically interdependent</strong>: we rely on each other to produce and circulate knowledge about the world.</p>
</blockquote>
<p>Basically, there’s so much to know that one person can’t do it all. We need other people to know things on our behalf. Experts are the people to whom we entrust this knowing. I go on to introduce <a href="https://fairy-shrimp.netlify.app/09-expertise.html#two-sources-of-expertise">two sources of expertise</a>, formal education or training and lived experience. Taking into account lived experience, most people have expertise in something, though often the scope is very local. I give the example of one of the nieces, who has expertise in the social groups of her high school. One student suggested that her mom has expertise in the mystery novels written by her favorite author. I also talk about ACT-UP and environmental justice movements, as examples of marginalized people — generally completely lacking formal training in science — who became scientific experts.</p>
<p>The examples of ACT-UP and environmental justice also motivate a key distinction in feminist accounts of trustworthiness <span class="citation" data-cites="AlmassiRelationallyResponsiveExpert2022">(Almassi 2022)</span>, between <a href="https://fairy-shrimp.netlify.app/09-expertise.html#act-up-and-unresponsive-experts">competence and responsiveness</a>: during the HIV-AIDS crisis, public health authorities were entirely competent credentialed experts, but were unresponsive to the concerns of ACT-UP and consequently were not trustworthy. We can same basically the same thing about EPA and environmental justice communities.</p>
<p>In this framing, I’m assuming that most people have more or less the same potential or “innate capacity” for expertise. The difference between credentialed experts and groups like ACT-UP is primarily the accident of privilege vs.&nbsp;marginalization, and even marginalized groups can successfully challenge credentialed experts in highly technical domains — though this probably requires a lot of organization and hard work, and will be of very restricted scope.<sup>1</sup></p>
<hr>
<p>By contrast, here’s how <span class="citation" data-cites="GrundmannFacingEpistemicAuthorities2021">Grundmann (2021)</span> frames expertise:</p>
<blockquote class="blockquote">
<p>Epistemic abilities are unequally distributed in society. Not everyone has the same cognitive competences. Some people stand out by being much more reliable than average …. Epistemic superiority is the product of two independent factors: the available body of evidence and one’s reasoning competences …. Some people have a more reliable judgment than others because they are more clever in drawing rational inferences from the shared body of evidence. This is, e.g., the case when Sherlock Holmes outperforms Watson in his judgments about who was the murderer. <span class="citation" data-cites="GrundmannFacingEpistemicAuthorities2021">(Grundmann 2021, 138–39)</span></p>
</blockquote>
<p>Call this framing <em>epistemic hierarchy</em>. Given a set of evidence, this framing suggests that we can rank people in order of their “cognitive competences,” and the experts are the ones with the best/most/greatest cognitive competence. This hierarchical framing is reinforced by referring to non-experts as “laypeople,” evoking the formal hierarchy of the Catholic church.</p>
<p>Grundmann does qualify this hierarchical framing, noting that “epistemic superiority … is always relative to a domain of expertise” (ibid.), and of course often some people will have access to more or better evidence than others. But these qualifications are often quietly dropped or bracketed. Grundmann defines “epistemic authority” with respect to domain D; but then refers to “superior reasoning competences” simpliciter, that is, not <em>domain-specific</em> reasoning competences. The definition also requires that the candidate authority “has very likely considered all of [the trustor’s] relevant evidence” (ibid.) By dropping these qualifications — and using Sherlock Holmes as the paradigm of an expert — this framing tends to suggest that expertise is primarily a matter of cognitive competence. And, further, seeing cognitive competence as “unequally distributed in society” suggests to me that cognitive competence — and thus expertise — is primarily a matter of innate “general intelligence.”<sup>2</sup><sup>3</sup> In short, with this framing, it seems to me quite easy to slide from domain-specific and dynamic comparisons that might not form a total order (A has better evidence, B has better domain-specific reasoning) to a domain-general epistemic hierarchy based on an innate ability.</p>
<p>(I want to stress that the implicatures and slides I see in the last paragraph aren’t logical entailments. I’m not claiming that, if you adopt this framing, you’re thereby committed to some sort of epistemic <a href="https://en.wikipedia.org/wiki/Great_chain_of_being">scala natura</a>. My point is more that this framing coheres with the idea of a natural hierarchy based on intelligence, and can easily be read as either assuming or justifying such a hierarchy.)</p>
<p>Working with the epistemic hierarchy framing, <span class="citation" data-cites="GrundmannFacingEpistemicAuthorities2021">Grundmann (2021)</span> goes on to argue for the “Preemption View” of “layperson”-expert relations:</p>
<blockquote class="blockquote">
<p>When we discover that an epistemic authority believes that p, we should not make any more use of our own reasoning about p as evidence for or against p.&nbsp;The use of our own reasoning concerning p is to be bracketed. <span class="citation" data-cites="GrundmannFacingEpistemicAuthorities2021">(Grundmann 2021, 140)</span></p>
</blockquote>
<p>In short, the “layperson” should defer to the expert’s judgment, and in particular should not challenge the expert’s reasoning, because of the expert’s superior cognitive capabilities (within the domain, and given that the expert has a superset of the layperson’s evidence). I didn’t read the rest of the paper carefully, but from here <span class="citation" data-cites="GrundmannFacingEpistemicAuthorities2021">Grundmann (2021)</span> seems to argue that a failure to follow the Preemption View explains why we have a problem with conspiracy theories and rampant misinformation.</p>
<p>It’s hard to read the cases of ACT-UP, environmental justice, or feminist history of science into this analysis. I suppose one could say that the marginalized groups in each case had access to evidence that the purported authority did not, or the purported authorities neglected relevant evidence, or something like this; and thus they were not actually authorities. But then the whole analysis of <span class="citation" data-cites="GrundmannFacingEpistemicAuthorities2021">Grundmann (2021)</span> is irrelevant, in a strictly logical sense: an early premise in the big argument (145) is false and so we get no particular guidance on these kinds of cases.</p>
<hr>
<p>One additional contrast between these framings seems noteworthy. Epistemic interdependence lends itself to posing the “question of expertise” in terms of trust: who should I trust (or) who is trustworthy? Starting by asking about trust, and reflecting on trust and vulnerability — in the footsteps of feminist philosophers — can lead us to a conception of trustworthiness that requires responsiveness along with (epistemic) competence. Expertise is a power relation, and the people with power (experts) can cause great harm when they abuse this power or use it to perpetrate injustice.</p>
<p>The epistemic hierarchy framing seems to understand the “question of expertise” in terms of the doxastic attitude that an (abstracted) agent should take towards an (abstracted) proposition: should S believe that p? Specifically, should S believe that p on the grounds that this other person A says so? Insofar as the ultimate virtue of belief is truth, the ultimate virtue of an expert is whether their claims are more likely to be true. Unless they can be reframed in terms of truth,<sup>4</sup> questions of power, vulnerability, and justice are lost in the brilliant glare cast by truth.</p>
<p>So this is one major reason why so much of the social epistemology literature leaves me cold. Not only does it deploy a rival framing of the question of expertise — with some implicatures that I find highly disagreeable — but, within this framing, it’s difficult to explore the kinds of cases that motivate me and ask the kinds of questions that I want to ask.</p>





<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-AlmassiRelationallyResponsiveExpert2022" class="csl-entry">
Almassi, Ben. 2022. <span>“Relationally Responsive Expert Trustworthiness.”</span> <em>Social Epistemology</em> 36 (5): 576–85. <a href="https://doi.org/10.1080/02691728.2022.2103475">https://doi.org/10.1080/02691728.2022.2103475</a>.
</div>
<div id="ref-GrundmannFacingEpistemicAuthorities2021" class="csl-entry">
Grundmann, Thomas. 2021. <span>“Facing Epistemic Authorities: Where Democratic Ideals and Critical Thinking Mislead Cognition.”</span> In <em>The Epistemology of Fake News</em>, edited by Sven Bernecker, Amy K. Flowerree, and Thomas Grundmann, 0. Oxford University Press. <a href="https://doi.org/10.1093/oso/9780198863977.003.0007">https://doi.org/10.1093/oso/9780198863977.003.0007</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This point didn’t come up in class, but we could also say that queer communities often had expertise in <em>what it’s like to life with HIV-AIDS</em> that public health authorities generally lacked. The epistemic interdependence runs both ways; no party has uniformly better knowledge than any other party.↩︎</p></li>
<li id="fn2"><p>A dedicated Holmes fan might object that Holmes’ abilities were much more dependent on domain-specific reasoning and a vast mental storehouse of esoteric evidence than “innate general intelligence.” This is correct; I vaguely recall a scene in which Watson is surprised that Holmes doesn’t know some commonplace fact (maybe Holmes doesn’t recognize the name of a popular opera singer?), and Holmes explains that he is intentionally ignorant of basically everything that’s not relevant to solving crime. But I would argue that Doyle also portrays Holmes’ capabilities as a matter of innate general intelligence — for example, his brother Mycroft has the same innate genius, just applied to national security and espionage — and the <em>popular</em> understanding of Holmes emphasizes his supposed innate general intelligence.↩︎</p></li>
<li id="fn3"><p>I’ve also spent a lot of time the last 2-1/2 year working on a project on race science, which might make me oversensitive to the possibility that ideas of innate general intelligence are waiting just offstage.↩︎</p></li>
<li id="fn4"><p>I think the “epistemic injustice” literature is often trying to do this, which is probably why I don’t find that literature particularly engaging either.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-11-12-epistemic-interdependence.html</guid>
  <pubDate>Sun, 12 Nov 2023 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Fact checking ChatGPT: An activity for Critical Thinking</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-11-10-ChatGPT.html</link>
  <description><![CDATA[ 




<p>This week I did a “fact checking ChatGPT” activity with my Critical Thinking class. It has some distinct aims from other ChatGPT activities I’ve seen, and seemed to work well, so I thought I’d write it up real quick.</p>
<p>I’ve seen lots of examples of classroom activities that explore the limitations (and strengths) of ChatGPT as a writer. Some of my students mentioned they or their friends had done things like this. For this activity, I decided to focus instead on ChatGPT as a source of information. We’ve just spent three weeks learning how to evaluate other sources: <a href="https://fairy-shrimp.netlify.app/09-expertise.html">a unit on appeals to authority</a> based on <a href="https://dhicks.github.io/posts/2023-09-25-teaching-expertise.html">feminist work on epistemic trustworthiness</a>, and then <a href="https://fairy-shrimp.netlify.app/11-SIFT-1.html">a unit on fact checking (social) media using the SIFT method</a>.</p>
<p>To prepare for the activity, I asked students to create an OpenAI account if they didn’t already have one, and bring their laptops to class if they don’t usually do so. I also told them to read Ted Chiang’s <a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web">essay</a> “ChatGPT is a Blurry JPEG of the Web,” and set up the Google Sheet that’s linked below.</p>
<p>Then, in class, I gave students these instructions for the activity:</p>
<ol type="1">
<li><p>Prompt ChatGPT to write a short paper on a topic that includes citations and a bibliography: &gt; Hi ChatGPT. Can you write a short paper on the rise of college debt, including a bibliography with five recent academic citations?</p></li>
<li><p>For each bibliography entry, use search tools to try to determine whether the item actually exists.</p>
<ul>
<li>Does the author list match what ChatGPT said?</li>
<li>Was it published in the place ChatGPT said it was published?</li>
</ul></li>
<li><p>Record your findings on this Google Sheet: <a href="https://tinyurl.com/2xdcz7mo">tinyurl.com/2xdcz7mo</a></p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://dhicks.github.io/img/2023-11-10-ChatGPT-spreadsheet.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The first few rows of the spreadsheet</figcaption>
</figure>
</div>
<p>The spreadsheet was set to “anyone can edit.” I filled in the first few rows myself as an example, using the citations from the essay ChatGPT generated in response to my college debt prompt. Columns C, E, and F (does the item exist, does the author list match, does the publication info match) use data validation rules, restricting the possible values. This makes it simple to set up the pivot table in the “exists?” sheet, which we use in the debrief/reflection phase.</p>
<p>The prompt needs to request citations and a bibliography explicitly; otherwise ChatGPT doesn’t include them. This might be different if you’re using something like Microsoft Bing Chat.</p>
<p>After walking through the instructions abstractly, I gave a short demonstration by repeating the work I’d done with the first couple of rows. This included a quick reminder of Google Scholar, which a student had mentioned a few weeks earlier when we were discussing expertise.</p>
<p>Students spent about 40 minutes working on this activity, mostly in pairs or individually. I said explicitly that I wanted to get as large a sample of bib items as possible, and so didn’t want them to work in their table-groups (5-8 students). It was a nice, chill, student-oriented class session: I just wandered around, answering questions, helping with the rare tricky case, and chatting with students.</p>
<p>To close out the activity, I spent 15 minutes on a whole-class discussion of three questions:</p>
<ol type="1">
<li><p>How frequently did ChatGPT manufacture citations?</p></li>
<li><p>How does this relate to Chiang’s essay, “ChatGPT Is a Blurry JPEG of the Web”?</p></li>
<li><p>What implications does this have for using LLMs like ChatGPT</p>
<ul>
<li>To write papers?</li>
<li>As information sources (Google Bard, Microsoft Bing Chat)?</li>
</ul></li>
</ol>
<p>Question 1 was easy to answer with the pivot table. In our case, ChatGPT manufactured 38% of citations (counting only the ones that do not exist at all) or 58% (including the ones that exist but have metadata errors). A few students said this is what they expected to see — based on doing some similar experiments on their own — but I and other students expected the rate of manufactured citations to be somewhat higher.</p>
<p>Not many of the students had really digested Chiang’s essay, so this turned into an impromptu lecture. We’re covering argument by analogy next week, so I’ll probably use Chiang’s analogy as a recurrent example.</p>
<p>Students were pretty thoughtful about the implications. We talked about how faculty are experts, and can instantly recognize manufactured citations in our areas of expertise. Students were a little uncertain about how to regard the citations that actually existed. Some felt these were reliable, but others made the inference that you’d have to actually go and read the paper itself to figure out whether it made the claims ChatGPT was attributing to it. In their one-minute papers, lots of students indicated that they no longer trust ChatGPT as a source of information.</p>
<p>All together, this activity went really well: students were very engaged, and came away with a healthy skepticism of LLMs. Next time, I might make the spreadsheet-filling itself homework rather than an in-class activity. Specifically, I might devote the last 15-20 minutes of the previous session to getting student started on filling in the spreadsheet, then have them complete it (for two different prompts) before the next session. Then, in “the ChatGPT session proper,” we’d start with 10 minutes or so on any tricky references, and have about an hour for the reflective discussion. This would give us lots of time to unpack Chinag’s essay and relate it to the results of the fact check.</p>
<hr>
<p>Coincidentally, this morning <em>Nature</em> published a very nice Perspective piece that proposes two non-anthropomorphic conceptual frameworks for thinking about the behavior of LLMs <span class="citation" data-cites="ShanahanRolePlayLarge2023">(Shanahan, McDonell, and Reynolds 2023)</span>. Their second framework is a bit technical for the typical first-year undergrad. But the first framework is very accessible: LLMs as role-playing, or specifically as actors in an improv show.</p>
<blockquote class="blockquote">
<p>If the model has generalized well from the training data, the most plausible continuation [of the prompt text] will be a response to the user that conforms to the expectations we would have of someone who fits the description in the preamble. In other words, the dialogue agent will do its best to role-play the character of a dialogue agent as portrayed in the dialogue prompt ….</p>
</blockquote>
<blockquote class="blockquote">
<p>What sorts of roles might the agent begin to take on? This is determined in part, of course, by the tone and subject matter of the ongoing conversation. But it is also determined, in large part, by the panoply of characters that feature in the training set, which encompasses a multitude of novels, screenplays, biographies, interview transcripts, newspaper articles and so on. In effect, the training set provisions the language model with a vast repertoire of archetypes and a rich trove of narrative structure on which to draw as it ‘chooses’ how to continue a conversation, refining the role it is playing as it goes, while staying in character.</p>
</blockquote>
<p>Like an experienced improv actor, a LLM is able to adopt the distinctive behaviors of the role they’re playing, making the role very convincing. But — when playing the role of an expert — this doesn’t come with the competence required for actual expertise. An actor can play an expert in starship engineering, for example, saying the kinds of things we’d expect starship engineers to say (based on decades of fiction featuring starship engineer characters), without any actual knowledge of starship engineering. Similarly, a LLM can convincingly say the kinds of things that we expect student loan experts to say — including producing the kinds of citations that student loan experts use in their papers — without any actual knowledge of student loans.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-ShanahanRolePlayLarge2023" class="csl-entry">
Shanahan, Murray, Kyle McDonell, and Laria Reynolds. 2023. <span>“Role Play with Large Language Models.”</span> <em>Nature</em>, November, 1–6. <a href="https://doi.org/10.1038/s41586-023-06647-8">https://doi.org/10.1038/s41586-023-06647-8</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-11-10-ChatGPT.html</guid>
  <pubDate>Fri, 10 Nov 2023 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Teaching ethics of expertise in Critical Thinking</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-09-25-teaching-expertise.html</link>
  <description><![CDATA[ 




<p>I’m teaching expertise and appeals to authority in Critical Thinking about a month, and start to wrestle yet again with how to approach this.</p>
<p>The textbook I use — like pretty much all of them? — takes the “believe the impartial sources who have the right credentials” approach. Completely out of touch with five decades of feminist philosophy and STS.</p>
<p>I want to start by talking about multiple sources of expertise (credentials, but also lived experience), then Baier’s conception of trust and subsequent work on the ethics+epistemology of trust. Using Baier’s conception suggests two relatively clear criteria for a trustworthy expert:</p>
<dl>
<dt>competence</dt>
<dd>
On the topic in question, the expert is likely to have true beliefs.
</dd>
<dt>responsiveness</dt>
<dd>
“[T]hey have encapsulated my interests: the trusted party pursues or protects particular interests, at least in part, because they are the trusted party’s interests …. the expectation that the [expert] will be directly and favorably moved by the thought that we are counting on her” <span class="citation" data-cites="AlmassiRelationallyResponsiveExpert2022">(Almassi 2022, 578)</span>
</dd>
</dl>
<p>From here we can talk about why different groups of people can reasonably assess the trustworthiness of experts differently. And <span class="citation" data-cites="AlmassiRelationallyResponsiveExpert2022">Almassi (2022)</span> has a really nice argument that “owning the libs” — communicating that one is actively hostile towards an outgroup — isn’t actually evidence that they’re interested in promoting the ingroup’s interests, and thus does not make them more trustworthy for members of the ingroup. Which will nicely fit into our discussion, over the subsequent two weeks, of the SIFT method for assessing online information sources and then whether ChatGPT is trustworthy (spoiler alert: it is not).</p>
<p>So class time looks good. Where I’m feeling uncertainty is in assessment.</p>
<p>In this class, the discrete course units are assessed using a weekly quiz. The quiz has a simple format: students are given an argument as a paragraph of prose, and then have to answer a series of short response questions that walk them through analyzing the argument. The short response questions are explicitly tied to “rules” that the textbook gives for assessing that type of argument. In theory, this direct line between assessments and course content will help students study. For example, last year’s course used this question to get at competence:</p>
<blockquote class="blockquote">
<p>What is the source’s area of expertise? Is this credentialed or experiential expertise? (Rule 14)</p>
<p>Expectations: 1 sentence characterizing area and source of expertise.</p>
</blockquote>
<p>A typical prompt/argument to analyze for this unit goes like this:</p>
<blockquote class="blockquote">
<p>Massimiliano Vasile, an aerospace engineer at the University of Glasgow, spent two years comparing nine different technologies that could be used if an asteroid were on a collision course with Earth. Dr.&nbsp;Vasile’s study revealed that it would be a bad idea to blow up an incoming asteroid with nuclear weapons. Thus, blowing up Earth-bound asteroids with nuclear weapons is a bad idea. <span class="small">(Adapted from: Lia Miller, “The Best Way to Deflect an Asteroid,” New York Times Magazine, December 9, 2007, &lt;http://www.nytimes.com/2007/12/09/magazine/ 09_5_asteroid.html&gt;)</span></p>
</blockquote>
<p>Prompts like this contain no useful information for assessing responsiveness. Examples of appeal to expertise like this “seem to treat our communities of inquirers rather generically …. experts’ particular relationship to us … is elided, treated as irrelevant to the question at hand” <span class="citation" data-cites="AlmassiRelationallyResponsiveExpert2022">(Almassi 2022, 578)</span>.</p>
<p>Some prompts do give negative information, indications that the expert is not responsive to our interests, in the form of indicators that the source has a financial conflict of interest, “four out of five doctors smoke Camels.” That’s something, but I don’t want to reduce responsiveness to a question about conflicts of interest.</p>
<p>I need 7 of these prompts (there’s a new quiz each week, so students can re-take them until they pass). Scrapping them all — or almost all of them — and writing whole new ones would be a lot of work. If I have to go actively looking, it can take 60-90 minutes to find a good example — usually from a piece of science journalism — and distill it into a single, undergrad-accessible paragraph.</p>
<p>My dilemma is this: Stick with an inadequate approach to expertise, or put in an extra 7-10 hours of work to rewrite the quizzes?</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-AlmassiRelationallyResponsiveExpert2022" class="csl-entry">
Almassi, Ben. 2022. <span>“Relationally Responsive Expert Trustworthiness.”</span> <em>Social Epistemology</em> 36 (5): 576–85. <a href="https://doi.org/10.1080/02691728.2022.2103475">https://doi.org/10.1080/02691728.2022.2103475</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-09-25-teaching-expertise.html</guid>
  <pubDate>Mon, 25 Sep 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Do scientists have power? Does science?</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-09-20-power.html</link>
  <description><![CDATA[ 




<p>I recently read (most of) <span class="citation" data-cites="LukesPowerRadicalView2005">Lukes (2005)</span>. Lukes offers an influential account of (political) power, and I wanted to see if it would be useful for answering a question that came up in a discussion with (IIRC) Heather Douglas and Matt Brown quite a while ago: Do scientists have power?</p>
<p>Lukes identifies three “faces” of power. Unless I missed something, his definitions for these “faces” aren’t very clear; they’re often demonstrative or negative. And he doesn’t label them. So here’s a quick attempt at labeled, positive definitions:</p>
<dl>
<dt>coercive</dt>
<dd>
This is what we usually think of as political power: comply or you will be harmed.
</dd>
<dt>prioritizing</dt>
<dd>
Influencing whose interests have priority, are seen as important and should be protected or promoted. Lukes talks about this with respect to agenda-setting, as in whose complaints show up on the agenda.
</dd>
<dt>ideological</dt>
<dd>
Influencing how interests are recognized and understood. In terms of Brown’s Deweyan model of inquiry <span class="citation" data-cites="BrownScienceMoralImagination2020">(Brown 2020)</span>, ideological power shapes not only how a problem is framed, but potentially also <em>whether a situation is experienced as indeterminate or problematic in the first place</em>.
</dd>
</dl>
<hr>
<p>Given these definitions, do scientists have power?</p>
<p>As posed, the question is ambiguous, between individualistic and corporate readings of “scientists.” Let’s use “scientists” for the individualistic reading (do certain particular scientists have power?) and “science” for the corporate reading (does science as an institution have power?)</p>
<p>Individual scientists basically never have coercive power. Even when a scientist, qua scientist, works for the state, the role of scientist doesn’t come with law enforcement powers or some other entitlement to coercive power.</p>
<p>On the other hand, individual scientists can have prioritizing and ideological power. In the risk assessment framework, the first phase is “problem formulation and scoping,” which is supposed to be on the “politics” side of the science/politics dichotomy. But typically risk assessment is initiated in response to scientific findings — findings made, communicated, and taken up by certain particular scientists, qua scientists — that indicate a problem and its potential severity.</p>
<p>In these kinds of cases, individual scientists generally won’t have <em>exclusive</em> prioritizing and ideological power. At EPA or other regulatory agencies, certain particular political appointees and “regulatory” (as opposed to “scientific”) staff will also exercise these forms of power.</p>
<p>And, outside of regulatory agencies, individual scientists often do not have any more prioritizing or ideological power than any other member of the public. Well-connected faculty at prestigious universities might be able to exercise this kind of power. (I’m thinking of Scott Atlas, a Hoover Institution fellow who was on Trump’s Coronavirus Task Force, or the psychologist Arthur Jensen, who was a professor at Berkeley and became well-know to the public for his race science views.)</p>
<p>So, academic scientists, <em>as individuals</em>, generally don’t have much of any of the three forms of power.</p>
<hr>
<p>Turn now to science in the corporate sense, and specifically institutions that are taken to speak on behalf of science: EPA, IPCC, the National Academies, AAAS.</p>
<p>Regulatory agencies, like EPA, do have coercive power. Though the prioritizing and ideological power of regulated industries means that EPA doesn’t exercise that coercive power as much as I would like. Still, insofar as EPA is a representative of “science,” and does exercise coercive power, we can understand this as science exercising (and therefore having) coercive power.</p>
<p>The other science institutions that I listed above don’t have coercive power. But they do have substantial prioritizing and ideological power. The National Academies basically made up the risk assessment framework, which is a fundamental component of how environmental health threats are understood and prioritized. Indeed, even the category label “environmental health threats” adopts some assumptions of the risk assessment framework. Opponents of genetically modified foods have a number of concerns that aren’t related to health and safety <span class="citation" data-cites="HicksGMOsNonhealthIssues2016">(Hicks and Millstein 2016)</span> and are therefore systematically ignored by regulatory agencies <span class="citation" data-cites="HicksGeneticallyModifiedCrops2017">(Hicks 2017)</span>.</p>
<p>Using Lukes’ terminology, a whole strain of STS is devoted to studying the prioritizing and ideological power of scientism: on the assumption that only “scientific” evidence counts as evidence, claims that a situation is problematic, is sufficiently severe and warrants action, and that the key cause of the problem is such-and-such, all need to be justified by “scientific” evidence <span class="citation" data-cites="WynneSheepfarmingChernobylCase1989 EpsteinImpureScience1996 FrickelUndoneScienceCharting2010 KleinmanHoneyBeesThreat2013 SuryanarayananVanishingBeesScience2016">(Wynne 1989; Epstein 1996; Frickel et al. 2010; Kleinman and Suryanarayanan 2013; Suryanarayanan and Kleinman 2016)</span>.</p>
<p>Scientism has this power because it serves the interests of multiple groups. Most obviously it bolsters the status of scientists. It was taken up by regulatory agencies as a strategy to maintain legitimacy in the face of opposition from regulated industries <span class="citation" data-cites="JasanoffFifthBranchScience1998">(Jasanoff 1998)</span>. But, at the same time, regulated industries have used it to dismiss criticism and block or delay regulation <span class="citation" data-cites="MichaelsDoubtTheirProduct2008">(Michaels 2008)</span>.</p>
<hr>
<p>If we think of political power only in coercive terms, then it’s difficult to come up with examples of the political power of science beyond agencies like EPA. <span class="citation" data-cites="LukesPowerRadicalView2005">Lukes (2005)</span> is explicitly challenging a parallel situation in mid-century political science: a line of research that studied “power,” but understood it only as coercive power, and generally came to the conclusion that there are not significant concentrations of power in American politics. He identified prioritization power in some previous research, and proposed ideological power as all but completely overlooked by empirical political scientists.</p>
<p>When we follow Lukes in thinking of power in terms of shaping the agenda and framing the recognition and understanding of problematic situations, then it’s much easier to articulate the political power of science. Science has power not because it can threaten you with imprisonment, but rather because problems that aren’t articulated and diagnosed “scientifically” are systematically ignored.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-BrownScienceMoralImagination2020" class="csl-entry">
Brown, Matthew. 2020. <em>Science and Moral Imagination: A New Ideal for Values in Science</em>. University of Pittsburgh Press.
</div>
<div id="ref-EpsteinImpureScience1996" class="csl-entry">
Epstein, Steven. 1996. <em>Impure Science</em>. AIDS, Activism, and the Politics of Knowledge. Berkeley, Los Angeles, and Oxford: University of California Press. <a href="http://books.google.ca/books?id=kZOso0FMsrMC&amp;pg=PA39&amp;dq=intitle:Impure+Science+inauthor:epstein&amp;hl=&amp;cd=1&amp;source=gbs_api">http://books.google.ca/books?id=kZOso0FMsrMC&amp;pg=PA39&amp;dq=intitle:Impure+Science+inauthor:epstein&amp;hl=&amp;cd=1&amp;source=gbs_api</a>.
</div>
<div id="ref-FrickelUndoneScienceCharting2010" class="csl-entry">
Frickel, S., S. Gibbon, J. Howard, J. Kempner, G. Ottinger, and D. J. Hess. 2010. <span>“Undone Science: Charting Social Movement and Civil Society Challenges to Research Agenda Setting.”</span> <em>Science, Technology &amp; Human Values</em> 35 (4): 444–73. <a href="https://doi.org/10.1177/0162243909345836">https://doi.org/10.1177/0162243909345836</a>.
</div>
<div id="ref-HicksGeneticallyModifiedCrops2017" class="csl-entry">
Hicks, Daniel J. 2017. <span>“Genetically Modified Crops, Inclusion, and Democracy.”</span> <em>Perspectives on Science</em> 25 (4): 488–520. <a href="https://doi.org/10.1162/POSC_a_00251">https://doi.org/10.1162/POSC_a_00251</a>.
</div>
<div id="ref-HicksGMOsNonhealthIssues2016" class="csl-entry">
Hicks, Daniel J., and Roberta L. Millstein. 2016. <span>“GMOs: Non-Health Issues.”</span> In <em>Encyclopedia of Food and Agricultural Ethics</em>, edited by Paul B. Thompson and David M. Kaplan, 1–11. Springer Netherlands. <a href="https://doi.org/10.1007/978-94-007-6167-4_545-1">https://doi.org/10.1007/978-94-007-6167-4_545-1</a>.
</div>
<div id="ref-JasanoffFifthBranchScience1998" class="csl-entry">
Jasanoff, Sheila. 1998. <em>The Fifth Branch: Science Advisers as Policymakers</em>. Harvard University Press.
</div>
<div id="ref-KleinmanHoneyBeesThreat2013" class="csl-entry">
Kleinman, Daniel Lee, and Sainath Suryanarayanan. 2013. <span>“Honey Bees Under Threat: A Political Pollinator Crisis.”</span> <em>The Guardian</em>, May 8, 2013, sec. Science. <a href="http://www.theguardian.com/science/political-science/2013/may/08/honey-bees-threat-political-pollinator-crisis">http://www.theguardian.com/science/political-science/2013/may/08/honey-bees-threat-political-pollinator-crisis</a>.
</div>
<div id="ref-LukesPowerRadicalView2005" class="csl-entry">
Lukes, Steven. 2005. <em>Power: A Radical View, Second Edition</em>. Palgrave Macmillan.
</div>
<div id="ref-MichaelsDoubtTheirProduct2008" class="csl-entry">
Michaels, David. 2008. <em>Doubt is their product how industry’s assault on science threatens your health</em>. Oxford: Oxford Univ. Press.
</div>
<div id="ref-SuryanarayananVanishingBeesScience2016" class="csl-entry">
Suryanarayanan, Sainath, and Daniel Lee Kleinman. 2016. <em>Vanishing Bees: Science, Politics, and Honeybee Health</em>. Rutgers University Press.
</div>
<div id="ref-WynneSheepfarmingChernobylCase1989" class="csl-entry">
Wynne, Brian. 1989. <span>“Sheepfarming After Chernobyl: A Case Study in Communicating Scientific Information.”</span> <em>Environment: Science and Policy for Sustainable Development</em> 31 (2): 10–39. <a href="https://doi.org/10.1080/00139157.1989.9928930">https://doi.org/10.1080/00139157.1989.9928930</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-09-20-power.html</guid>
  <pubDate>Wed, 20 Sep 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Misinformation and trustworthiness: Frenemies in the analysis of public scientific controversies</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-09-08-misinfo.html</link>
  <description><![CDATA[ 




<p><span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">Lewandowsky et al. (2022)</span> examine public scientific controversies through two contrasting “lenses” or analytical frameworks, using Covid-19 as their primary case study. The first lens, “science denial,” is pretty explicitly scientistic: in cases such as “the link between AIDS and HIV, climate change, evolution, and other clearly established scientific facts,”</p>
<blockquote class="blockquote">
<p>absent new evidence, dissent from the scientifically accepted position cannot be supported by legitimate evidence and theorizing but must necessarily—that is, in virtually all instances—involve misleading or flawed argumentation. <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">(Lewandowsky et al. 2022, 31)</span></p>
</blockquote>
<p>There are some qualifiers in this particular quotation — “absent new evidence” and “<em>virtually</em> all instances” — but in other places these are dropped: “The rules of scientific evidence formation and argumentation are inescapable and cannot be discarded or side-stepped for political expediency” <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">(Lewandowsky et al. 2022, 32)</span> This lens explains public scientific controversies by appealing to a combination of irrationality and disinformation/propaganda.</p>
<p><span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">Lewandowsky et al. (2022)</span> recognize the technocratic implications of the “science denial” lens:</p>
<blockquote class="blockquote">
<p>this insistence on quality of argumentation may seemingly curtail the public’s involvement in any scientifically informed debate. After all, members of the public are often nonexperts on topics and issues whose outcomes affect their lives. <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">(Lewandowsky et al. 2022, 32)</span></p>
</blockquote>
<p>Their first “solution” to this problem is to communicate “scientific issues” using “stories or pictures” <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">(Lewandowsky et al. 2022, 32)</span>. The second “solution” is a segue into the second lens.</p>
<hr>
<p>The “trust” lens explains controversies by appealing to the ways that “people differ in how much trust they put on various information sources (e.g., scientists vs.&nbsp;their neighbor on social media)” <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">(Lewandowsky et al. 2022, 33)</span>. Further, the lens recognizes that differences in trust can be due to differences in <em>trustworthiness</em>.</p>
<blockquote class="blockquote">
<p>Ethnic minorities, for example, have historically been discriminated against in the health care system. Western countries, especially those with colonial histo- ries, have also damaged people’s trust in medical treatments through their previ- ous mistreatment of indigenous populations (Lowes and Montero 2021) and misuse of vaccination centers, for example, by the CIA in its hunt for Osama bin Laden (Reardon 2011). <strong>It is unsurprising that people would question scientific evidence communicated by the same institutions that caused them harm or deceived them in the past</strong> (Jamison, Quinn, and Freimuth 2019). <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">(Lewandowsky et al. 2022, 33, my emphasis)</span></p>
</blockquote>
<p>To elaborate this a little more, it might be helpful to make a three-way distinction, between (a) (occurrent) trust, (b) perceived trustworthiness, and (c) actual trustworthiness. A potential trustee might satisfy criteria for trustworthiness, but be incorrectly perceived to be untrustworthy by the potential trustor. For example, under the influence of politicization campaigns, many US conservatives might incorrectly believe climate scientists to be more interested in their own careers than the public interest, and thus incorrectly perceive climate scientists to be untrustworthy.</p>
<p>The “trust” lens implies that “appreciation of why evidence is mistrusted in these communities is essential” and the importance of “regain[ing] trust rather than dismiss[ing] beliefs based on lived experience as simple denialism” <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">(Lewandowsky et al. 2022, 33)</span>.</p>
<p><span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">Lewandowsky et al. (2022)</span> attempt to bring these two lenses together. Understanding why a group’s “cultural background or lived experience” undermines the trustworthiness of mainstream institutions “can provide pointers as to why [they] engage[] in (or fall[] for)” irrationality and disinformation <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">(Lewandowsky et al. 2022, 34)</span>. In addition (or perhaps “specifically”), understanding the causes of trustworthiness failures “can reveal shortcomings in the scientific process or evidence base …. analysis of those arguments can provide valuable pointers to underlying issues—such as lacking representation in medical research—that can be addressed by suitable policies or remedial research” <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">(Lewandowsky et al. 2022, 34)</span>.</p>
<p>I would add that the example of incorrectly perceiving climate scientists to be untrustworthy suggests that perceived untrustworthiness and misinformation can be co-producing. Climate propagandists have not only promulgated “first order” misinformation about climate change (“it’s all natural variation”) but also “second order” misinformation about climate <em>scientists</em> (“they’ll say whatever it takes to get published”).</p>
<hr>
<p>Unfortunately, in their final section, “Recommendations,” <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">Lewandowsky et al. (2022)</span> seem to revert back to the “science denial” lens alone. Their two primary recommendations are that “misleading and inappropriate argumentation must be identified” and that “when misleading arguments have been identified, they can be used to ‘inoculate’ the public against their ill effects” <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">(Lewandowsky et al. 2022, 35)</span>. The “trust” lens’ emphasis on understanding why scientific institutions might not be (perceived to be) trustworthy has all but disappeared. The “trust” lens does get one paragraph, arguing that “Policies that take into account the reasons underlying misleading arguments can be more effective than those agnostic about these reasons” and that emphasizing “‘winning’ an argument” is unlikely to be successful” <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">(Lewandowsky et al. 2022, 35)</span>. But the final sentence asserts that deliberation “can be achieved” “only when misleading arguments can be identified and rejected” <span class="citation" data-cites="LewandowskyWhenScienceBecomes2022">(Lewandowsky et al. 2022, 35)</span>, that is, only when “science” wins the argument.</p>
<hr>
<p>I really like the way this piece contrasts two common frameworks for understanding public scientific controversies. The scientistic and technocratic implications of the “science denial”/misinformation framework are on full display, which is useful for illustrating why I typically dislike this framework and find it, at best, incomplete. And the attempt to synthesize the two frameworks is useful: as you can see, it prompted me to think about how I understand the role of propaganda and misinformation in my analysis of controversies.</p>
<p>But that “Recommendations” section. To me, the paper reads like there were two sets of authors here. One set working with the misinfo framework, the other with the trustworthiness framework. The trustworthiness folks made a reasonable effort to integrate trustworthiness and misinformation in their section. But the misinfo folks ignored this, relying just on their framework to write the concluding section.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-LewandowskyWhenScienceBecomes2022" class="csl-entry">
Lewandowsky, Stephan, Konstantinos Armaos, Hendrik Bruns, Philipp Schmid, Dawn Liu Holford, Ulrike Hahn, Ahmed Al-Rawi, Sunita Sah, and John Cook. 2022. <span>“When Science Becomes Embroiled in Conflict: Recognizing the Public’s Need for Debate While Combating Conspiracies and Misinformation.”</span> <em>The ANNALS of the American Academy of Political and Social Science</em> 700 (1): 26–40. <a href="https://doi.org/10.1177/00027162221084663">https://doi.org/10.1177/00027162221084663</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-09-08-misinfo.html</guid>
  <pubDate>Fri, 08 Sep 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Revisiting the Nazi Problem</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-06-07-nazi-problem.html</link>
  <description><![CDATA[ 




<p>In theory this will be a quick post. In theory.</p>
<p>At a conference a couple of weeks ago, I was talking (read: complaining) about how, as far as I’m aware, no one has ever really responded to the “Nazi Problem,” a concern about Longino’s conception of objectivity raised by Natalia Baeza in a grad seminar she and I took with Janet Kourany at Notre Dame circa 2007 and that I wrote up in my first publication a few years later <span class="citation" data-cites="HicksLonginoConceptionObjectivity2011">(Hicks 2011)</span>. Yet, with the revitalization of race science over the last 8 years or so, the Nazi Problem is even more urgent today than it was in 2007-ish. Heather Douglas told me she had a brief solution in a recent paper of hers <span class="citation" data-cites="DouglasDifferentiatingScientificInquiry2023">(Douglas 2023)</span>. In this post, after summarizing the Nazi Problem, I’ll present Douglas’ solution and my response.</p>
<p>A key implication of Longino’s conception of objectivity is that it requires the active cultivation of a wide range of perspectives, in order to ensure the critical scrutiny of implicit assumptions made by the scientific mainstream. For Longino this provides an epistemic argument for the active cultivation of women (feminist) and POC (antiracist) scientists. The “Nazi Problem” is that this reasoning seems to apply equally to Nazi/sexist/racist scientists, and therefore it seems that Longino’s account of objectivity requires actively cultivating them as well.</p>
<p>The primary solution to the Nazi Problem that I consider in the paper<sup>1</sup> was to point out an incompatibility between the political liberalism built into Longino’s account — ideas like formal (epistemic) equality — and Nazi anti-liberalism. A sexist can’t authentically comply with norms that require them to treat women as equals, and so we just qualify Longino’s account by saying that it doesn’t require the active cultivation of inauthentic perspectives. On the few occasions where people have responded to the Nazi Problem in print, this is the response they give <span class="citation" data-cites="RolinCanSocialDiversity2017">(for example, Rolin 2017, 123–24)</span>.</p>
<p>The problem with this response, for me, is that it also rules out feminist standpoint theory. Because they think certain standpoints are epistemically advantaged, standpoint theorists also can’t authentically comply with norms that require them to treat disadvantaged standpoints as formal epistemic equals. I make a similar point about the commitment to neutrality between comprehensive conceptions of the good life and communitarian feminists.</p>
<p><span class="citation" data-cites="DouglasDifferentiatingScientificInquiry2023">Douglas (2023)</span>’s response to the Nazi Problem is raised in the context of identifying some key differences between inquiry (science) and democratic politics. A key contribution in this paper is the notion of <em>inquirer façades</em>, “faking one of the central norms of inquiry without following through on its demands.” Douglas proposes that “When inquirer facades are discovered, they can be legitimately ignored by those within the space of inquiry” <span class="citation" data-cites="DouglasDifferentiatingScientificInquiry2023">(Douglas 2023, 133)</span>.<sup>2</sup> Here’s Douglas on the Nazi Problem:</p>
<blockquote class="blockquote">
<p>One might worry, though, that fostering diversity can include bringing into the space of inquiry ideological views impervious to criticism (Hicks, 2011). However, if we keep in mind that all participants in the space of inquiry must be responsive to criticism (norm 2), this concern is alleviated. The ideologically pre-committed cannot survive the demands of criticism and response for long, revealing the fact that they are inquirer facades and thus can be legitimately ignored within the space of inquiry. <span class="citation" data-cites="DouglasDifferentiatingScientificInquiry2023">(Douglas 2023, 136)</span></p>
</blockquote>
<p>I have four comments. My first point is that Douglas has reframed the Nazi Problem in terms of closed-mindedness or “dogmatism.”<sup>3</sup> But, like <span class="citation" data-cites="AlcoffCommentaryElizabethAnderson2006">Alcoff (2006)</span> and <span class="citation" data-cites="YapFeministRadicalEmpiricism2015">Yap (2015)</span>, I don’t regard closed-mindedness as such a severe vice. As Yap puts it, “values such as respecting women’s autonomy and self-determination should be seen as basic commitments and not ‘up for grabs’ in the same kind of way” as our taste in music <span class="citation" data-cites="YapFeministRadicalEmpiricism2015">(Yap 2015, 5)</span>.</p>
<p>Second, rather than thinking the problem with Nazis is that they’re closed-minded, the problem with Nazis is that they’re Nazis. They hold odious racist, sexist, and anti-democratic beliefs, and often embrace offensive political violence as a means of achieving political power.<sup>4</sup></p>
<p>Third, the history of behavior genetics shows how a scientific community can protect racists and sexists by deflecting values-based critique while still responding to technical criticism <span class="citation" data-cites="PanofskyMisbehavingScienceControversy2014">(Panofsky 2014)</span>. Panofsky argues that, prior to Jensen’s 1969 “How Much Can We Boost IQ and Scholastic Achievement?” paper, behavior geneticists explicitly marginalized eugenics and race science, with the goal of avoiding controversy and having their work compared to Nazi race science <span class="citation" data-cites="PanofskyMisbehavingScienceControversy2014">(Panofsky 2014 ch.&nbsp;2)</span>. Consequently many behavior geneticists were at least deeply uncomfortable with the controversy (and Nazi comparisons) that Jensen attracted <span class="citation" data-cites="PanofskyMisbehavingScienceControversy2014">(Panofsky 2014, 80)</span>. However, in reaction to broad criticisms of behavior genetics <em>as such</em> by Lewontin, Kamin, and other critics, behavior geneticists adopted a kind of fortress mentality — with the field under constant threat of repression by political radicals <span class="citation" data-cites="PanofskyMisbehavingScienceControversy2014">(Panofsky 2014, 114)</span> — along with an “absolutist interpretation of intellectual freedom” that did not include any sense of social responsibility for, for example, racist appropriations of behavior genetics research <span class="citation" data-cites="PanofskyMisbehavingScienceControversy2014">(Panofsky 2014, 194–97)</span>. At the same time, behavior geneticists — including those who continue to study intelligence and race — have at least somewhat engaged with technical critiques, at least nominally rejecting a strong hereditarianism in favor of gene-environment interactions, and shifting from twin studies to GWAS (genome-wide association studies) and polygenic indices as genomics technology has developed <span class="citation" data-cites="PanofskyMisbehavingScienceControversy2014">(Panofsky 2014 ch.&nbsp;6)</span>. These changes in the field are regarded as inadequate by many critics, both internal <span class="citation" data-cites="TurkheimerThisTimeMean2022">(Turkheimer 2022)</span> and external <span class="citation" data-cites="DownesChangesHeritabilityUnpredictable2022">(Downes and Kaplan 2022)</span>. But they do give behavior geneticists ground — however thin — to claim that race-and-intelligence researchers are responsive to criticism.</p>
<p>More generally, if Longino’s account can justify excluding Nazis who are closed-minded and/or not responsive to criticism, it would still seem to require actively cultivating Nazis, so long as they’re open-minded and engage in uptake. And, over the past decade or so, right-wing intellectuals and provocateurs — including proponents of race science — have adopted at least the posture of open-mindedness and the value of debate and free speech <span class="citation" data-cites="RobinCriticClownTale2017 ReiheldGimletEyeJournal2018 TaylorFreeSpeechHypocrisy2018">(Robin 2017; Reiheld 2018; Taylor 2018)</span>.</p>
<p>Fourth and finally, I didn’t develop my own solution to the Nazi Problem in <span class="citation" data-cites="HicksLonginoConceptionObjectivity2011">Hicks (2011)</span>. A decade and change later, my own solution would align with Douglas’ recognition that “Both societal and epistemic responsibilities generate reasons for excluding some topics and methods from legitimate inquiry” <span class="citation" data-cites="DouglasDifferentiatingScientificInquiry2023">(Douglas 2023, 129)</span>. Indeed, Douglas discusses the case of behavior genetics:</p>
<blockquote class="blockquote">
<p>Current debates over restraints often combine moral, political, and epistemic concerns. Consider the debate over the legitimacy of research on race-based or gender-based intelligence. The history of such work has been so consistently shoddy that doubts have been raised about whether this is an epistemically productive line of inquiry (Rose, 2009). Further, <strong>it has been so socially harmful that it may not be unreasonable to declare such work outside the space of legitimate inquiry at this point</strong> (Kourany, 2016). Merely pointing to generic scientific freedom or the need to pursue truth is inadequate in the face of these concerns (Ceci &amp; Williams, 2009). Well-formed and reasoned restraints on the space of inquiry have a long history of legitimacy. <span class="citation" data-cites="DouglasDifferentiatingScientificInquiry2023">(Douglas 2023, 130–31, my emphasis)</span></p>
</blockquote>
<p>I’ve emphasized the moral argument, which comes closest to the grounds on which I would exclude racists and sexists. But rather than the downstream social harms of research, I would point directly to the racism and sexism itself. These values — and acting in line with them — are odious and not to be tolerated <span class="citation" data-cites="SchroederLimitsDemocratizingScience2022">(Schroeder 2022)</span>. Nearly 80 years after the formal demise of Nazism, there is good reason why it is almost universally reviled.</p>
<p>The problem with Nazis is that they are Nazis, and this is also the reason why we should not actively cultivate them, in science or elsewhere.</p>





<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-AlcoffCommentaryElizabethAnderson2006" class="csl-entry">
Alcoff, Linda Martín. 2006. <span>“Commentary on Elizabeth Anderson’s <span>‘Uses of Value Judgments in Science’</span>.”</span> <em>Symposia on Gender, Race and Philosophy</em> 2 (1). <a href="https://sgrponline.files.wordpress.com/2019/07/alcoff0106-1.pdf">https://sgrponline.files.wordpress.com/2019/07/alcoff0106-1.pdf</a>.
</div>
<div id="ref-DouglasDifferentiatingScientificInquiry2023" class="csl-entry">
Douglas, Heather. 2023. <span>“Differentiating Scientific Inquiry and Politics.”</span> <em>Philosophy</em> 98 (2): 123–46. <a href="https://doi.org/10.1017/S0031819122000432">https://doi.org/10.1017/S0031819122000432</a>.
</div>
<div id="ref-DownesChangesHeritabilityUnpredictable2022" class="csl-entry">
Downes, Stephen M., and Jonathan Michael Kaplan. 2022. <span>“Changes in Heritability: Unpredictable and of Limited Use.”</span> <em>Behavioral and Brain Sciences</em> 45 (January): e159. <a href="https://doi.org/10.1017/S0140525X2100159X">https://doi.org/10.1017/S0140525X2100159X</a>.
</div>
<div id="ref-HicksLonginoConceptionObjectivity2011" class="csl-entry">
Hicks, Daniel J. 2011. <span>“Is Longino’s Conception of Objectivity Feminist?”</span> <em>Hypatia: A Journal of Feminist Philosophy</em> 26 (2): 333–51. <a href="https://doi.org/10.1111/j.1527-2001.2010.01160.x">https://doi.org/10.1111/j.1527-2001.2010.01160.x</a>.
</div>
<div id="ref-HicksWhenVirtuesAre2022" class="csl-entry">
———. 2022. <span>“When Virtues are Vices: <span>‘Anti-Science’</span> Epistemic Values in Environmental Politics.”</span> <em>Philosophy, Theory, and Practice in Biology</em> 14 (0). <a href="https://doi.org/10.3998/.2629">https://doi.org/10.3998/.2629</a>.
</div>
<div id="ref-PanofskyMisbehavingScienceControversy2014" class="csl-entry">
Panofsky, Aaron. 2014. <em>Misbehaving Science: Controversy and the Development of Behavior Genetics</em>. Chicago, IL: University of Chicago Press. <a href="https://press.uchicago.edu/ucp/books/book/chicago/M/bo16124298.html">https://press.uchicago.edu/ucp/books/book/chicago/M/bo16124298.html</a>.
</div>
<div id="ref-ReiheldGimletEyeJournal2018" class="csl-entry">
Reiheld, Alison. 2018. <span>“A Gimlet Eye: The Journal of Controversial Ideas and Jonathan Anomaly’s <span>‘Defending Eugenics’</span> (Guest Post).”</span> Discrimination and Disadvantage. November 16, 2018. <a href="https://web.archive.org/web/20210227220754/https://philosophycommons.typepad.com/disability_and_disadvanta/2018/11/a-gimlet-eye-a-journal-of-controversial-ideas-and-jonathon-anomalys-defending-eugenics.html">https://web.archive.org/web/20210227220754/https://philosophycommons.typepad.com/disability_and_disadvanta/2018/11/a-gimlet-eye-a-journal-of-controversial-ideas-and-jonathon-anomalys-defending-eugenics.html</a>.
</div>
<div id="ref-RobinCriticClownTale2017" class="csl-entry">
Robin, Corey. 2017. <span>“The Critic and the Clown: A Tale of Free Speech at Berkeley.”</span> September 11, 2017. <a href="http://coreyrobin.com/2017/09/11/the-critic-and-the-clown-a-tale-of-free-speech-at-berkeley/">http://coreyrobin.com/2017/09/11/the-critic-and-the-clown-a-tale-of-free-speech-at-berkeley/</a>.
</div>
<div id="ref-RolinCanSocialDiversity2017" class="csl-entry">
Rolin, Kristina. 2017. <span>“Can Social Diversity Be Best Incorporated into Science by Adopting the Social Value Management Ideal?”</span> In <em>Current Controversies in Values and Science</em>, edited by Kevin C. Elliott and Daniel Steel, 1 edition, 113–29. Routledge.
</div>
<div id="ref-SchroederLimitsDemocratizingScience2022" class="csl-entry">
Schroeder, S. Andrew. 2022. <span>“The Limits of Democratizing Science: When Scientists Should Ignore the Public.”</span> <em>Philosophy of Science</em> 89 (5): 1034–43. <a href="https://doi.org/10.1017/psa.2022.54">https://doi.org/10.1017/psa.2022.54</a>.
</div>
<div id="ref-TaylorFreeSpeechHypocrisy2018" class="csl-entry">
Taylor, Keeanga-Yamahtta. 2018. <span>“The <span>‘Free Speech’</span> Hypocrisy of Right-Wing Media.”</span> <em>The New York Times</em>, January 20, 2018, sec. Opinion. <a href="https://www.nytimes.com/2017/08/14/opinion/the-free-speech-hypocrisy-of-right-wing-media.html">https://www.nytimes.com/2017/08/14/opinion/the-free-speech-hypocrisy-of-right-wing-media.html</a>.
</div>
<div id="ref-TurkheimerThisTimeMean2022" class="csl-entry">
Turkheimer, Eric. 2022. <span>“This Time I Mean It: The Nature–Nurture Debate Is Over.”</span> <em>Behavioral and Brain Sciences</em> 45 (January): e177. <a href="https://doi.org/10.1017/S0140525X21001771">https://doi.org/10.1017/S0140525X21001771</a>.
</div>
<div id="ref-YapFeministRadicalEmpiricism2015" class="csl-entry">
Yap, Audrey. 2015. <span>“Feminist Radical Empiricism, Values, and Evidence.”</span> <em>Hypatia</em>, November, n/a–. <a href="https://doi.org/10.1111/hypa.12221">https://doi.org/10.1111/hypa.12221</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>which was my response to Natalia during seminar↩︎</p></li>
<li id="fn2"><p>One thing that’s a little unclear to me is the extent to which this definition depends on intentions or authenticity rather than behavior. “Faking” suggests something like inauthenticity, a mismatch between actual and voiced commitments, that I think can be difficult to get at empirically <span class="citation" data-cites="HicksWhenVirtuesAre2022">(Hicks 2022, 4n3)</span>. But norm compliance is often operationalized in behavior. For example, as I understand it liability in tort law generally doesn’t depend on intentions or anything else about mental states.↩︎</p></li>
<li id="fn3"><p>I dislike this term, because I suspect etymologically it’s entangled with anti-Catholicism.↩︎</p></li>
<li id="fn4"><p>I include “offensive” here to leave room for views, such as those of Malcolm X and Franz Fanon, that political violence might be necessary for community self-defense against a violently oppressive state.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-06-07-nazi-problem.html</guid>
  <pubDate>Wed, 07 Jun 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>New preprint: tmfast fits topic models fast</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-05-04-tmfast.html</link>
  <description><![CDATA[ 




<p>Yesterday I published a new preprint on the arXiv: <a href="https://arxiv.org/abs/2305.01535">“<code>tmfast</code> fits topic models fast”</a>. Here’s the abstract:</p>
<blockquote class="blockquote">
<p><code>tmfast</code> is an R package for fitting topic models using a fast algorithm based on partial PCA and the varimax rotation. After providing mathematical background to the method, we present two examples, using a simulated corpus and aggregated works of a selection of authors from the long nineteenth century, and compare the quality of the fitted models to a standard topic modeling package.</p>
</blockquote>
<p>The package itself is available <a href="https://github.com/dhicks/tmfast">on Github</a>.</p>



 ]]></description>
  <guid>https://dhicks.github.io/posts/2023-05-04-tmfast.html</guid>
  <pubDate>Thu, 04 May 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Anarchy and Covid-19</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-04-12-anarchy-covid19.html</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>I started working on this two or three weeks ago, got about 1500 words in, and then had to shelve it to deal with a minor deluge of grading and some other things. I thought I would be able to pick it back up, but I’ve lost the thread. In the “writer’s notebook” spirit of this project, I’m just going to post it.</p>
</div>
</div>
<p>In our Facebook discussion of <a href="2023-04-07-pluralism-problems">my last post</a>, Matt Brown referred to me a more recent paper of his on Feyerabend <span class="citation" data-cites="BrownExpertiseLessonFeyerabend2021">(Brown 2021)</span>. I really like this paper, and want to focus on it instead of the older one.</p>
<p>Brown discusses Feyerabend’s critique of expertise from “How to Defend Society against Science” <span class="citation" data-cites="FeyerabendHowDefendSociety1975">(Feyerabend 1975)</span> and <em>Science in a Free Society</em> [<span class="citation" data-cites="FeyerabendScienceFreeSociety2017">Feyerabend (2017)</span>; originally published 1978 . After tying Feyerabend’s critique to the current literature on science, values, and policy, and noting that Feyerabend had some overly simplistic political philosophy in the ’70s (which don’t really impact the argument Brown is interested in), Brown summarizes the critique of expertise as a sequence of “four increasingly radical claims about science and its place in society that, together, point toward a decentering of experts in society and a rejection of expert authority” (195).</p>
<p>In this post, I want to read these four claims together with <span class="citation" data-cites="LeeViralVisualizationsHow2021">Lee et al. (2021)</span>, a fantastic study of the data analysis practices of Covid-19 skeptics. If you haven’t read this paper before, I can’t recommend it enough. <span class="citation" data-cites="LeeViralVisualizationsHow2021">Lee et al. (2021)</span> challenge the widespread (among natural scientists and public health official) deficit model of public scientific controversies, which blames controversies on public ignorance. Examining social media communications from 2020, <span class="citation" data-cites="LeeViralVisualizationsHow2021">Lee et al. (2021)</span> show that “anti-mask groups on Twitter often create polished counter-visualizations that would not be out of place in scientific papers, health department reports, and publications like the Financial Times” and that “these groups leverage the language of scientific rigor—being critical about data sources, explicitly stating analytical limitations of specific models, and more—in order to support ending public health restrictions despite the consensus of the scientific establishment” <span class="citation" data-cites="LeeViralVisualizationsHow2021">(Lee et al. 2021, 1–2)</span>. I think this is a useful test case for distinguishing my own views from those of both Feyerabend and Brown.</p>
<section id="citizens-can-and-should-evaluate-expert-opinion" class="level1">
<h1>Citizens can and should evaluate expert opinion</h1>
<p>Brown discusses three arguments that Feyerabend gives for this first claim. I agree with this claim, and find the second and third argument most compelling.<sup>1</sup></p>
<p>The second argument is that public<sup>2</sup> evaluation of expert opinion will “contribute to … the development of a mature democratic citizenry” and that “the learning and maturation necessary for citizens to become wise or skillful public participants requires … that they be allowed to try and, perhaps, do poorly” (196). I actually don’t have much to say about this here; I’m noting it because I think it fits well with the broader project on experts as epistemic representatives, where part of the good of representation is actually promoting the democratic capacities of the constituency.</p>
<p>The third argument is that “motivated ‘laymen,’ in fact, can be competent enough to make good decisions regarding scientific information: ‘science is not beyond the natural shrewdness of the human race’” (196, quoting Feyerabend). This is something I emphasize with my students: that members of the general public are capable of getting together, educating themselves (collectively — this will be important later), and developing incisive technical critiques of credentialed expertise. My standard examples are ACT-UP <span class="citation" data-cites="EpsteinImpureScience1996">(Epstein 1996)</span> and environmental justice movements <span class="citation" data-cites="OttingerTechnoscienceEnvironmentalJustice2011">(Ottinger and Cohen 2011)</span>. <span class="citation" data-cites="LeeViralVisualizationsHow2021">Lee et al. (2021)</span> find that members of the Covid-19 skeptic movement “provide[d] numerous tutorials on how to access government health data” and construct visualizations and perform analyses in ways that “encourage[d] … [social media] followers to begin their own data analysis projects” (11).</p>
</section>
<section id="citizens-can-and-should-supervise-science" class="level1">
<h1>Citizens can and should supervise science</h1>
<p>I don’t like the word “supervise” here; Brown discusses an important ambiguity in this term later (201). Brown gives a more precise gloss on the second claim, albeit in a way that won’t work as a section header: “According to Feyerabend, we should elect committees of non-experts to regularly subject scientists and their work to review before it is put to social use” (197).</p>
<p>The primary argument for this second claim comes from a view that Feyerabend shares with Kuhn and Lakatos, that “science contains some necessarily presumptive, dogmatic element that both makes scientific progress possible” but that also obstructs what Feyerabend calls “unchained curiosity” (198). As philosophers of science understand this thought today, scientific practice requires background assumptions, conceptual frameworks, and methodological standards that are often arbitrary, unjustified, left implicit, and/or known to be false. This epistemic infrastructure is all but necessary even to pose research questions and design data collection plans, and so well-functioning scientific communities have norms and enforcement mechanisms (often implicit) that compel their members to accept it. But there is a risk of mismatch, between the epistemic infrastructure developed by a group of experts to address the concerns they find interesting, and the practical needs of policymakers and publics. Feyerabend emphasizes the role of iconoclasts and outsiders in challenging the adequacy of the epistemic infrastructure of the scientific establishment.</p>
<p><span class="citation" data-cites="LeeViralVisualizationsHow2021">Lee et al. (2021)</span> provide an example of this with</p>
<blockquote class="blockquote">
<p>an ongoing animated debate within these groups about which metrics matter. Some users contend that deaths, not cases, should be the ultimate arbiter in policy decisions, since case rates are easily “manipulated” (e.g., with increased testing) and do not necessarily signal severe health problems (people can be asymptomatic). (11)</p>
</blockquote>
<p>One might argue that case rates are an appropriate metric for the health care system, insofar as a rise in case rates is a leading indicator of a rise of hospital admissions a week or two into the future. This assumes that some significant fraction of cases will be sufficiently severe to lead to hospital admissions. But Covid-19 skeptics are skeptical of exactly this fraction. In their eyes, the central question is whether Covid-19 is as severe as the scientific establishment claims. Since death is an unambiguous measure of severity, arguably death rates are a better metric for what they see as the central question. So here we might be able to see a mismatch between the epistemic infrastructure of the experts and that of (the Covid-19 skeptical section of) the public.</p>
<p>In <a href="2023-04-07-pluralism-problems">my last post</a> I made some points about reliability vs.&nbsp;relevance. Even if the methods used by the scientific community are highly reliable (truth-apt, for some non-pragmatic correspondence conception of truth), they’re not necessarily relevant to the epistemic and practical needs of the broader public(s). <span class="citation" data-cites="LeeViralVisualizationsHow2021">Lee et al. (2021)</span> also have a few examples of Covid-19 skeptics challenging the reliability of the scientific establishment. They quote one skeptic:</p>
<blockquote class="blockquote">
<p>You can’t simply subtract the current death tally from the typical value for this time of year and attribute the difference to Covid,” a user wrote. “Because of the actions of our governments, we are actually causing excess deaths. Want to kill an old person quickly? Take away their human interaction and contact.</p>
</blockquote>
<p>This skeptic is arguing that <a href="https://kieranhealy.org/blog/archives/2020/10/01/walk-the-walk/">simple calculations of excess deaths due to the pandemic</a> cannot distinguish deaths due to Covid-19 and deaths due to Covid-19 mitigation policies, and thus these methods are not reliable for estimating deaths due to Covid-19. I suspect almost all public health officials already understood this problem. But I would agree with Feyerabend that it’s appropriate for members of the public to ask public health officials whether and how they’ve taken this confounding into account.</p>
</section>
<section id="science-is-just-another-ideology-or-interest-group" class="level1">
<h1>Science Is Just Another Ideology or Interest Group</h1>
<p>This is where I start to diverge from Feyerabend and the Covid-19 skeptics. <span class="citation" data-cites="LeeViralVisualizationsHow2021">Lee et al. (2021)</span> argue that Covid-19 skeptics “seek to identify bias by being critical about specific profit motives that come from releasing (or suppressing) specific kinds of information,” (12) especially the interests of the pharmaceutical industry (12) and state authorities (11). That is, they see public health officials and the mainstream scientific community as corrupted, pursuing power and wealth rather than truth and health. But this is not a rejection of science as such: Covid-19 skeptics “use ‘data-driven’ narratives to justify their heterodox beliefs” (10), creating data visualizations, appealing to mainstream epistemic values such as data validation, data quality, and the need for control groups, and touting their own formal scientific credentials (10-12). As <span class="citation" data-cites="LeeViralVisualizationsHow2021">Lee et al. (2021)</span> put it, “their approach to the pandemic is grounded in a more scientific rigor, not less,”</p>
<p>Brown’s discussion of Feyerabend’s view is biographical:</p>
<blockquote class="blockquote">
<p>Feyerabend taught at the University of California, Berkeley during the desegregation of public education in the United States and the attendant increase in non-white enrollments at Berkeley …. He came to see his educational role as essentially oppressive, pushing “reflections of the conceit of a small group who had succeeded in enslaving everyone else with their ideas,” and to find the very idea revolting …. The problem was that the ideology of the privileged remained centered. (199)</p>
</blockquote>
<p>And in <a href="2023-04-07-pluralism-problems">my last post</a> I wrote</p>
<blockquote class="blockquote">
<p>In other words, the “we” who find ourselves in a problematic situation can often be diverse in ways that lead “us” to different ways of understanding the nature of the problem. Giving “science” priority or greater standing over other considerations amounts to giving one group or perspective priority over the others.</p>
</blockquote>
<p>I even cited “How to Defend Society against Science” here. So why do I say that I diverge from Feyerabend and the Covid-19 skeptics here?</p>
<p>Feminist scientists and HPSTSers have long held a nuanced attitude towards science, which I some gloss as “<em>exactly</em> one and a half cheers for science.”</p>
<blockquote class="blockquote">
<p>In exposing widespread, largely unremarked androcentric and sexist bias, feminists demonstrate that good science, even our best science, can be deeply structured by the values and interests of its makers …. Feminists engage the sciences not only as critics of bias and partiality but also as practitioners who recognize that systematic empirical inquiry has an indispensable role to play in understanding and changing oppressive conditions …. [T]he goal of understanding and changing conditions of life that disadvantage women requires as much empirical accuracy and explanatory precision as scientific inquiry can afford. <span class="citation" data-cites="WylieComingTermsValues2007">(Wylie and Hankinson Nelson 2007, 59)</span></p>
</blockquote>
<p>On the one hand, science has often been (and continues to be) a powerful tool of oppression, wielded against women and other oppressed groups. On the other hand, science can also be a powerful tool of justice, especially when those same groups find ways to turn the methods of scientific inquiry back on themselves, systematically studying systems of oppression and the role that science has played in them. (This — presented in Deweyan terms — is how I proposed to avoid anything-goes relativism in the previous post.)</p>
<p>Feyerabend was evidently sensitive to the first point here. But it seems to me that he missed the second point, falling into an irremediable cynicism about science and its role(s) in society.</p>
</section>
<section id="science-should-be-separated-from-the-state" class="level1">
<h1>Science Should Be Separated from the State</h1>
<p>Building off the previous point, Feyerabend’s argument here is an analogy with religion. Science, like religion, is just another interest group or ideology; respecting the fact of reasonable pluralism, liberal democracies formally separate religion and state power; and therefore liberal democracies should formally separate science and state power. Science should have no special standing or influence in policymaking, but also should be protected from excessive state interference.</p>
<p>At this point Brown revisits the notion of “supervise” from the second claim, that the public should supervise science. If “supervise” is interpreted as regulatory oversight and the exercise of state authority, then there’s a tension between the second claim and the idea that science, like religion, is a private activity and as such the state shouldn’t interfere with it. But if “supervise” is interpreted as “elect[ing] committees of non-experts to regularly subject scientists and their work to review <em>before it is put to social use</em>” (197, my emphasis) then there’s no tension. The public supervises the moment at which scientific findings are communicated to policymakers as advice. Scientists can do whatever they want on their side of the science-policy wall; it’s passage through the wall that’s subject to formal scrutiny and review.</p>
<p>This picture becomes much more complicated when the scientists in question are government employees, such as public health officials managing the collection, transmission, and publication of pandemic data. But perhaps Feyerabend’s view is that government employees shouldn’t be doing this kind of work. Managing public health is no more the state’s business than managing spiritual health. In this way, Feyerabend’s epistemic anarchism would seem to lead to political anarchism.</p>
<hr>
<p>I teach <span class="citation" data-cites="LeeViralVisualizationsHow2021">Lee et al. (2021)</span> in a data science course open to any graduate student in quantitative social science. A couple of years ago, one of the students noted that Covid-19 skeptics have an “individualistic epistemology.” As <span class="citation" data-cites="LeeViralVisualizationsHow2021">Lee et al. (2021)</span> put it, “anti-maskers value unmediated access to information and privilege personal research and direct reading over ‘expert’ interpretations.” I would suggest that this reflects an ideal from (right-) libertarian political philosophy, of self-reliance and independence (non-dependence) on others.</p>
<p>But this ideal is incompatible with the way the community of Covid-19 skeptics actually operates. Community members teach each other how to find and work with data, reinforce perceptions of state and scientific corruption, develop shared interpretations of their custom visualizations, and promote counterexperts (credentialed or not) with favorable views.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-BrownExpertiseLessonFeyerabend2021" class="csl-entry">
Brown, Matthew J. 2021. <span>“Against Expertise: A Lesson from Feyerabend’s Science in a Free Society?”</span> In <em>Interpreting Feyerabend: Critical Essays</em>, edited by Jamie Shaw and Karim Bschir, 191–212. Cambridge: Cambridge University Press. <a href="https://doi.org/10.1017/9781108575102.011">https://doi.org/10.1017/9781108575102.011</a>.
</div>
<div id="ref-EpsteinImpureScience1996" class="csl-entry">
Epstein, Steven. 1996. <em>Impure Science</em>. AIDS, Activism, and the Politics of Knowledge. Berkeley, Los Angeles, and Oxford: University of California Press. <a href="http://books.google.ca/books?id=kZOso0FMsrMC&amp;pg=PA39&amp;dq=intitle:Impure+Science+inauthor:epstein&amp;hl=&amp;cd=1&amp;source=gbs_api">http://books.google.ca/books?id=kZOso0FMsrMC&amp;pg=PA39&amp;dq=intitle:Impure+Science+inauthor:epstein&amp;hl=&amp;cd=1&amp;source=gbs_api</a>.
</div>
<div id="ref-FeyerabendHowDefendSociety1975" class="csl-entry">
Feyerabend, Paul. 1975. <span>“How to Defend Society Against Science.”</span> <em>Radical Philosophy</em>, no. 011 (Summer): 3–8.
</div>
<div id="ref-FeyerabendScienceFreeSociety2017" class="csl-entry">
———. 2017. <em>Science in a Free Society</em>. Verso Books.
</div>
<div id="ref-LeeViralVisualizationsHow2021" class="csl-entry">
Lee, Crystal, Tanya Yang, Gabrielle Inchoco, Graham M. Jones, and Arvind Satyanarayan. 2021. <span>“Viral Visualizations: How Coronavirus Skeptics Use Orthodox Data Practices to Promote Unorthodox Science Online.”</span> <em>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May, 1–18. <a href="https://doi.org/10.1145/3411764.3445211">https://doi.org/10.1145/3411764.3445211</a>.
</div>
<div id="ref-OttingerTechnoscienceEnvironmentalJustice2011" class="csl-entry">
Ottinger, Gwen, and Benjamin R. Cohen, eds. 2011. <em>Technoscience and Environmental Justice: Expert Cultures in a Grassroots Movement</em>. Urban and Industrial Environments. Cambridge, Mass: MIT Press.
</div>
<div id="ref-WylieComingTermsValues2007" class="csl-entry">
Wylie, Alison, and Lynn Hankinson Nelson. 2007. <span>“Coming to Terms with the Values of Science: Insights from Feminist Science Studies Scholarship.”</span> In <em>Value-Free Science?</em>, edited by Harold Kincaid, John Dupré, and Alison Wylie, 58–86. Oxford University Press. <a href="http://books.google.com/books?id=9Nqppv6uq7oC">http://books.google.com/books?id=9Nqppv6uq7oC</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The first is based on Mill’s argument in chapter 2 of <em>On Liberty</em>. My thoughts about this argument are complex and not what I’m interested in here.↩︎</p></li>
<li id="fn2"><p>Brown uses “citizen” 45 times in the paper, compared to 28 uses of “public.” I’m guessing this is following Feyerabend. But because of my location in California’s Central Valley, with its large population of migrant and often undocumented farmworkers, I prefer the broader term.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-04-12-anarchy-covid19.html</guid>
  <pubDate>Thu, 04 May 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Pluralism about problematic situations</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-04-07-pluralism-problems.html</link>
  <description><![CDATA[ 




<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Matt Brown has some commentary <a href="https://www.facebook.com/hicks.daniel.j/posts/pfbid0Z31gfZEy2EgeYCGyBdD3XXfLVUQHJXW2gjjE1jwFQWQmuWQSCmRTtMon6PRDx4Q8l?comment_id=1392646594824355&amp;notif_id=1680900361004955&amp;notif_t=feed_comment&amp;ref=notif">here</a>.</p>
</div>
</div>
<p>I’ve been invited to give <a href="https://values.utdallas.edu/2022/12/20/2023-conference/">a keynote</a> on Matt Brown’s work at the UT Dallas conference in May. <span class="citation" data-cites="BrownDemocraticControlScientific2013">Brown (2013)</span> fits with my current project (as seen in the recent blog posts), so this post is maybe the first of a sub-series of posts on his paper. All headless citations are to <span class="citation" data-cites="BrownDemocraticControlScientific2013">Brown (2013)</span>.</p>
<p>Brown first argues that there is a tension between the autonomy (lack of accountability) of science and its social and political authority in a democratic society. He quotes Douglas:</p>
<blockquote class="blockquote">
<p>[A]n autonomous and authoritative science is intolerable …. A fully autonomous and authoritative science is too powerful, with no attendant responsibility <span class="citation" data-cites="DouglasSciencePolicyValuefree2009">(Douglas 2009, 7–8, as quoted by Brown, 481)</span></p>
</blockquote>
<p>Scholars and activists who have been sensitive to this tension — he names Feyerabend and STS scholars; we could include many feminists, anti-racists, and Marxists — “have tended to challenge the … authority of science …. weakening or denying the existence of expertise in politics” (481). This is objectionable because it amounts to “giv[ing] up tools in policy-making that we cannot do without, … given the remarkable success of science” (481-2).</p>
<p>Here, and elsewhere in the paper, Brown links the epistemic credibility or <em>reliability</em> of science to its political authority. Science should have authority in policymaking because it is an especially (the most?) reliable way of producing knowledge. But the kind or degree of political authority that Brown has in mind is vague. It’s one thing to say that scientific findings or methods can be useful in many policymaking situations, and so typically there should be a deliberate effort to solicit advice from scientists; it’s another to say that science should have some kind of priority or greater standing over other considerations.</p>
<p>I argue that Brown’s own account of inquiry should lead us towards the former formulation, of science as frequently useful rather than science taking priority. On Brown’s account, inquiry (scientific or otherwise) is a systematic response to a <em>problematic situation</em>, a situation in which we (we’ll come back to this “we”) realize that our established habits fail to realize their aims. Inquiry is considered <em>successful</em> insofar as it establishes a better fit between habits and aims; this might involve developing completely new habits, tweaking old habits, and/or reconstituting the aims.</p>
<p>A number of different kinds of controversies can emerge from this simple framework. In <strong>aim controversies</strong>, parties disagree over the aims that our habits are supposed to be realizing. <strong>Diagnostic controversies</strong> concern the explanation for why the habits are failing to realize their aims. We can even have <strong>problem controversies</strong>, in which parties disagree over whether we are in a problematic situation at all. Brown gives the example of US policy debates surrounding health care reform in 2009-2010 (483-4), which featured each of these kinds of controversies.</p>
<p>When parties disagree with each other in these kinds of ways, they will disagree with each other about whether a line of inquiry should be considered <strong>relevant</strong> to a policy problem, and thus whether it should be counted as successful with respect to the problematic situation at hand. For example, consider an economic analysis that finds that a combination of employer and individual health insurance mandates and a means-tested subsidy for private insurance will tend to increase health insurance coverage. This line of inquiry might be considered successful by someone whose diagnosis of the problem with the US health care system is that private insurance is too expensive, but not by people who think the problem is a profit-driven healthcare industry or creeping government overreach. For the latter two groups of people, even if the economic analysis is highly reliable it is not relevant to developing a solution to the problem <span class="citation" data-cites="CartwrightEvidenceBasedPolicyPractical2012 HicksEpistemologicalDepthGM2015">(Cartwright and Hardie 2012; Hicks 2015)</span>.</p>
<p>In other words, the “we” who find ourselves in a problematic situation can often be diverse in ways that lead “us” to different ways of understanding the nature of the problem. Giving “science” priority or greater standing over other considerations amounts to giving one group or perspective priority over the others <span class="citation" data-cites="FeyerabendHowDefendSociety1975 LonginoCanThereBe1987 LaceyScientificResearchTechnological2014 HicksGeneticallyModifiedCrops2017">(Feyerabend 1975; Longino 1987; Lacey 2014; Hicks 2017)</span>.</p>
<p>This analysis might seem to threaten to degenerate into anything-goes relativism, with partisans cherry-picking the science that fits their preferred account of the problem <span class="citation" data-cites="SarewitzHowScienceMakes2004">(Sarewitz 2004)</span>. But I think we can avoid relativism by viewing a policy controversy as itself a problematic situation prompting inquiry. In other words, we can use scientific inquiry as a way to understand and thereby navigate political conflict, not as a cudgel that one side gets to use against the others.</p>
<p>Consider climate change. Climate scientists have raised concerns in public about the impacts of carbon dioxide emissions for over 35 years, but policy responses have been faltering at best and public attitudes seem deeply divided. Why is this, and what could be done to get a better match between the apparent problem and a policy solution? Scholars in several fields of the social sciences and humanities have tackled this question for at least 20 years, working to understand the roles of fossil fuel companies <span class="citation" data-cites="OreskesMerchantsDoubtHow2011 SupranAssessingExxonMobilClimate2017">(Oreskes and Conway 2011; Supran and Oreskes 2017)</span>; uncertainties in climate science, both real and manufactured <span class="citation" data-cites="LloydModelRobustnessConfirmatory2015">(Lloyd 2015)</span>; public perceptions of and attitudes towards climate change <span class="citation" data-cites="LewandowskyAliceWonderlandMechanics2016 LeiserowitzGlobalWarmingSix2021">(Lewandowsky, Cook, and Lloyd 2016; Leiserowitz et al. 2021)</span>; and how well political representatives understand the views and preferences of their constituents <span class="citation" data-cites="BolsenCitizensScientistsPolicy2015 Hertel-FernandezLegislativeStaffRepresentation2019">(Bolsen, Druckman, and Cook 2015; Hertel-Fernandez, Mildenberger, and Stokes 2019)</span>. Taken together, this body of work supports a model on which there is strong public support for climate policy, even among Republican/conservative voters, but that the fossil fuel industry’s propaganda efforts have succeeded in creating the misleading appearance of scientific and public controversy, what <span class="citation" data-cites="SparkmanAmericansExperienceFalse2022">Sparkman, Geiger, and Weber (2022)</span> call a “false social reality: a near universal perception of public opinion that is the opposite of true public sentiment.”</p>
<p>Much like using physical climate models to model future flood risks <span class="citation" data-cites="ParkerIncorporatingUserValues2019 LuskPoliticalLegitimacyDemocratic2020">(Parker and Lusk 2019; Lusk 2020)</span>, this body of social scientific-humanistic climate research is useful without claiming priority over other concerns. By understanding key actors and constituencies in the climate controversy, policymakers can design policies that are more likely to gather the support of broad coalitions. For example, a distinct feature of Ocasio-Cortez and Markey’s 2019 Green New Deal resolution was its call for a “just transition,” enabling fossil fuel workers to gradually move into other industries or take early retirement. These ideas played a significant role in the 2021 American Jobs Plan <span class="citation" data-cites="SicotteWillGreenNew2021">(Sicotte 2021)</span>, and thereby influenced the Bipartisan Infrastructure Bill.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-BolsenCitizensScientistsPolicy2015" class="csl-entry">
Bolsen, Toby, James N. Druckman, and Fay Lomax Cook. 2015. <span>“Citizens’, Scientists’, and Policy Advisors’ Beliefs about Global Warming.”</span> <em>The ANNALS of the American Academy of Political and Social Science</em> 658 (1): 271–95. <a href="https://doi.org/10.1177/0002716214558393">https://doi.org/10.1177/0002716214558393</a>.
</div>
<div id="ref-BrownDemocraticControlScientific2013" class="csl-entry">
Brown, Matthew J. 2013. <span>“The Democratic Control of the Scientific Control of Politics.”</span> In <em>The European Philosophy of Science Association Proceedings</em>, edited by Vassilios Karakostas and Dennis Dieks, 2:479–91. Springer International Publishing. <a href="http://dx.doi.org/10.1007/978-3-319-01306-0_39">http://dx.doi.org/10.1007/978-3-319-01306-0_39</a>.
</div>
<div id="ref-CartwrightEvidenceBasedPolicyPractical2012" class="csl-entry">
Cartwright, Nancy, and Jeremy Hardie. 2012. <em>Evidence-Based Policy: A Practical Guide to Doing It Better</em>. Oxford University Press. <a href="http://books.google.ca/books?id=kN3OdpkUyTcC&amp;printsec=frontcover&amp;dq=intitle:Evidence+Based+Policy+A+Practical+Guide+to+Doing+It+Better&amp;hl=&amp;cd=1&amp;source=gbs_api">http://books.google.ca/books?id=kN3OdpkUyTcC&amp;printsec=frontcover&amp;dq=intitle:Evidence+Based+Policy+A+Practical+Guide+to+Doing+It+Better&amp;hl=&amp;cd=1&amp;source=gbs_api</a>.
</div>
<div id="ref-DouglasSciencePolicyValuefree2009" class="csl-entry">
Douglas, Heather E. 2009. <em>Science, Policy, and the Value-Free Ideal</em>. Pittsburgh, Pa: University of Pittsburgh Press.
</div>
<div id="ref-FeyerabendHowDefendSociety1975" class="csl-entry">
Feyerabend, Paul. 1975. <span>“How to Defend Society Against Science.”</span> <em>Radical Philosophy</em>, no. 011 (Summer): 3–8.
</div>
<div id="ref-Hertel-FernandezLegislativeStaffRepresentation2019" class="csl-entry">
Hertel-Fernandez, Alexander, Matto Mildenberger, and Leah C. Stokes. 2019. <span>“Legislative Staff and Representation in Congress.”</span> <em>American Political Science Review</em> 113 (1): 1–18. <a href="https://doi.org/10.1017/S0003055418000606">https://doi.org/10.1017/S0003055418000606</a>.
</div>
<div id="ref-HicksEpistemologicalDepthGM2015" class="csl-entry">
Hicks, Daniel J. 2015. <span>“Epistemological Depth in a GM Crops Controversy.”</span> <em>Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences</em> 50 (April): 1–12. <a href="https://doi.org/10.1016/j.shpsc.2015.02.002">https://doi.org/10.1016/j.shpsc.2015.02.002</a>.
</div>
<div id="ref-HicksGeneticallyModifiedCrops2017" class="csl-entry">
———. 2017. <span>“Genetically Modified Crops, Inclusion, and Democracy.”</span> <em>Perspectives on Science</em> 25 (4): 488–520. <a href="https://doi.org/10.1162/POSC_a_00251">https://doi.org/10.1162/POSC_a_00251</a>.
</div>
<div id="ref-LaceyScientificResearchTechnological2014" class="csl-entry">
Lacey, Hugh. 2014. <span>“Scientific Research, Technological Innovation and the Agenda of Social Justice, Democratic Participation and Sustainability.”</span> <em>Scientiae Studia</em> 12 (SPE): 37–55. <a href="https://doi.org/10.1590/S1678-31662014000400003">https://doi.org/10.1590/S1678-31662014000400003</a>.
</div>
<div id="ref-LeiserowitzGlobalWarmingSix2021" class="csl-entry">
Leiserowitz, Anthony, Connie Roser-Renouf, Jennifer Marlon, and Edward Maibach. 2021. <span>“Global Warming’s Six Americas: A Review and Recommendations for Climate Change Communication.”</span> <em>Current Opinion in Behavioral Sciences</em>, Human Response to Climate Change: From Neurons to Collective Action, 42 (December): 97–103. <a href="https://doi.org/10.1016/j.cobeha.2021.04.007">https://doi.org/10.1016/j.cobeha.2021.04.007</a>.
</div>
<div id="ref-LewandowskyAliceWonderlandMechanics2016" class="csl-entry">
Lewandowsky, Stephan, John Cook, and Elisabeth Lloyd. 2016. <span>“The <span>‘Alice in Wonderland’</span> Mechanics of the Rejection of (Climate) Science: Simulating Coherence by Conspiracism.”</span> <em>Synthese</em>, September, 1–22. <a href="https://doi.org/10.1007/s11229-016-1198-6">https://doi.org/10.1007/s11229-016-1198-6</a>.
</div>
<div id="ref-LloydModelRobustnessConfirmatory2015" class="csl-entry">
Lloyd, Elisabeth A. 2015. <span>“Model Robustness as a Confirmatory Virtue: The Case of Climate Science.”</span> <em>Studies in History and Philosophy of Science Part A</em> 49 (February): 58–68. <a href="https://doi.org/10.1016/j.shpsa.2014.12.002">https://doi.org/10.1016/j.shpsa.2014.12.002</a>.
</div>
<div id="ref-LonginoCanThereBe1987" class="csl-entry">
Longino, Helen E. 1987. <span>“Can There Be A Feminist Science?”</span> <em>Hypatia</em> 2 (3): 51–64. <a href="https://doi.org/10.1111/j.1527-2001.1987.tb01341.x">https://doi.org/10.1111/j.1527-2001.1987.tb01341.x</a>.
</div>
<div id="ref-LuskPoliticalLegitimacyDemocratic2020" class="csl-entry">
Lusk, Greg. 2020. <span>“Political Legitimacy in the Democratic View: The Case of Climate Services.”</span> <em>Philosophy of Science</em>, July. <a href="https://doi.org/10.1086/710803">https://doi.org/10.1086/710803</a>.
</div>
<div id="ref-OreskesMerchantsDoubtHow2011" class="csl-entry">
Oreskes, Naomi, and Erik M. Conway. 2011. <em>Merchants of Doubt: How a Handful of Scientists Obscured the Truth on Issues from Tobacco Smoke to Global Warming</em>. New York, NY, USA: Bloomsbury.
</div>
<div id="ref-ParkerIncorporatingUserValues2019" class="csl-entry">
Parker, Wendy S., and Greg Lusk. 2019. <span>“Incorporating User Values into Climate Services.”</span> <em>Bulletin of the American Meteorological Society</em> 100 (9): 1643–50. <a href="https://doi.org/10.1175/BAMS-D-17-0325.1">https://doi.org/10.1175/BAMS-D-17-0325.1</a>.
</div>
<div id="ref-SarewitzHowScienceMakes2004" class="csl-entry">
Sarewitz, Daniel. 2004. <span>“How Science Makes Environmental Controversies Worse.”</span> <em>Environmental Science and Policy</em> 7 (5): 385–403. <a href="https://doi.org/10.1016/j.envsci.2004.06.001">https://doi.org/10.1016/j.envsci.2004.06.001</a>.
</div>
<div id="ref-SicotteWillGreenNew2021" class="csl-entry">
Sicotte, Diane. 2021. <span>“Will the Green New Deal Bring About a <span>‘Just Transition,’</span> or Just Transition? - American Sociological Association.”</span> Summer 2021. <a href="https://www.asanet.org/footnotes-article/will-green-new-deal-bring-about-just-transition-or-just-transition/">https://www.asanet.org/footnotes-article/will-green-new-deal-bring-about-just-transition-or-just-transition/</a>.
</div>
<div id="ref-SparkmanAmericansExperienceFalse2022" class="csl-entry">
Sparkman, Gregg, Nathan Geiger, and Elke U. Weber. 2022. <span>“Americans Experience a False Social Reality by Underestimating Popular Climate Policy Support by Nearly Half.”</span> <em>Nature Communications</em> 13 (1): 4779. <a href="https://doi.org/10.1038/s41467-022-32412-y">https://doi.org/10.1038/s41467-022-32412-y</a>.
</div>
<div id="ref-SupranAssessingExxonMobilClimate2017" class="csl-entry">
Supran, Geoffrey, and Naomi Oreskes. 2017. <span>“Assessing ExxonMobil’s Climate Change Communications (1977–2014).”</span> <em>Environmental Research Letters</em> 12 (8): 084019. <a href="https://doi.org/10.1088/1748-9326/aa815f">https://doi.org/10.1088/1748-9326/aa815f</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-04-07-pluralism-problems.html</guid>
  <pubDate>Fri, 07 Apr 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>The Nature endorsement and the value-free ideal</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-03-22-endorsement.html</link>
  <description><![CDATA[ 




<p>In October 2020, <em>Nature</em> published an editorial endorsing Joe Biden and criticizing Trump <span class="citation" data-cites="WhyNatureSupports2020">(<span>“Why Nature Supports Joe Biden for US President”</span> 2020)</span>. A few days ago, <em>Nature Human Behavior</em> published an online survey experimental study <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">(Zhang 2023)</span> that appeared to find that “some who saw the endorsement lost confidence in the journal as a result” <span class="citation" data-cites="LupiaPoliticalEndorsementsCan2023">(Lupia 2023)</span>. The study has prompted a lot of discussion on social media, especially after the editors of <em>Nature</em> stood by their decision to endorse Biden <span class="citation" data-cites="ShouldNatureEndorse2023">(<span>“Should Nature Endorse Political Candidates? Yes — When the Occasion Demands It”</span> 2023)</span> and Holden Thorp, editor-in-chief of <em>Science</em>, posted <a href="https://twitter.com/hholdenthorp/status/1638203875160104961">a short Twitter thread</a> in agreement.</p>
<p>In many of these discussions, <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> is being interpreted in terms of <a href="https://twitter.com/KevinZollman/status/1638542905018306560">the value-free ideal (VFI)</a>. This interpretation is mistaken, because the <em>Nature</em> endorsement doesn’t violate VFI. It does violate a more general principle, but by putting <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> in the context of my own empirical research I argue that violations of this principle don’t explain the headline findings. I develop a better explanation using another branch of my research, developing the “aims approach” to values in science.</p>
<section id="a-little-background-on-me" class="level1">
<h1>A little background on me</h1>
<p>Since I plan to share links to this post in social media threads with strangers, I thought it would be helpful to introduce myself and explain my expertise in this topic. I’m a philosopher of science and STS (science and technology studies) researcher at the University of California, Merced. As a philosopher of science I specialize in the subfield we call “science, values, and policy” or “science and values.” I’ve written <a href="https://orcid.org/0000-0001-7945-4416">a number of papers</a> on the value-free ideal, why almost all contemporary philosophers of science reject it, and the implications this has for policy-based science. My work experience includes two years as a AAAS Science and Technology Policy Fellow, during which I worked at<sup>1</sup> the US Environmental Protection Agency and the National Science Foundation. I also conduct empirical research, and recently published a collaboration with a cognitive psychologist that I’m going to discuss more below <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">(Hicks and Lobato 2022)</span>.</p>
</section>
<section id="the-nature-endorsement-doesnt-violate-vfi" class="level1">
<h1>The <em>Nature</em> endorsement doesn’t violate VFI</h1>
<p>My first point is that neither the <em>Nature</em> endorsement nor the summary seen by the participants in <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> violates VFI. The endorsement’s argument is straightforwardly political: Trump has made bad policy decisions, and Biden will make better ones.</p>
<blockquote class="blockquote">
<p>No US president in recent history has so relentlessly attacked and undermined so many valuable institutions, from science agencies to the media, the courts, the Department of Justice — and even the electoral system. Trump claims to put ‘America First’. But in his response to the pandemic, Trump has put himself first, not America. <span class="citation" data-cites="WhyNatureSupports2020">(<span>“Why Nature Supports Joe Biden for US President”</span> 2020)</span></p>
</blockquote>
<p>Philosophers of science define the value-free ideal strictly, as a norm prohibiting the influence of non-epistemic values<sup>2</sup> in the “core” stages of scientific inquiry, especially the evaluation of hypotheses. While the <em>Nature</em> endorsement does make some claims that could be evaluated empirically as hypotheses, it’s not doing this empirical evaluation itself. Instead these claims are presented as premises in its argument that Trump has made bad policy decisions. So, there’s no violation of VFI here.</p>
<p><span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> provided the participants (specifically, the ones in the intervention arm of the study) with this summary of the endorsement:</p>
<blockquote class="blockquote">
<p><strong>Scientific journal <em>Nature</em> endorsed Joe Biden</strong></p>
<p>Weeks before the 2020 presidential election, <strong><em>Nature</em></strong>’s editorial board official endorsed Joe Biden, citing:</p>
<ul>
<li>Donald Trump’s pandemic response had been “disastrous”;</li>
<li>Biden, unlike Trump, would listen to science, and thus;</li>
<li>Biden would handle the COVID-19 pandemic better.</li>
</ul>
<p><strong><em>Nature</em></strong> is one of the most-cited and most prestigious peer-reviewed scientific journals in the world.</p>
</blockquote>
<p><span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> also included a screenshot of the endorsement headline and a link to the story. This summary also does not include a VFI violation.</p>
</section>
<section id="the-nature-endorsement-does-violate-value-neutrality-if-it-applies" class="level1">
<h1>The <em>Nature</em> endorsement does violate value neutrality (if it applies)</h1>
<p>The <em>Nature</em> endorsement doesn’t violate VFI, but it does seem to violate a more general norm, <strong>value neutrality</strong>. Havstad and Brown give a nice definition in their analysis of the IPCC’s ambition to be “policy-relevant but policy-neutral”:</p>
<blockquote class="blockquote">
<p>When making value judgments, parties who are objective in the sense of value-neutrality remain as neutral as possible where values are controversial, either by avoiding value judgments in that specific instance or by seeking balanced or conciliatory positions within the range of values. <span class="citation" data-cites="HavstadNeutralityRelevancePrescription2017 DouglasIrreducibleComplexityObjectivity2004">(Havstad and Brown 2017; see also H. Douglas 2004, 460)</span></p>
</blockquote>
<p>Endorsing a candidate in a contested election is pretty much certain to involve controversial values.<sup>3</sup> So the endorsement obviously violated this neutrality norm.</p>
<p>However, we might question whether value neutrality applies to the <em>Nature</em> editors <em>in their role as editors</em>. I take it that many people think value neutrality applies to journalists in their role as journalists, but also that <a href="https://www.washingtonpost.com/opinions/2020/09/28/editorial-board-endorsement-joe-biden/">newspaper endorsements of candidates</a> are not regarded objectionable. This would mean that the value neutrality norm applies to journalists as journalists, but not as editors (perhaps with restrictions, for example, to clearly-labelled opinion pieces).</p>
<p>Since <em>Nature</em> is a science journal, its editors are likely to be thought of as scientists (I’m too lazy to check to what extent its editors do in fact have scientific backgrounds, and in any case I expect editing the journal is sufficiently demanding that they’re not doing much actual research), and I take it most people think value neutrality applies to scientists. Still, for the endorsement to violate value neutrality, the scope of the norm would have to be broader for scientists than for journalists.</p>
<p>Value neutrality might be confused for VFI because the former entails the latter, at least so long as we assume (<a href="https://dhicks.github.io/posts/2023-01-30-democracy-defense.html">incorrectly</a>) “that epistemic values are politically neutral while non-epistemic values are politically controversial” and that value neutrality applies to scientists in the “core” stages of inquiry.</p>
<p><em>Should</em> value neutrality apply to scientists in their roles as journal editors writing a clearly-labelled opinion piece? Given that this norm doesn’t apply to newspaper editors, I can’t think of a compelling reason to apply it to scientific journal editors. There are also the critiques of neutrality given by <span class="citation" data-cites="HavstadNeutralityRelevancePrescription2017">Havstad and Brown (2017)</span>; and because value neutrality (plus assumptions) entails VFI, the many compelling arguments against VFI also apply to value neutrality (unless one tries to preserve it by rejecting some of the assumptions instead).<sup>4</sup> Though perhaps the results of <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> give us a consequentialist argument for applying the norm to scientists in their roles as journal editors: violating value neutrality undermines public trust in science.</p>
</section>
<section id="zhangpoliticalendorsementnature2023-findings-and-a-concern-about-generalizability" class="level1">
<h1><span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span>: Findings and a concern about generalizability</h1>
<p>So let’s turn to <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span>. The headline finding — as communicated with some really bad data visualization — was that Trump voters who saw the <em>Nature</em> endorsement summary in the experiment rated <em>Nature</em> editors as much less informed and much more biased, compared to Trump voters in the control condition. Treating the five-point Likert scale as a continuous one, the treatment effect was about -0.9 for informed and 0.6 for biased; that is, about a full point drop for informed and more than half a point for biased. Trump voters who saw the endorsement were also substantially less likely to get covid-19 information from <em>Nature</em>, again compared to Trump voters in the control condition.<sup>5</sup> Importantly, these effects were only observed for Trump voters. <strong>There was no sign of similar effects for Biden voters.</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://dhicks.github.io/posts/https:/media.nature.com/lw767/magazine-assets/d41586-023-00799-3/d41586-023-00799-3_24644262.png?as=webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The headline finding of <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span>: Trump voters who saw the <em>Nature</em> endorsement summary in the experiment rated <em>Nature</em> editors as much less informed and much more biased. I can’t get over how awful this visualization is. Source: <span class="citation" data-cites="LupiaPoliticalEndorsementsCan2023">Lupia (2023)</span></figcaption>
</figure>
</div>
<p>Much of the commentary around <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> is generalizing this headline finding, to a claim that the endorsement undermined trust in science generally. <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> did examine this generalization, and did find statistically significant results for Trump voters. However, these effects were much smaller in magnitude, about 0.15 on the five-point Likert scale. This means that a few Trump voters saw science generally as a little less trustworthy after seeing the endorsement, but overall this effect is negligible. An old complaint in statistics is that “practical significance is different from statistical significance.”</p>
<p><span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> would still be worrisome, insofar as one is worried that violations of neutrality could undermine trust in individual or well-defined groups of scientists (eg, the IPCC). But I don’t think we need to worry about the <em>Nature</em> endorsement undermining trust in science in general.</p>
</section>
<section id="the-zhangpoliticalendorsementnature2023-results-arent-explained-by-vfi-or-neutrality-violations" class="level1">
<h1>The <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> results aren’t explained by VFI or neutrality violations</h1>
<p>My overall claim in this post is that <strong>violations of neither VFI nor neutrality provide a good explanation of the findings of <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span></strong>. The first argument against VFI is that, as I discussed above, the endorsement doesn’t violate VFI. It’s hard to use a norm violation to explain these findings if the norm wasn’t violation. We could make a similar argument against a neutrality-based explanation if it turned out that most people (or Trump voters specifically) don’t think this norm applies to science journal editors when writing clearly-labelled opinion pieces.</p>
<p>I doubt that will be a compelling argument to many readers, though. My second argument starts by noting that there was no effect for Biden voters. If VFI and/or value neutrality violations explained these findings, we would expect to see effects for both Trump and Biden voters. But we don’t, so they don’t.</p>
<p>One potential response to this argument would be to posit an interaction model, on which the norm is evaluated differently by Trump vs.&nbsp;Biden voters. <a href="https://twitter.com/KevinZollman/status/1638542905018306560">Matt Weiner and Kevin Zollman</a> gave versions of this response on Twitter, in a thread started by João Ohara. In particular, my reading of Zollman’s version is that <em>VFI applies if and only if the scientist does not share one’s values</em>. So, for Trump voters, the <em>Nature</em> editors don’t share the participant’s values, so VFI applies, and so the <em>Nature</em> editors violated VFI. On the other hand, for Biden voters, the <em>Nature</em> editors <em>do</em> share the participant’s values, so VFI does <em>not</em> apply, and so the <em>Nature</em> editors <em>did not</em> violate VFI. Call this the <strong>shared values VFI model</strong>. (For the reasons given above, we should probably focus on a neutrality version of this explanation, rather than VFI. However, for simplicity, throughout the rest of the post I’m often going to use VFI as an umbrella term that includes neutrality.)</p>
<p>It would be difficult to study empirically whether and how VFI was playing a role in participants’ reactions to the endorsement. It wouldn’t be enough to simply ask participants whether they accept VFI (written in a way that a general audience can understand) and then run the same experiment as <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span>, because that wouldn’t get at the question of whether and how the participants are reasoning with VFI when they encounter the endorsement. There would also be a serious risk of priming effects, whichever order the questions were presented: seeing VFI first might make participants more likely to use it to react to the endorsement, while seeing the endorsement first might change their acceptance of VFI insofar as they see it as supporting or challenging their reaction to the endorsement. Qualitative methods — having participants narrate their reasoning about the endorsement, either out loud or in writing — could actually get at whether and how VFI is playing a role without worrying about priming. Though these methods are extremely labor intensive. None of this is a reason to think the shared values VFI model is false, of course. But it is reason to think this model is not pursuitworthy, which in turn is a reason to not accept it.</p>
</section>
<section id="conflicting-results-from-hicksvaluesdisclosurestrust2022" class="level1">
<h1>Conflicting results from <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span></h1>
<p>Another problem with the shared values VFI model for the findings of <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> is it conflicts with the results of a pair of extremely similar studies, <span class="citation" data-cites="ElliottValuesEnvironmentalResearch2017">Elliott et al. (2017)</span> and <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span>. The latter paper is an independent replication of the former, using the same experimental design but a larger sample (slightly under 500 vs.&nbsp;slightly under 1,000 participants) from a more reliable pool (Amazon Mechanical Turk vs.&nbsp;a representative sample of US adults from the polling firm Prolific).</p>
<p>In both studies, participants were told that bisphenol A (BPA) is a controversial chemical, and shown a slide attributed to a (fictional) scientist, Dr.&nbsp;Riley Spence. The contents of the side differed by condition; here’s one example:</p>
<blockquote class="blockquote">
<p><strong>My conclusion</strong></p>
<ul>
<li><strong><em>Protecting public health</em></strong> should be a top national priority.</li>
<li>I examined the scientific evidence on potential health risks of BPA.</li>
<li>I conclude that BPA in consumer products is <strong><em>causing harm</em></strong> to people.</li>
</ul>
</blockquote>
<p>The first bullet point had three possibilities: either “protecting public health” (in this case), “promoting economic growth,” or this bullet was missing entirely. The third bullet had two possibilities, either “causing harm” (in this case) or “not causing harm.”<sup>6</sup> Participants then filled out the Muenster Epistemic Trustworthiness Inventory <span class="citation" data-cites="HendriksMeasuringLaypeopleTrust2015">(METI; Hendriks, Kienhues, and Bromme 2015)</span> to assess the perceived trustworthiness of Dr.&nbsp;Spence.<sup>7</sup> Participants were also asked for their own views on the tradeoff between public health and economic growth.</p>
<p>Hopefully it’s obvious how similar this study design is to <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span>; they’re so similar I’m a little disappointed that neither paper was cited! Both designs have a clear violation of neutrality. <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> does have a much larger sample size (over 4,000 participants) than either of the Riley Spence design studies. But in every other respect I think the Riley Spence design is better for examining the effects of violations of VFI or neutrality: the value judgments are stated explicitly, rather than implied by partisan alignment; those values have a clear partisan valence but the topic isn’t emotionally heated (this will be important in the next section); and the conclusion is a scientific claim rather than a political one.<sup>8</sup> The one major limitation in the Riley Spence design is that this isn’t an unambiguous violation of VFI; though to me including a value statement on the concluding slide of a presentation does implicate a VFI violation. And of course the prompt in <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> doesn’t include a violation of VFI either.</p>
<p>An additional, important advantage of the Riley Spence design is that it can distinguish shared values from the “first order” value held by the participant (whether the participant prioritizes public health or economic growth). This means this design can test a version of the shared values VFI model. We examine a “shared values” effect: given that the scientist discloses values, then “if the participant and scientist share the same values, the scientist is perceived as more trustworthy than if the participant and scientist do not share the same values” <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">(Hicks and Lobato 2022, 05)</span>. This doesn’t get at <em>why</em> shared values have this different effect — whether the participant is using VFI conditional on shared values or doing something else — but it does allow us to look for a shared values effect independent of which values the participant holds. Because <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> doesn’t have a Republican values arm, it can’t separate these potential effects.</p>
<p>Two findings of <span class="citation" data-cites="ElliottValuesEnvironmentalResearch2017">Elliott et al. (2017)</span> would seem to support <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> and the alternative explanation: they report a loss of perceived trust for a scientist who discloses values, compared to one who does not; and a shared values effect. However, <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span> does not find evidence for either effect, and shows that the shared values finding was an artifact of the unusual analytic approach taken by <span class="citation" data-cites="ElliottValuesEnvironmentalResearch2017 HicksValuesDisclosuresTrust2022">Elliott et al. (2017; Hicks and Lobato 2022, 10)</span>.</p>
<p>Consider first a reduction in trust for violating neutrality and disclosing a value judgment. <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span>’s estimate for this effect (for the “informed” item) is -0.85 on a 5-point scale (95% CI roughly -0.75 to -0.95), which would be about -1.19 (-1.05 to -1.33) if we (crudely) rescaled to the 7-point scales used in both the Riley Spence papers. <span class="citation" data-cites="ElliottValuesEnvironmentalResearch2017">Elliott et al. (2017)</span>’s data gives an estimate of -0.5 (-.3, -0.8), and <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span> estimate (0.0, -0.3). The more careful Riley Spence design yields much smaller effects than the design in <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span>. I’ll come back to the question of how we might explain differences in these estimates in the final section.</p>
<p>Suppose we side with <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> (despite the lower-quality design) and <span class="citation" data-cites="ElliottValuesEnvironmentalResearch2017">Elliott et al. (2017)</span> (despite the smaller and lower-quality sample) over <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span>, and judge that the total evidence favors an decline of trust from violating neutrality. Might an interaction between shared values and VFI be at work here? Again, <span class="citation" data-cites="ElliottValuesEnvironmentalResearch2017">Elliott et al. (2017)</span> report a shared values effect. But <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span> show that, when their data are re-analyzed using a more conventional method, there is no evidence of such an effect. What’s more, the data from both of these papers do support a “scientist values” effect: given that the scientist discloses values (violates neutrality), a scientist who values public health is perceived as more trustworthy than one who values economic growth, <em>even by participants who prioritize economic growth</em>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://dhicks.github.io/posts/https:/www.frontiersin.org/files/Articles/1017362/fcomm-07-1017362-HTML/image_m/fcomm-07-1017362-g006.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">A key finding from <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span>: There is no shared values effect, but instead a “scientist values” effect: a scientist who values public health is perceived as more trustworthy than one who values economic growth, even by participants who value economic growth. Panels correspond to participant values (relatively few participants valued economic growth), and columns within panels correspond to scientist values. In red, the vertical bars indicate the mean and 95% confidence interval, and comparisons between scientist value conditions are connected with a thin line. The “scientist values” effect is shown by the increase in the thin line. This is a much better data visualization: better contrast with the background, showing the individual observations, and highlighting the comparison of interest.</figcaption>
</figure>
</div>
<p>Because the Riley Spence design can separate shared and partisan values, while the design in <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> cannot, I think the Riley Spence design provides much more relevant and reliable evidence regarding a shared values VFI model. And so I think the findings from <span class="citation" data-cites="ElliottValuesEnvironmentalResearch2017">Elliott et al. (2017)</span> and <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span> are a major challenge to that model.</p>
</section>
<section id="the-aims-of-science" class="level1">
<h1>The aims of science</h1>
<p>Based on study quality features, I think <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span> provides the strongest evidence among the three studies I’ve reviewed here; and this study finds evidence against both the “simple” VFI model and the more complex shared values VFI model. Of course I have a conflict of interest, and rather than simply declaring <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span> the “winner” a better approach would be to develop a model that can explain why <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> and <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span> produced different results <span class="citation" data-cites="DouglasWeighingComplexEvidence2012">(Heather Douglas 2012)</span>.</p>
<p>Following the discussion in <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span>, I suggest that the “aims approach” to values in science can provide a model that explains all of the results. On this approach, we start by recognizing that scientific research typically has a mix of epistemic and practical aims. For example, the aim of environmental epidemiology is not just to <em>understand</em> how pollution harms human health, but also to develop <em>policies</em> that reduce pollution and its health impacts. Proponents of this approach argue that the practical aims can play legitimate roles in all stages of scientific inquiry, even when evaluating hypotheses <span class="citation" data-cites="FernandezPintoLegitimizingValuesRegulatory2019 HicksWhenVirtuesAre2022">(Fernández Pinto and Hicks 2019; Hicks 2022)</span>.</p>
<p>In <span class="citation" data-cites="HicksValuesDisclosuresTrust2022">Hicks and Lobato (2022)</span>, we use this approach to explain the scientist values effect. We posit that almost all participants — even ones who prioritize economic growth — recognize protecting public health as one of the aims of environmental public health, while promoting economic growth is <em>not</em> among its aims. So a scientist who prioritizes public health is acting appropriately, while a scientist who prioritizes economic growth is not, regardless of the participant’s own priorities.</p>
<p>In <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span>, I suggest that Trump and Biden voters saw the implied aims of the <em>Nature</em> editors very differently. Again, in the summary of the endorsement, the argument for endorsing Biden was that “Donald Trump’s pandemic response had been ‘disastrous’”; “Biden, unlike Trump, would listen to science”; and “Biden would handle the COVID-19 pandemic better.” I suggest that, for Biden voters, this argument communicated the legitimate scientific aim of informing public policy to promote public health. On the other hand, I suggest that many Trump voters would read this argument as scientists trying to shift blame from scientific authorities (CDC, NIH) to Trump, calling for even more power and influence than they already have, and trying to justify policies that these voters saw as creeping authoritarianism (stay-at-home orders, restrictions on restaurants and other businesses, mask and vaccination requirements). None of these are legitimate scientific aims.</p>
<p>On this model, shared values don’t play a role in any of these studies. The participants’ values (more properly, the participants’ epistemic community and their interpretive frames) can play a role, but early on, when the participant reads the stimulus. But then it’s the scientist’s values that matter, and whether those values align or conflict with the aims of science, not whether the align or conflict with the participants’ values.</p>
<p>And neither neutrality nor VFI play a role in this model. The aims approach has been developed as an alternative to VFI (and neutrality), to explain how scientists legitimately use non-epistemic values throughout their research.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-DouglasIrreducibleComplexityObjectivity2004" class="csl-entry">
Douglas, H. 2004. <span>“The Irreducible Complexity of Objectivity.”</span> <em>Synthese</em> 138 (3): 453–73. <a href="https://doi.org/10.1023/B:SYNT.0000016451.18182.91">https://doi.org/10.1023/B:SYNT.0000016451.18182.91</a>.
</div>
<div id="ref-DouglasWeighingComplexEvidence2012" class="csl-entry">
Douglas, Heather. 2012. <span>“Weighing Complex Evidence in a Democratic Society.”</span> <em>Kennedy Institute of Ethics Journal</em> 22 (2).
</div>
<div id="ref-ElliottValuesEnvironmentalResearch2017" class="csl-entry">
Elliott, Kevin C., Aaron M. McCright, Summer Allen, and Thomas Dietz. 2017. <span>“Values in Environmental Research: Citizens’ Views of Scientists Who Acknowledge Values.”</span> <em>PLOS ONE</em> 12 (10): e0186049. <a href="https://doi.org/10.1371/journal.pone.0186049">https://doi.org/10.1371/journal.pone.0186049</a>.
</div>
<div id="ref-FernandezPintoLegitimizingValuesRegulatory2019" class="csl-entry">
Fernández Pinto, Manuela, and Daniel J. Hicks. 2019. <span>“Legitimizing Values in Regulatory Science.”</span> <em>Environmental Health Perspectives</em> 127 (3): 035001. <a href="https://doi.org/10.1289/EHP3317">https://doi.org/10.1289/EHP3317</a>.
</div>
<div id="ref-HavstadNeutralityRelevancePrescription2017" class="csl-entry">
Havstad, Joyce C., and Matthew J. Brown. 2017. <span>“Neutrality, Relevance, Prescription, and the IPCC.”</span> <em>Public Affairs Quarterly</em> 31 (4): 303–24. <a href="https://www.jstor.org/stable/44732800">https://www.jstor.org/stable/44732800</a>.
</div>
<div id="ref-HendriksMeasuringLaypeopleTrust2015" class="csl-entry">
Hendriks, Friederike, Dorothe Kienhues, and Rainer Bromme. 2015. <span>“Measuring Laypeople’s Trust in Experts in a Digital Age: The Muenster Epistemic Trustworthiness Inventory (METI).”</span> <em>PLOS ONE</em> 10 (10): e0139309. <a href="https://doi.org/10.1371/journal.pone.0139309">https://doi.org/10.1371/journal.pone.0139309</a>.
</div>
<div id="ref-HicksWhenVirtuesAre2022" class="csl-entry">
Hicks, Daniel J. 2022. <span>“When Virtues are Vices: <span>‘Anti-Science’</span> Epistemic Values in Environmental Politics.”</span> <em>Philosophy, Theory, and Practice in Biology</em> 14 (0). <a href="https://doi.org/10.3998/.2629">https://doi.org/10.3998/.2629</a>.
</div>
<div id="ref-HicksValuesDisclosuresTrust2022" class="csl-entry">
Hicks, Daniel J., and Emilio Jon Christopher Lobato. 2022. <span>“Values Disclosures and Trust in Science: A Replication Study.”</span> <em>Frontiers in Communication</em> 7. <a href="https://www.frontiersin.org/articles/10.3389/fcomm.2022.1017362">https://www.frontiersin.org/articles/10.3389/fcomm.2022.1017362</a>.
</div>
<div id="ref-LupiaPoliticalEndorsementsCan2023" class="csl-entry">
Lupia, Arthur. 2023. <span>“Political Endorsements Can Affect Scientific Credibility.”</span> <em>Nature</em> 615 (7953): 590–91. <a href="https://doi.org/10.1038/d41586-023-00799-3">https://doi.org/10.1038/d41586-023-00799-3</a>.
</div>
<div id="ref-SchroederDemocraticValuesBetter" class="csl-entry">
Schroeder, S. Andrew. n.d. <span>“Democratic Values: A Better Foundation for Public Trust in Science.”</span> <em>The British Journal for the Philosophy of Science</em>. Accessed June 29, 2019. <a href="https://doi.org/10.1093/bjps/axz023">https://doi.org/10.1093/bjps/axz023</a>.
</div>
<div id="ref-ShouldNatureEndorse2023" class="csl-entry">
<span>“Should Nature Endorse Political Candidates? Yes — When the Occasion Demands It.”</span> 2023. <em>Nature</em> 615 (7953): 561–61. <a href="https://doi.org/10.1038/d41586-023-00789-5">https://doi.org/10.1038/d41586-023-00789-5</a>.
</div>
<div id="ref-WhyNatureSupports2020" class="csl-entry">
<span>“Why Nature Supports Joe Biden for US President.”</span> 2020. <em>Nature</em> 586 (7829): 335–35. <a href="https://doi.org/10.1038/d41586-020-02852-x">https://doi.org/10.1038/d41586-020-02852-x</a>.
</div>
<div id="ref-ZhangPoliticalEndorsementNature2023" class="csl-entry">
Zhang, Floyd Jiuyun. 2023. <span>“Political Endorsement by Nature and Trust in Scientific Expertise During COVID-19.”</span> <em>Nature Human Behaviour</em>, March, 1–11. <a href="https://doi.org/10.1038/s41562-023-01537-5">https://doi.org/10.1038/s41562-023-01537-5</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Technically “at,” not “for.”↩︎</p></li>
<li id="fn2"><p>It’s common in the literature to use “non-epistemic values” interchangeably with something like “social and political values.” But I think this is incorrect, because — on my preferred definition of epistemic values, namely, features of scientific practice that promote the pursuit of epistemic aims such as truth and understanding — social and political values can sometimes be epistemic <span class="citation" data-cites="HicksWhenVirtuesAre2022">(Hicks 2022, esp.&nbsp;pages 1-2)</span>.↩︎</p></li>
<li id="fn3"><p>This is actually tricky in the <em>Nature</em> endorsement, because their value judgments are bundled into thick claims. For example, one of their premises is that Trump’s management of the pandemic was “disastrous.” This is a <em>thick concept</em>, with both descriptive criteria for application and evaluative implications: Trump’s handling of the pandemic resulted in hundreds of thousands of preventable deaths, and this is a severely bad thing. (Note that “preventable death” is another thick concept.) The controversy here isn’t over the evaluative side, the idea that preventable deaths are bad. Instead it’s over the descriptive side: that Trump’s handling of the pandemic had these effects.↩︎</p></li>
<li id="fn4"><p>Specifically, someone might try to concede VFI and preserve value neutrality by rejecting the alignment between epistemic/non-epistemic and neutral/controversial values. <span class="citation" data-cites="SchroederDemocraticValuesBetter">Schroeder (n.d.)</span> might potentially be read in this way: democratically-endorsed non-epistemic values are treated as not controversial, and so appealing to them does not violate neutrality.↩︎</p></li>
<li id="fn5"><p>I’m confused by the scale used in reporting this effect, so I’m just calling it “substantial” here.↩︎</p></li>
<li id="fn6"><p>Varying these independently means that we can distinguish any effects related to the value judgment from effects of the conclusion. The design in <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> can’t do this, because in a sense the value judgment <em>is</em> the conclusion.↩︎</p></li>
<li id="fn7"><p>One incidental concern about <span class="citation" data-cites="ZhangPoliticalEndorsementNature2023">Zhang (2023)</span> is that they used a single item for every variable of interest. They did ask about two aspects of trustworthiness, whether the trustee was informed and whether they were biased. But that was done using exactly two questions, and responses weren’t combined. Psychologists seem to prefer multi-item instruments. For example, the METI has 14 items covering three aspects of trustworthiness: competence, benevolence, and integrity. I’m not entirely sure why psychologists prefer multi-item instruments, though, so I decided not to discuss it in the body of the post.↩︎</p></li>
<li id="fn8"><p><span class="citation" data-cites="ElliottValuesEnvironmentalResearch2017">Elliott et al. (2017)</span> report a second study with a policy conclusion, namely, whether BPA should be regulated. Due to limited funds my collaborator and I didn’t replicate this part.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-03-22-endorsement.html</guid>
  <pubDate>Wed, 22 Mar 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Constructivist accounts of representation, in science and politics</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-02-08-demagoguery.html</link>
  <description><![CDATA[ 




<p>Political theorists developed a <em>constructivist</em> approach to political representation around 2010; <span class="citation" data-cites="SawardRepresentativeClaim2010">Saward (2010)</span> seems to be the canonical citation for this approach, but I think <span class="citation" data-cites="BrownScienceDemocracyExpertise2009">M. B. Brown (2009)</span> is doing something quite similar. Prior to 2010, work by political theorists on representation focused on either (a) formal procedures of elections and voting or (b) deliberative practices, either in civil society (that is, outside of government) or in boundary institutions that connected public deliberation to policymakers. Recently several philosophers of science have picked up ideas from the latter (citations in <a href="2023-01-30-democracy-defense.html">a previous post</a>).</p>
<p>Constructivists challenge an implicit assumption of these approaches, namely, the there is a constituency or public that exists prior to representation and has more-or-less well-defined interests or preferences. This assumption might (<em>might</em>) make sense when we’re talking about legislative districts: the State of California existed before Diane Feinstein was first elected Senator in 1992. But it doesn’t work as well for less formal types of representation, for example, Martin Luther King, Jr., or Greta Thunberg as representatives of social movements.</p>
<p><span class="citation" data-cites="SawardRepresentativeClaim2010">Saward (2010)</span> apparently (I haven’t actually read the book) draws on Gricean philosophy of language that will be familiar to most philosophers today. Specifically, <span class="citation" data-cites="SawardRepresentativeClaim2010">Saward (2010)</span> examines representative claims as speech acts, claims that the speaker represents a certain public or constituency to an audience in a certain context and that this constituency believes or wants or does something. These acts are successful insofar as they are taken up by the audience. In other words, to be a representative for a constituency is nothing more or less than to be taken as a representative for that constituency.</p>
<p>Importantly, the constituency does not need to exist for a representative claim to be successful. One practical <em>effect</em> of the speech act might be that the constituency comes into existence as members of the audience identify themselves as members of that constituency. For example, what we now call the “MAGA movement” or “MAGA Republicans” didn’t exist until after Trump presented himself as a representative for that then-unnamed constituency in 2015. Drawing on Hobbes, <span class="citation" data-cites="BrownScienceDemocracyExpertise2009">M. B. Brown (2009)</span> calls this <em>virtual representation</em><sup>1</sup>, and he uses it to develop an account of (natural) scientists as representatives of nature in policy contexts. Even if it makes sense to talk about “nature” as a single coherent entity, that entity can’t do something like authorize a scientist as its representative through a formal election or have beliefs or interests that are accurately or inaccurately represented by a scientist’s representative claims.</p>
<p><span class="citation" data-cites="UrbinatiJudgmentAloneCloven2018">Urbinati (2018)</span> discusses this constructivist approach to political representation. I found her writing somewhat difficult to follow, so I’m not going to try to fully reconstruct her views. But she does seem to present three objections to constructivism:</p>
<ol type="1">
<li>It neglects inequalities in who will make representative claims and whose representative claims will be taken up.<br>
</li>
<li>The representative is accountable to the audience rather than the constituency.<br>
</li>
<li>There’s no reason to think that successful representative claims (as constructivists understand that success) will have any effects on policy. <span class="citation" data-cites="UrbinatiJudgmentAloneCloven2018">(Urbinati 2018, 74ff)</span></li>
</ol>
<p>The first objection echoes <span class="citation" data-cites="YoungInclusionDemocracy2000">Young (2000)</span>’s critique of deliberative democracy, which (prior to Young’s critique) tended to focus on idealized deliberative scenarios and norms of deliberation that favored highly educated white men. The third objection seems to provide the best support for <span class="citation" data-cites="UrbinatiJudgmentAloneCloven2018">Urbinati (2018)</span>’s “diarchic” approach, which emphasizes the relationship between formal institutions of political power and the open-ended deliberation of civil society. (I suspect some of the later contributions to this collection will do this as well.)</p>
<p>On the “pre-constructivist” models of representation, representatives are primarily accountable to the constituency that they represent. This might be done formally — the representative needs to stand for (re)election — or it might happen informally — as constituents call representatives to give an account of their actions qua representatives.<sup>2</sup> Urbinati’s point seems to be that, by basing legitimacy on audience uptake, constructivists lose the demand for democratic accountability. Audience uptake matters, and representative and constituency co-construct each other (again, consider Trump), but <em>good</em> representatives must still represent their constituents <em>accurately</em>.</p>
<p>For <span class="citation" data-cites="BrownScienceDemocracyExpertise2009">M. B. Brown (2009)</span>, a constructivist account of scientific representation (scientists as representatives of nature) is attractive because democratic accountability to “nature” seems to be nonsensical.<sup>3</sup> So if scientists are to be held accountable to anyone besides themselves, the audience seems to be the only option.</p>
<p>In the context of Brown’s account, we might understand Urbinati’s second concern as a critique of “wishful thinking” or “anything-goes” relativism: nothing seems to be stopping the audience from rejecting scientists whose claims are inconvenient or unfavorable (no matter how well-supported), and accepting scientists whose claims are convenient and favorable (no matter how flimsy) <span class="citation" data-cites="SarewitzHowScienceMakes2004 SteelQualifiedEpistemicPriority2017 LichtensteinInconvenientTruthInductive2022">(Sarewitz 2004; Steel 2017; Lichtenstein 2022)</span>.</p>
<p>One constructivist response to this critique might be to reject the notion of accuracy, as in the formulation “good representatives must still represent their constituents accurately.” Such a constructivist might argue that “accuracy” assumes a pre-existing constituency; but representative claims are prior to constituencies, etc. On the science side, this leads to the interminable and fatally abstract debates over “realism” of the 1970s and ’80s.</p>
<p>An alternative approach might draw on Dewey’s account of inquiry, specifically as articulated by <span class="citation" data-cites="BrownScienceMoralImagination2020">M. Brown (2020)</span>. On this account, inquiry starts with a problematic situation — things aren’t working — and inquiry is successful insofar as it resolves the situation — develops alternative things that do work. Importantly, “working” here has both “realist” and “constructivist” aspects. It’s realist insofar as the agent can <em>incorrectly</em> believe that a certain alternative will work when “in fact” it will not. But it’s also constructivist insofar as one tactic to resolve the situation is for the agent to change their understanding of the activity and its goals, and resolving the situation does not require a “true” representation of the world (in a classical correspondence sense).<sup>4</sup></p>
<p>I suggest we can view political representation as “inquiry” using this account. The problematic situation is that some people<sup>5</sup> have some felt dissatisfaction with the status quo. A representative claim is then a hypothesis that there is a group (the constituency) whose interests are not being served by the policy status quo (the diagnosis), and that adopting a different policy will resolve the problematic situation (the remedy).</p>
<p>On this model, a representative claim will have locutionary success criteria, that is, it needs to be taken up by the audience, especially people with the power to enact the proposed policy. And the constituency can be constructed by the representative claim, insofar as the “some people” subsequently decide that they are indeed members of the group and their interests are not being served by the status quo. But <strong>the representative claim also has pragmatic success criteria</strong>: would the remedy actually resolve the problematic situation as characterized by the diagnosis? This is unlikely to be the case if, for example, there is good evidence that the felt dissatisfaction experienced by group members is not caused by the factors identified in the diagnosis.</p>
<p>Consider how this model might work for Trump’s 2016 campaign. We can reconstruct the central representative claim of Trump’s campaign as something like the hypothesis that rural poverty is caused by mass undocumented immigration and so strong immigration restrictions will solve this problem. This claim was taken up by the media, who did numerous profiles of “Trump voters” (the newly-constructed constituency) in impoverished rural areas and their attitudes towards immigration. So it was evidently a locutionary success. But its pragmatic success is tenuous at best; agricultural consolidation and demographic shifts (depopulation and aging as young adults go to college and move to urban areas) are probably much more important causes of rural poverty than immigration, undocumented or otherwise.</p>





<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-BrownScienceDemocracyExpertise2009" class="csl-entry">
Brown, Mark B. 2009. <em>Science in Democracy: Expertise, Institutions, and Representation</em>. Cambridge, Mass: MIT Press.
</div>
<div id="ref-BrownScienceMoralImagination2020" class="csl-entry">
Brown, Matthew. 2020. <em>Science and Moral Imagination: A New Ideal for Values in Science</em>. University of Pittsburgh Press.
</div>
<div id="ref-LiboironPollutionColonialism2021" class="csl-entry">
Liboiron, Max. 2021. <em>Pollution Is Colonialism</em>. Duke University Press.
</div>
<div id="ref-LichtensteinInconvenientTruthInductive2022" class="csl-entry">
Lichtenstein, Eli I. 2022. <span>“Inconvenient Truth and Inductive Risk in Covid-19 Science.”</span> <em>Philosophy of Medicine</em> 3 (1). <a href="https://doi.org/10.5195/pom.2022.132">https://doi.org/10.5195/pom.2022.132</a>.
</div>
<div id="ref-SarewitzHowScienceMakes2004" class="csl-entry">
Sarewitz, Daniel. 2004. <span>“How Science Makes Environmental Controversies Worse.”</span> <em>Environmental Science and Policy</em> 7 (5): 385–403. <a href="https://doi.org/10.1016/j.envsci.2004.06.001">https://doi.org/10.1016/j.envsci.2004.06.001</a>.
</div>
<div id="ref-SawardRepresentativeClaim2010" class="csl-entry">
Saward, Michael. 2010. <em>The Representative Claim</em>. OUP Oxford.
</div>
<div id="ref-SimpsonLandPedagogyNishnaabeg2014" class="csl-entry">
Simpson, Leanne Betasamosake. 2014. <span>“Land as Pedagogy: Nishnaabeg Intelligence and Rebellious Transformation.”</span> <em>Decolonization: Indigeneity, Education &amp; Society</em> 3 (3). <a href="https://jps.library.utoronto.ca/index.php/des/article/view/22170">https://jps.library.utoronto.ca/index.php/des/article/view/22170</a>.
</div>
<div id="ref-SteelQualifiedEpistemicPriority2017" class="csl-entry">
Steel, Daniel. 2017. <span>“Qualified Epistemic Priority.”</span> In <em>Current Controversies in Values and Science</em>, edited by Kevin C. Elliott and Daniel Steel, 49–63. New York and London: Routledge.
</div>
<div id="ref-UrbinatiJudgmentAloneCloven2018" class="csl-entry">
Urbinati, Nadia. 2018. <span>“Judgment Alone: Cloven Citizenship in the Era of the Internet.”</span> In <em>Creating Political Presence: The New Politics of Democratic Representation</em>, edited by Dario Castiglione and Johannes Pollak. Chicago, IL: University of Chicago Press. <a href="https://press.uchicago.edu/ucp/books/book/chicago/C/bo29203064.html">https://press.uchicago.edu/ucp/books/book/chicago/C/bo29203064.html</a>.
</div>
<div id="ref-YoungInclusionDemocracy2000" class="csl-entry">
Young, Iris Marion. 2000. <em>Inclusion and Democracy</em>. Oxford University Press. <a href="http://books.google.ca/books?id=Gonp8GXTxb0C&amp;pg=PA18&amp;dq=intitle:INCLUSION+AND+DEMOCRACY+inauthor:young&amp;hl=&amp;cd=1&amp;source=gbs_api">http://books.google.ca/books?id=Gonp8GXTxb0C&amp;pg=PA18&amp;dq=intitle:INCLUSION+AND+DEMOCRACY+inauthor:young&amp;hl=&amp;cd=1&amp;source=gbs_api</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Or maybe “fictional representation”? Need to check.↩︎</p></li>
<li id="fn2"><p>Young cite here?↩︎</p></li>
<li id="fn3"><p>Yesterday I was preparing <span class="citation" data-cites="SimpsonLandPedagogyNishnaabeg2014">Simpson (2014)</span> as a reading on Indigenous epistemology and land relations for the Environmental Philosophy course that I’m teaching this semester. From the perspective of Indigenous land relations, accountability to “nature” — or rather, nonhuman agents — does make sense. Indeed, as presented by various Indigenous authors <span class="citation" data-cites="SimpsonLandPedagogyNishnaabeg2014 LiboironPollutionColonialism2021">(Simpson 2014; Liboiron 2021)</span> a key idea in Indigenous knowledge-making practices is consent and collaboration with nonhuman agents. I’m going to bracket this thought for the purposes of this post.↩︎</p></li>
<li id="fn4"><p>Sorry Matt Brown, nobody else thinks Dewey was a correspondence theorist.↩︎</p></li>
<li id="fn5"><p>This should be understood as a Boolosean plural quantifier: “there are some <img src="https://latex.codecogs.com/png.latex?x">s,” rather than “there is a set <img src="https://latex.codecogs.com/png.latex?X">.”↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-02-08-demagoguery.html</guid>
  <pubDate>Wed, 08 Feb 2023 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Undue influence of epistemic values</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-01-30-democracy-defense.html</link>
  <description><![CDATA[ 




<p>[We discussed this post <a href="https://www.facebook.com/hicks.daniel.j/posts/pfbid0UYjAapCWUSVvtfqg5bM5viGYJ3fCR9XU5DGLBkAcGn1u6cEoCrAymY2nRBbBtqqxl?comment_id=1342087143214313&amp;notif_id=1675111791180975&amp;notif_t=feed_comment&amp;ref=notif">on Facebook</a>.]</p>
<p><span class="citation" data-cites="LuskDoesDemocracyRequire2021">Lusk (2021)</span> examines <em>the political legitimacy argument for the value free ideal</em>; elsewhere collaborators and I have called this the “democracy defense” <span class="citation" data-cites="FernandezPintoLegitimizingValuesRegulatory2019 HicksInductiveRiskScience2020">(Fernández Pinto and Hicks 2019; Hicks, Magnus, and Wright 2020)</span>. Here’s Lusk’s reconstruction of the argument <span class="citation" data-cites="LuskDoesDemocracyRequire2021">(Lusk 2021, 104; edited down a bit further by me)</span>:</p>
<ol type="1">
<li><strong>Legitimacy Premise</strong>: No set of non-epistemic values should have an undue influence in coercive democratic political decisions.</li>
<li><strong>Infiltration Premise</strong>: If non-epistemic values play a role in the empirical justification of political decisions, then those values have an undue influence.</li>
<li>Therefore, non-epistemic values should not play a role in empirical justification of political decisions.</li>
</ol>
<p>Lusk focuses on the Infiltration Premise, arguing that institutions for deliberative democracy can “generate [democratically] acceptable sets of values for use in scientific methodology” <span class="citation" data-cites="LuskDoesDemocracyRequire2021">(Lusk 2021, 215)</span>; influences of these values would therefore not be undue, and so the Infiltration Premise would be false.</p>
<p>I was actually more interested in the Legitimacy Premise. Lusk gives it a brief defense:</p>
<blockquote class="blockquote">
<p>The legitimacy premise follows from liberal democratic principles. One of the core ideals of liberal democracy is that the democratic state should be <em>neutral</em> with regard to the different values and desires held by citizens; the state should not play favorites. <span class="citation" data-cites="LuskDoesDemocracyRequire2021">(Lusk 2021, 214, my emphasis)</span></p>
</blockquote>
<p>The Legitimacy Premise implicates — though does not logically entail — a difference in status between epistemic and non-epistemic values<sup>1</sup>. Specifically, the Legitimacy Premise picks out non-epistemic values as specifically problematic or worrisome if they play a role in political decisionmaking; epistemic values are not regarded as problematic or worrisome.</p>
<p>But epistemic values can have influences that undermine (descriptive) legitimacy. Consider <em>Strengthening Transparency in Regulatory Science</em>, a rule proposed and (very briefly) adopted by the US EPA under the Trump administration <span class="citation" data-cites="HicksWhenVirtuesAre2022 HicksOpenScienceReplication2023">(Hicks 2022, 2023)</span>. The rule restricted the science that EPA could use to justify regulation, effectively imposing a strong open data requirement on environmental public health research. The rule was (publicly) justified by an appeal to epistemic values, namely, that open science practices will make the underlying science more reliable and reduce the rate/influence of false positive results. The rule received on the order of a million public comments, the vast majority of which were sharply negative. One of the most common criticisms was that the rule would undermine and delay regulation necessary to protect human health and the environment.</p>
<p>In other words, <em>Strengthening Transparency</em> was widely regarded as illegitimate because it prioritized an epistemic value (avoiding false positives) over a non-epistemic value (protecting human health and the environment).</p>
<p>I suggest that the Legitimacy Premise relies on a pair of assumptions, namely, that epistemic values are politically neutral while non-epistemic values are politically controversial. Consider the block quotation from Lusk above. A principle of state neutrality only justifies excluding non-epistemic values insofar as they are controversial (ie, not neutral), and would also apply to controversial epistemic values. In the case of <em>Strengthening Transparency</em>, the non-epistemic value of protecting human health and the environment was, mostly, politically neutral; it was the epistemic value of eliminating false positives that was controversial.</p>
<p>The ideal of state neutrality is an ideal of depoliticization: that we can somehow get beyond the implacable chaos of power hierarchies, deep disagreement, sophistry, and naked (or artfully clothed) self-interest. The attraction of technocracy is that it purports to move us closer to this ideal, at least in the realm of the administrative state.</p>
<p>So it seems to me there is a deep irony within the “democracy defense” of the value-free ideal. Proponents of this argument present themselves as defending democracy from technocrats run amock. But the conceptual framework and institutional forms assumed by the argument function to carve out a depoliticized, technocratic realm that is protected from democratic accountability.</p>





<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-FernandezPintoLegitimizingValuesRegulatory2019" class="csl-entry">
Fernández Pinto, Manuela, and Daniel J. Hicks. 2019. <span>“Legitimizing Values in Regulatory Science.”</span> <em>Environmental Health Perspectives</em> 127 (3): 035001. <a href="https://doi.org/10.1289/EHP3317">https://doi.org/10.1289/EHP3317</a>.
</div>
<div id="ref-HicksWhenVirtuesAre2022" class="csl-entry">
Hicks, Daniel J. 2022. <span>“When Virtues are Vices: <span>‘Anti-Science’</span> Epistemic Values in Environmental Politics.”</span> <em>Philosophy, Theory, and Practice in Biology</em> 14 (0). <a href="https://doi.org/10.3998/.2629">https://doi.org/10.3998/.2629</a>.
</div>
<div id="ref-HicksOpenScienceReplication2023" class="csl-entry">
———. 2023. <span>“Open Science, the Replication Crisis, and Environmental Public Health.”</span> <em>Accountability in Research</em> 30 (1): 34–62. <a href="https://doi.org/10.1080/08989621.2021.1962713">https://doi.org/10.1080/08989621.2021.1962713</a>.
</div>
<div id="ref-HicksInductiveRiskScience2020" class="csl-entry">
Hicks, Daniel J., P. D. Magnus, and Jessey Wright. 2020. <span>“Inductive Risk, Science, and Values: A Reply to MacGillivray.”</span> <em>Risk Analysis</em> 40 (4): 667–73. <a href="https://doi.org/10.1111/risa.13434">https://doi.org/10.1111/risa.13434</a>.
</div>
<div id="ref-LuskDoesDemocracyRequire2021" class="csl-entry">
Lusk, Greg. 2021. <span>“Does Democracy Require Value-Neutral Science? Analyzing the Legitimacy of Scientific Information in the Political Sphere.”</span> <em>Studies in History and Philosophy of Science Part A</em> 90 (December): 102–10. <a href="https://doi.org/10.1016/j.shpsa.2021.08.009">https://doi.org/10.1016/j.shpsa.2021.08.009</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Many authors in the SVP literature use “non-epistemic” values as synonymous with social, ethical, and political values. However, if epistemic values are defined as features of scientific practice or its products that promote the attainment of truth, then paradigm “non-epistemic” values can be truth-promoting.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-01-30-democracy-defense.html</guid>
  <pubDate>Mon, 30 Jan 2023 08:00:00 GMT</pubDate>
</item>
<item>
  <title>The presence-absence model and epistemic representation</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-01-26-principle-agent.html</link>
  <description><![CDATA[ 




<p>In 2015-16 I was working for<sup>1</sup> the US Environmental Protection Agency. My work colleagues were technocratic managers, most of whom had PhDs and were professionalized as scientists (even when they were on the policymaking, risk management side of things). Because the concept of “legitimacy” in questions about “the legitimate roles for values in science” comes from <em>political</em> legitimacy — at least, it does for me — I started to ruminate on the idea of <strong>scientists as representatives</strong>, in close analogy to elected political representatives. Manuela Fernández-Pinto and I included a few of these ideas in <span class="citation" data-cites="FernandezPintoLegitimizingValuesRegulatory2019">Fernández Pinto and Hicks (2019)</span>. But I haven’t taken the time to develop the analogy further.</p>
<p>Among political theorists, a key work on political representation is Hanna Pitkin’s <em>The Concept of Representation</em> <span class="citation" data-cites="PitkinConceptRepresentation1967">(Pitkin 1967)</span>. While I haven’t read it myself, my understanding is that Pitkin developed an account of representation in terms of presence and absence. As Mark Warren puts it, “representatives stand in, speak, or act for the represented <em>in their absence</em>: they are <em>represented in</em> the spaces and activities where they cannot be” <span class="citation" data-cites="WarrenHowRepresentationEnables2018">(Warren 2018, 39, Warren’s emphasis)</span>. This way of thinking about representation goes back at least to Hobbes, where the relationship between the people and the sovereign is literally a principal-agent contract <span class="citation" data-cites="BrownScienceDemocracyExpertise2009">(Brown 2009 ch.&nbsp;5)</span>. In this line of political thought, the represented are absent primarily for logistical reasons: it’s impractical to have thousands (or millions) of people in one place all talking to each other and coming to agreement.</p>
<p>This model is simplistic, and in later posts I’ll start to explore its limits and follow the intellectual trails constructed by political theorists. But, just as this is a useful first model for political representation, I want to argue that it’s a useful first model for <strong>scientists as epistemic representatives</strong>. Namely: scientists stand in, speak, or act <em>epistemically</em> for the general public in their absence. Scientists are often epistemic agents on behalf of the public, who are generally absent from the space of technocratic decisionmaking.<sup>2</sup></p>
<p>The public are typically absent from these spaces for the same logistical reasons that they are typically absent from spaces of political decisionmaking. But also for two other, more interesting reasons. First, the public lacks the credentials of scientists, the diplomas and work history that are taken as evidence that someone has relevant knowledge and skill. And second, the public often lack relevant knowledge and skill.</p>
<p>When I talk about expertise in my undergraduate Critical Reasoning course, I like to illustrate the difference between credentials and actual expertise with the examples of ACT UP and the environmental justice movement <span class="citation" data-cites="EpsteinImpureScience1996 PauliFlintFightsBack2019">(Epstein 1996; Pauli 2019)</span>. <em>[Say more here in the actual paper]</em></p>
<p>However, on any given topic, most people have only very limited knowledge and skill. We are radically epistemically interdependent beings, relying on others with actual expertise to develop knowledge that promotes our epistemic interests <span class="citation" data-cites="WilholtEpistemicInterestsObjectivity2022">(Wilholt 2022)</span>. I suggest that this point corresponds to political theorists’ distinction between <em>preferences</em> and <em>interests</em> <span class="citation" data-cites="WarrenHowRepresentationEnables2018">(Warren 2018, 41)</span>. A good political representative acts to promote the considered interests of the constituency — the things they would want if they had the time and resources to engage in thoughtful deliberation — even if those interests don’t align with what they prefer in the moment. Similarly, scientists as epistemic representatives are responsible for producing the knowledge that we would produce if we had the time and resources to do so.</p>
<p>Following Warren’s discussion, we run into a problem:</p>
<blockquote class="blockquote">
<p>without more analysis, the formulation can tacitly legitimize paternalistic claims by representatives to know what is best for their constituents’ interests, often despite their preferences. At some limit, this formulation merges with Burke’s conception of elected representatives as <em>trustees</em> who substitute their (better) judgment for those of their relatively uninformed constituents. <span class="citation" data-cites="WarrenHowRepresentationEnables2018">(Warren 2018, 42, Warren’s emphasis)</span></p>
</blockquote>
<p>The use of “paternalism” here is notable, because feminist philosophy of science was born out of feminist critics of paternalistic, patriarchal science, especially (but not only) biomedical science. It’s therefore tempting to say that this same paternalism is a key problem for the legitimacy of the technocratic state today. Consider the backlash to lockdown approaches to Covid-19 <span class="citation" data-cites="HarvardValueJudgmentsCOVID192021 HarvardCausalInferenceMoral2021 LeeViralVisualizationsHow2021">(Harvard et al. 2021; Harvard and Winsberg 2021; Lee et al. 2021)</span>.</p>
<p>But let’s follow the political theorists for one more dialectical step. Warren fends off the spectre of Burke by offering “two loci of judgment [that] must be robust for <em>democratic</em> representation to occur:”</p>
<blockquote class="blockquote">
<ul>
<li>The representative must be responsive to the represented, which involves judgments about their interests as affected by a relevant collectivity.</li>
<li>The represented must judge in what ways and how well they are represented by a representative, especially insofar as their interests are affected by a relevant collectivity. <span class="citation" data-cites="WarrenHowRepresentationEnables2018">(Warren 2018, 43)</span></li>
</ul>
</blockquote>
<p>In other words, democratic representation requires continuous, reciprocal exchange between representatives and their constituents. Representatives might make decisions that contradict the constituents’ preferences. But they will need to be able to justify those decisions in terms that the constituents recognize as their interests. And the constituents will need to engage in quasi-public deliberation and remonstration, and ultimately decide whether or not to accept the claimed interests as theirs <span class="citation" data-cites="WarrenHowRepresentationEnables2018">(Warren 2018, 45)</span>.</p>
<p>On this analysis, the problem with the technocratic management state is that it lacks institutions — or even informal effective processes — for the necessary continuous reciprocal exchange.<sup>3</sup> And, per my last blog post, this reflects the value-free ideal.</p>
<p>A responsive, reciprocal relationship between scientists as representatives and their constituents can also help to address the gap between credentials and actual expertise. In environmental justice contexts, one important role of credentialed expert allies is to translate the expertise of community members and activists into the idiom of the technocratic management state <span class="citation" data-cites="OttingerTechnoscienceEnvironmentalJustice2011">(Ottinger and Cohen 2011, 7–8)</span>. Activists might also be included more directly in policymaking, as with ACT-UP <span class="citation" data-cites="EpsteinImpureScience1996">(Epstein 1996)</span>.</p>
<p>All together, while it is oversimplified, this first model of representation supports an argument for incorporating more substantive opportunities for direct interaction between publics and experts in technocratic policymaking. For decades, science policy scholars and political scientists have examined a variety of institutional forms for such interactions <span class="citation" data-cites="WarrenCanDeliberativeMinipublics2015 SteelRethinkingRepresentationDiversity2020 KaplanDesigningParticipatoryTechnology2021">(Warren and Gastil 2015; Steel et al. 2020; Kaplan et al. 2021)</span>. However, it’s worth noting that some European studies have found that deliberative minipublics do not improve legitimacy <span class="citation" data-cites="JacobsRightKindParticipation2021 GoldbergCatchingDeliberativeWave2022">(Jacobs and Kaufmann 2021; Goldberg and Bächtiger 2022)</span>.</p>





<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-BrownScienceDemocracyExpertise2009" class="csl-entry">
Brown, Mark B. 2009. <em>Science in Democracy: Expertise, Institutions, and Representation</em>. Cambridge, Mass: MIT Press.
</div>
<div id="ref-CostaPublicCommentsInfluence2019" class="csl-entry">
Costa, Mia, Bruce A. Desmarais, and John A. Hird. 2019. <span>“Public Comments’ Influence on Science Use in U.S. Rulemaking: The Case of EPA’s National Emission Standards.”</span> <em>The American Review of Public Administration</em> 49 (1): 36–50. <a href="https://doi.org/10.1177/0275074018795287">https://doi.org/10.1177/0275074018795287</a>.
</div>
<div id="ref-EpsteinImpureScience1996" class="csl-entry">
Epstein, Steven. 1996. <em>Impure Science</em>. AIDS, Activism, and the Politics of Knowledge. Berkeley, Los Angeles, and Oxford: University of California Press. <a href="http://books.google.ca/books?id=kZOso0FMsrMC&amp;pg=PA39&amp;dq=intitle:Impure+Science+inauthor:epstein&amp;hl=&amp;cd=1&amp;source=gbs_api">http://books.google.ca/books?id=kZOso0FMsrMC&amp;pg=PA39&amp;dq=intitle:Impure+Science+inauthor:epstein&amp;hl=&amp;cd=1&amp;source=gbs_api</a>.
</div>
<div id="ref-FernandezPintoLegitimizingValuesRegulatory2019" class="csl-entry">
Fernández Pinto, Manuela, and Daniel J. Hicks. 2019. <span>“Legitimizing Values in Regulatory Science.”</span> <em>Environmental Health Perspectives</em> 127 (3): 035001. <a href="https://doi.org/10.1289/EHP3317">https://doi.org/10.1289/EHP3317</a>.
</div>
<div id="ref-GillamQuestionsEPAMonsantoCollusion2017" class="csl-entry">
Gillam, Carey. 2017. <span>“Questions about EPA-Monsanto Collusion Raised in Cancer Lawsuits.”</span> HuffPost. February 13, 2017. <a href="https://www.huffpost.com/entry/questions-about-epa-monsa_b_14727648">https://www.huffpost.com/entry/questions-about-epa-monsa_b_14727648</a>.
</div>
<div id="ref-GoldbergCatchingDeliberativeWave2022" class="csl-entry">
Goldberg, Saskia, and André Bächtiger. 2022. <span>“Catching the <span>‘Deliberative Wave’</span>? How (Disaffected) Citizens Assess Deliberative Citizen Forums.”</span> <em>British Journal of Political Science</em>, March, 1–9. <a href="https://doi.org/10.1017/S0007123422000059">https://doi.org/10.1017/S0007123422000059</a>.
</div>
<div id="ref-HaederInfluenceAdministrativeProcess2015" class="csl-entry">
Haeder, Simon F., and Susan Webb Yackee. 2015. <span>“Influence and the Administrative Process: Lobbying the U.S. President’s Office of Management and Budget.”</span> <em>American Political Science Review</em> 109 (3): 507–22. <a href="https://doi.org/10.1017/S0003055415000246">https://doi.org/10.1017/S0003055415000246</a>.
</div>
<div id="ref-HaederPresidentiallyDirectedPolicy2018" class="csl-entry">
Haeder, Simon F, and Susan Webb Yackee. 2018. <span>“Presidentially Directed Policy Change: The Office of Information and Regulatory Affairs as Partisan or Moderator?”</span> <em>Journal of Public Administration Research and Theory</em> 28 (4): 475–88. <a href="https://doi.org/10.1093/jopart/muy033">https://doi.org/10.1093/jopart/muy033</a>.
</div>
<div id="ref-HarvardCausalInferenceMoral2021" class="csl-entry">
Harvard, Stephanie, and Eric Winsberg. 2021. <span>“Causal Inference, Moral Intuition, and Modeling in a Pandemic.”</span> <em>Philosophy of Medicine</em> 2 (2). <a href="https://doi.org/10.5195/philmed.2021.70">https://doi.org/10.5195/philmed.2021.70</a>.
</div>
<div id="ref-HarvardValueJudgmentsCOVID192021" class="csl-entry">
Harvard, Stephanie, Eric Winsberg, John Symons, and Amin Adibi. 2021. <span>“Value Judgments in a COVID-19 Vaccination Model: A Case Study in the Need for Public Involvement in Health-Oriented Modelling.”</span> <em>Social Science &amp; Medicine</em> 286 (October): 114323. <a href="https://doi.org/10.1016/j.socscimed.2021.114323">https://doi.org/10.1016/j.socscimed.2021.114323</a>.
</div>
<div id="ref-JacobsRightKindParticipation2021" class="csl-entry">
Jacobs, Daan, and Wesley Kaufmann. 2021. <span>“The Right Kind of Participation? The Effect of a Deliberative Mini-Public on the Perceived Legitimacy of Public Decision-Making.”</span> <em>Public Management Review</em> 23 (1): 91–111. <a href="https://doi.org/10.1080/14719037.2019.1668468">https://doi.org/10.1080/14719037.2019.1668468</a>.
</div>
<div id="ref-KaplanDesigningParticipatoryTechnology2021" class="csl-entry">
Kaplan, Leah R., Mahmud Farooque, Daniel Sarewitz, and David Tomblin. 2021. <span>“Designing Participatory Technology Assessments: A Reflexive Method for Advancing the Public Role in Science Policy Decision-Making.”</span> <em>Technological Forecasting and Social Change</em> 171 (October): 120974. <a href="https://doi.org/10.1016/j.techfore.2021.120974">https://doi.org/10.1016/j.techfore.2021.120974</a>.
</div>
<div id="ref-LeeViralVisualizationsHow2021" class="csl-entry">
Lee, Crystal, Tanya Yang, Gabrielle Inchoco, Graham M. Jones, and Arvind Satyanarayan. 2021. <span>“Viral Visualizations: How Coronavirus Skeptics Use Orthodox Data Practices to Promote Unorthodox Science Online.”</span> <em>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May, 1–18. <a href="https://doi.org/10.1145/3411764.3445211">https://doi.org/10.1145/3411764.3445211</a>.
</div>
<div id="ref-OttingerTechnoscienceEnvironmentalJustice2011" class="csl-entry">
Ottinger, Gwen, and Benjamin R. Cohen, eds. 2011. <em>Technoscience and Environmental Justice: Expert Cultures in a Grassroots Movement</em>. Urban and Industrial Environments. Cambridge, Mass: MIT Press.
</div>
<div id="ref-PauliFlintFightsBack2019" class="csl-entry">
Pauli, Benjamin J. 2019. <em>Flint Fights Back: Environmental Justice and Democracy in the Flint Water Crisis</em>. MIT Press.
</div>
<div id="ref-PitkinConceptRepresentation1967" class="csl-entry">
Pitkin, Hanna F. 1967. <em>The Concept of Representation</em>. University of California Press.
</div>
<div id="ref-RichardsonAdministrativePolicymakingRule1999" class="csl-entry">
Richardson, Henry. 1999. <span>“Administrative Policy-Making: Rule of Law or Bureaucracy?”</span> In <em>Recrafting the Rule of Law: The Limits of Legal Order</em>, edited by David Dyzenhaus, 309–30. Oxford and Portland, Oregon: Hart Publishing.
</div>
<div id="ref-SteelRethinkingRepresentationDiversity2020" class="csl-entry">
Steel, Daniel, Naseeb Bolduc, Kristina Jenei, and Michael Burgess. 2020. <span>“Rethinking Representation and Diversity in Deliberative Minipublics.”</span> <em>Journal of Deliberative Democracy</em> 16 (1): 46–57. <a href="https://doi.org/10.16997/jdd.398">https://doi.org/10.16997/jdd.398</a>.
</div>
<div id="ref-WarrenHowRepresentationEnables2018" class="csl-entry">
Warren, Mark E. 2018. <span>“How Representation Enables Democratic Citizenship.”</span> In <em>Creating Political Presence: The New Politics of Democratic Representation</em>, edited by Dario Castiglione and Johannes Pollak, 39–60. Chicago, IL: University of Chicago Press. <a href="https://press.uchicago.edu/ucp/books/book/chicago/C/bo29203064.html">https://press.uchicago.edu/ucp/books/book/chicago/C/bo29203064.html</a>.
</div>
<div id="ref-WarrenCanDeliberativeMinipublics2015" class="csl-entry">
Warren, Mark E., and John Gastil. 2015. <span>“Can Deliberative Minipublics Address the Cognitive Challenges of Democratic Citizenship?”</span> <em>The Journal of Politics</em> 77 (2): 562–74. <a href="https://doi.org/10.1086/680078">https://doi.org/10.1086/680078</a>.
</div>
<div id="ref-WilholtEpistemicInterestsObjectivity2022" class="csl-entry">
Wilholt, Torsten. 2022. <span>“Epistemic Interests and the Objectivity of Inquiry.”</span> <em>Studies in History and Philosophy of Science</em> 91 (February): 86–93. <a href="https://doi.org/10.1016/j.shpsa.2021.11.009">https://doi.org/10.1016/j.shpsa.2021.11.009</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Technically, I was working for AAAS, as a Science and Technology Policy Fellow, and “hosted by” EPA.↩︎</p></li>
<li id="fn2"><p>Note that I intend to use “epistemic agent” in exactly the same way as feminist social epistemologists.↩︎</p></li>
<li id="fn3"><p>This is incorrect as stated. Agencies like EPA have at least two such processes. However, one is formal and ineffective, while the other is effective but fails criteria of democratic inclusion and justice. First, the formal notice-and-comment system solicits feedback from the general public on proposed regulation. But agency staff merely have to provide pro forma responses to these comments, giving the public very little effective political agency in this process <span class="citation" data-cites="RichardsonAdministrativePolicymakingRule1999 HaederInfluenceAdministrativeProcess2015 HaederPresidentiallyDirectedPolicy2018 CostaPublicCommentsInfluence2019">(Richardson 1999; Simon F. Haeder and Yackee 2015; Simon F. Haeder and Yackee 2018; Costa, Desmarais, and Hird 2019)</span>. Second, regulatory agency staff often have informal interactions with various “stakeholders,” providing opportunities for much more effective reciprocal exchange. But typically the only stakeholders included in these interactions are regulated industries <span class="citation" data-cites="GillamQuestionsEPAMonsantoCollusion2017">(eg, Gillam 2017)</span>. Environmental justice communities, for example, are unlikely to be included in these interactions, making these interactions both exclusionary and injust.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-01-26-principle-agent.html</guid>
  <pubDate>Thu, 26 Jan 2023 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Technocratic legitimacy and the value-free ideal</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2023-01-25-legitimacy-vfi.html</link>
  <description><![CDATA[ 




<p><span class="citation" data-cites="HolmanNewDemarcationProblem2022">Holman and Wilholt (2022)</span> argue that, as philosophers of science work to dismantle the value-free ideal, it is important to understand the function that the ideal played. Quoting Chesterton, they write</p>
<blockquote class="blockquote">
<p>only when one understands how an institution arose and what purposes it was intended to serve, is one in a position to say “they were bad purposes, or they have since become bad purposes, or that they are purposes which are no longer being served.” <span class="citation" data-cites="HolmanNewDemarcationProblem2022">(Holman and Wilholt 2022, 214)</span></p>
</blockquote>
<p>Consequently,</p>
<blockquote class="blockquote">
<p>if one rejects a purpose which motivated the adoption of the value-free ideal, then one must have a convincing answer as to why it should never have been endorsed, why it should no longer be endorsed, or at least show that the value-free ideal fails to serve the intended purpose. <span class="citation" data-cites="HolmanNewDemarcationProblem2022">(Holman and Wilholt 2022, 215)</span></p>
</blockquote>
<p>Holman and Wilholt identify three functions for the value-free ideal:</p>
<dl>
<dt>veracity</dt>
<dd>
science pursues truth and avoids error
</dd>
<dt>universality</dt>
<dd>
scientific results are useable by anyone, whether they share scientists’ personal value-judgments or not
</dd>
<dt>authority</dt>
<dd>
science provides “a trustworthy body of knowledge that has broadly recognized social legitimacy” <span class="citation" data-cites="HolmanNewDemarcationProblem2022">(Holman and Wilholt 2022, 214)</span>
</dd>
</dl>
<p>In the brief discussion that follows, Holman and Wilholt tie this sense of authority to questions of political legitimacy. In this post, I want to argue that the value-free ideal plays an important role in legitimizing the progressive, technocratic state, as illustrated in two historic moments where the value-free ideal was articulated.</p>
<p>The first moment was the Progressive Era of the late nineteenth and early twentieth centuries. The Progressive movement proposed that management of social-biological systems by credentialed, scientific experts would ensure prosperity and social peace, in areas including public health <span class="citation" data-cites="KriegerEpidemiologyPeopleHealth2011">(Krieger 2011 ch.&nbsp;4)</span>, the economy <span class="citation" data-cites="StaplefordCostLivingAmerica2009">(Stapleford 2009)</span>, and natural resources (Gifford Pinchot), but also the human gene pool (eugenics). Versions of the value-free ideal were developed around the same time, most famously by Weber.</p>
<p>The second moment was the emergence of the risk management state in the 1970s-1980s <span class="citation" data-cites="BeckRiskSocietyNew1992">(Beck 1992)</span>. Just as the Progressive movement was a technocratic response to the social and public health crises of the late nineteenth century, the risk management state was a technocratic response to the environmental and consumer safety crises of the 1950s-1960s. The primary methodology of the risk management state, risk assessment, institutionalized the value-free ideal, with (in principle) a strict demarcation between risk analysis — the scientific task of quantitatively determining the probability and magnitude of hazard — and risk management — the political task of designing and implementing policies to prevent and mitigate risk <span class="citation" data-cites="FernandezPintoLegitimizingValuesRegulatory2019">(Fernández Pinto and Hicks 2019)</span>.</p>
<p>In both moments, the value-free ideal enabled supporters of the technocratic state to de-politicize technocratic management, by distinguishing the realm of rational, expert decisionmaking — as “science” — from the realm of emotional, public “politics.” To reconcile technocracy with democracy — at least in the US — expert managers were placed under the authority of political appointees, who in turn were appointed and approved by elected officials. In other words, two forms of legitimacy were used to support the technocratic state. Democratic legitimacy was nominally top-down, from the voters electing politicians, who then delegate authority to experts in the executive branch agencies. While technocratic legitimacy was nominally bottom-up, depending on credentials and demonstrated competence as experts rose through the meritocratic hierarchy of those agencies. These two systems of legitimacy are not obviously compatible; consider today’s debates between epistocrats and democrats. The value-free ideal offers a solution, creating non-overlapping magisteria <span class="citation" data-cites="GouldRocksAgesScience2011">(Gould 2011)</span>: the systems do not come into conflict because potentially controversial values do not play a role in the experts’ work.</p>
<p>Unfortunately, in this role the value-free ideal is self-defeating <span class="citation" data-cites="FernandezPintoLegitimizingValuesRegulatory2019">(Fernández Pinto and Hicks 2019)</span>. The problem is not so much that properly assessing the downstream risk of error requires appeal to potentially controversial political and ethical values, as per the argument from inductive risk <span class="citation" data-cites="ElliottExploringInductiveRisk2017">(Elliott and Richards 2017)</span>. In the context of risk assessment, both scientists and policymakers have maintained the façade of value-freedom by, basically, pretending that all consideration of downstream consequences can be isolated on the political, risk management side of things.</p>
<p>Instead, the value-free ideal is self-defeating because of the <em>fact of reasonable scientific pluralism</em>. In almost any case, it’s possible to construct a reasonable, technically sophisticated argument in favor of a different methodology, more and better data, an overlooked possibility, or an alternative explanation. Given resources and connections, parties with an interest in a certain policy outcome can pretty much always find at least one credentialed expert who will not only make an apparently reasonable, highly technical case for a preferred interpretation of “the science,” but also will castigate opposing experts as “biased” or following “an agenda.” The STS scholar Dan Sarewitz called this scenario “an excess of objectivity” <span class="citation" data-cites="SarewitzScienceEnvironmentalPolicy2000 SarewitzHowScienceMakes2004">(Sarewitz 2000, 2004)</span> and I’ve referred to it as “Scientific Controversies as Proxy Politics” <span class="citation" data-cites="HicksScientificControversiesProxy2017">(Hicks 2017)</span>.</p>
<p>Given this analysis, what should a replacement for the value-free ideal attempt to do? One option would be something like “provide an alternative basis for the legitimacy of the technocratic state, more or less as it exists now.” I think many philosophers of science would support this sort of project. But to me this is not an attractive project — and I say this as someone who would probably be working for the US Environmental Protection Agency today if Clinton had won the 2016 election. Instead, I suggest that <strong>a replacement for the value-free ideal should help us democratize the technocratic state</strong>.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-BeckRiskSocietyNew1992" class="csl-entry">
Beck, Ulrich. 1992. <em>Risk society: towards a new modernity</em>. Theory, culture &amp; society. London ; Newbury Park, Calif: Sage Publications.
</div>
<div id="ref-ElliottExploringInductiveRisk2017" class="csl-entry">
Elliott, Kevin C., and Ted Richards, eds. 2017. <em>Exploring Inductive Risk: Case Studies of Values in Science</em>. New York: Oxford University Press.
</div>
<div id="ref-FernandezPintoLegitimizingValuesRegulatory2019" class="csl-entry">
Fernández Pinto, Manuela, and Daniel J. Hicks. 2019. <span>“Legitimizing Values in Regulatory Science.”</span> <em>Environmental Health Perspectives</em> 127 (3): 035001. <a href="https://doi.org/10.1289/EHP3317">https://doi.org/10.1289/EHP3317</a>.
</div>
<div id="ref-GouldRocksAgesScience2011" class="csl-entry">
Gould, Stephen Jay. 2011. <em>Rocks of Ages: Science and Religion in the Fullness of Life</em>. Random House Publishing Group.
</div>
<div id="ref-HicksScientificControversiesProxy2017" class="csl-entry">
Hicks, Daniel J. 2017. <span>“Scientific Controversies as Proxy Politics.”</span> <em>Issues in Science and Technology</em>, January 2017. <a href="https://www.jstor.org/stable/24891967">https://www.jstor.org/stable/24891967</a>.
</div>
<div id="ref-HolmanNewDemarcationProblem2022" class="csl-entry">
Holman, Bennett, and Torsten Wilholt. 2022. <span>“The New Demarcation Problem.”</span> <em>Studies in History and Philosophy of Science</em> 91 (February): 211–20. <a href="https://doi.org/10.1016/j.shpsa.2021.11.011">https://doi.org/10.1016/j.shpsa.2021.11.011</a>.
</div>
<div id="ref-KriegerEpidemiologyPeopleHealth2011" class="csl-entry">
Krieger, Nancy. 2011. <em>Epidemiology and the People’s Health: Theory and Context</em>. New York: Oxford University Press.
</div>
<div id="ref-SarewitzScienceEnvironmentalPolicy2000" class="csl-entry">
Sarewitz, Daniel. 2000. <span>“Science and Environmental Policy: An Excess of Objectivity.”</span> In <em>Earth Matters:&nbsp; The Earth Sciences, Philosophy, and the Claims of Community</em>, edited by Robert Frodeman, 79–98. Prentice Hall. <a href="http://www.cspo.org/_old_ourlibrary/ScienceandEnvironmentalPolicy.htm">http://www.cspo.org/_old_ourlibrary/ScienceandEnvironmentalPolicy.htm</a>.
</div>
<div id="ref-SarewitzHowScienceMakes2004" class="csl-entry">
———. 2004. <span>“How Science Makes Environmental Controversies Worse.”</span> <em>Environmental Science and Policy</em> 7 (5): 385–403. <a href="https://doi.org/10.1016/j.envsci.2004.06.001">https://doi.org/10.1016/j.envsci.2004.06.001</a>.
</div>
<div id="ref-StaplefordCostLivingAmerica2009" class="csl-entry">
Stapleford, Thomas A. 2009. <em>The Cost of Living in America: A Political History of Economic Statistics, 1880-2000</em>. Cambridge University Press.
</div>
</div></section></div> ]]></description>
  <guid>https://dhicks.github.io/posts/2023-01-25-legitimacy-vfi.html</guid>
  <pubDate>Tue, 24 Jan 2023 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Leftist dynamics for city simulation games</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2021-06-26-city-sim-dynamics.html</link>
  <description><![CDATA[ 




<p>I’ve been playing <em>Cities: Skylines</em> recently, and that prompted me to go back and re-read Kevin Baker’s <a href="https://logicmag.io/play/model-metropolis/">Model Metropolis</a>, on the libertarian assumptions baked in to the dynamics of <em>SimCity</em> and its descendants, including <em>C:S</em>.</p>
<p>City simulators aren’t static. Successive games have added new dynamics and more complex models of older dynamics. Turning all of this over, I asked myself what dynamics I would like to see in a city simulator, that would (a) mitigate or replace the libertarian assumptions Will Wright took from Forrester, and (b) would enable users/players to explore some of the pressing issues in today’s cities? I can’t claim any expertise whatsoever in urban geography, sociology, or dynamics, so I would really be interested in how folks who do have such expertise would answer these questions. For what it’s worth, here are some of the things I came up with.</p>
<p><strong>Cost of living</strong> and <strong>land/building/business ownership</strong>. Even in rural California we have conversations about homelessness, lack of affordable housing, and the cost of living. Insofar as <em>C:S</em> has a general measure of how well your city is doing, it’s land value. But, in the real world, protecting and promoting land value reflects the interests of owners rather than renters or employees; increasing land value tends to increase the cost of living, gentrification, homelessness, and sprawl. The simulation could also explore uncommon forms of ownership, such as co-operatives and publicly-owned residences or businesses.</p>
<p><strong>Alternative services</strong>, especially for policing and jails. Last summer the revival of the Black Lives Matter movement prompted renewed calls for abolition of policing and jails. In <em>C:S</em>, increasing police presence causes reduced crime which increases land values. There are no downsides to the expansion of police, and no way to explore non-carceral alternatives such as greater mental health and counseling services. Notably, <em>C:S</em> provides a wide range of transportation options, giving players ways to explore cities that emphasize bicycles and public transit rather than cars. It would be great to be able to do the same with policing.</p>
<p><strong>Factions</strong> are the first of two mechanics I’d like to see adopted from <em>Stellaris</em>, a 4X strategy game developed by Paradox (Paradox is the publisher of <em>C:S</em>). Briefly, factions have a happiness score based on the policies that the player has adopted; depending on how (un)happy the faction is, and the share of the population that supports the faction, they can give bonuses or penalties to gameplay. For a city simulation, I can imagine factions based on real-world movements such as NIMBYs, YIMBYs, and social housing advocates; small businesses, unions; abolition and “law and order”; and environmental justice. While not the most sophisticated simulation of democracy, factions would bring a model of politics into a genre that’s basically autocratic on its face. Perhaps, simulating the tension between a mayor and city council, significantly powerful factions would be able to enact or retract policies against the player’s will.</p>
<p>The need to hire <strong>managers</strong> is the other mechanic I’d like to see brought over from <em>Stellaris</em>. In <em>Stellaris</em>, you need to hire various officials to run your empire — scientists to conduct expeditions, governors of regions, military leaders. These officials bring bonuses (and sometimes penalties), and can also be allied with factions. Over time they gain experience, becoming more effective; but can also die. In a city simulation these managers would be heads of major service areas: policing, fire, education, health, sanitation, traffic, parks, etc. I can imagine complex interactions with the faction dynamics; different factions might support rival police chiefs, for example.</p>
<p>Finally, it would be really interesting to add <strong>explicit options to explore alternative dynamics</strong>, especially for controversial processes. For example, <em>C:S</em> uses the simple model that increased police causes lower crime causes people to be happy causes property values to increase. But Marxist and critical race analyses of policing suggest a different model of the effects of policing: increasing police protects ownership, which increases property values; this makes property owners and rich (white) people happy, but increases cost of living for poor people (of color) and (also) makes them unhappy directly, while increasing crime (as it’s measured by arrests and other data produced by the police), which can increase support for “law and order” factions. <em>C:S</em> already supports <a href="https://steamcommunity.com/sharedfiles/filedetails/?id=2025147082">mods that change the population dynamics</a>, and it’s easy to turn these on and off to compare different dynamics.</p>



 ]]></description>
  <guid>https://dhicks.github.io/posts/2021-06-26-city-sim-dynamics.html</guid>
  <pubDate>Sat, 26 Jun 2021 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Talk: Open science can’t solve the replication crisis</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2021-03-19-open-science-i.html</link>
  <description><![CDATA[ 




<p>I gave this talk yesterday at the <a href="https://pencelab.be/events/ds2-2021/">Digital Studies for Digital Science (DS2)</a> online conference. You can watch the recording <a href="https://www.youtube.com/watch?v=uheU_W7_oz8">on YouTube</a>.</p>
<section id="open-science-cant-solve-the-replication-crisis" class="level2">
<h2 class="anchored" data-anchor-id="open-science-cant-solve-the-replication-crisis">Open science can’t solve the replication crisis</h2>
<p>In the last few years, concerns about the replication crisis in biomedicine and social psychology have bolstered the open science movement, and played a significant role in arguing for open science requirements at scholarly journals and even government agencies — as in the case of the US Environmental Protection Agency’s “Strengthening Transparency in Regulatory Science” proposed rule. However, the discourse surrounding the replication crisis frequently conflates two very distinct desiderata of experimental-computational science, namely, replicability and reproducibility. Following definitions proposed by the US National Academies, <em>reproducibility</em> is a purely computational notation: the ability of an independent researcher to produce <em>numerically identical</em> output from an analysis, using the <em>same</em> data and analysis code. By contrast, <em>replicability</em> is the ability of an independent researcher to reach <em>qualitatively similar</em> conclusions by repeating an experiment, using the same analytical approach but collecting <em>different</em> data. I first argue that reproducibility has minimal (but non-zero) epistemic value, comparable to mere logical consistency. Next I survey a variety of proposed causes for the replication crisis: p-hacking, publication bias, insufficient statistical power, unrepresentative samples, publish-or-perish incentive structures, noisy measurement, underspecified phenomena, imprecise theory, data mismanagement, software bugs, and fraud. I argue that open science requirements effective promotely reproducibility, but promote replicability only insofar as replication failures are due to causes that leave traces (as in historical science) in data and code. Because very few of the proposed causes leave such traces, open science cannot solve the replication crisis.</p>


</section>

 ]]></description>
  <guid>https://dhicks.github.io/posts/2021-03-19-open-science-i.html</guid>
  <pubDate>Fri, 19 Mar 2021 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Teaching critical thinking in 2020</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2020-07-10-critical-thinking.html</link>
  <description><![CDATA[ 




<p>Back in May, I read a blog post by Cathy Davidson, an English professor at CUNY Grad Center, <a href="https://www.hastac.org/blogs/cathy-davidson/2020/05/11/single-most-essential-requirement-designing-fall-online-course">“The Single Most Essential Requirement in Designing a Fall Online Course.”</a> It’s a really useful meditation for academics (and other teachers, and really anyone who works with young people) start to plan our courses for Fall 2020. Here’s the question that’s Davidson’s answer to the question-as-statement of her title:</p>
<blockquote class="blockquote">
<p>Before we begin to design our fall syllabus, before we make clever instructional videos, we all need to think from a student’s point of view. We need to try to understand what it means to be studying for a future you don’t know that you will have. No one knows what lies ahead in the best of times. Now, all the predictions seem like some dystopian futuristic novel. Total social breakdown? Total economic collapse? A health emergency in which millions die over the next three or four years? How do you study to prepare for this future?</p>
<p>What do our students need now? That is the essential question for going on line. Whether teaching algebraic geometry or sociology or literature or art or religion, we need to begin with the question of: <strong>what would I need if I were a student in this historic moment?</strong> [my emphasis]</p>
</blockquote>
<p>I’m teaching a critical thinking course this Fall. I immediately knew what the students in my course would need in the current moment: <strong>the virtues of engaging well in conversation across deep, emotionally-laden disagreement</strong>. If you’re an 19-year-old lefty or progressive, how do you talk with your Trump supporter parents about the election? Maybe you’re white and Black Lives Matter is challenging some of your deep-seated beliefs about policing, prisons, and criminals. Or you’re on DACA and your bio lab partner is talking about the pro-ICE rally that he went to last week. (CW: This link describes a rally that actually happened at UC Merced in 2018. <a href="https://www.mercedsunstar.com/news/local/education/uc-merced/article204022279.html" class="uri">https://www.mercedsunstar.com/news/local/education/uc-merced/article204022279.html</a>) Our culture right now doesn’t provide many models of how (and whether) to have good conversations about these kinds of issues.</p>
<p>There’s significant demand for these models in our society right now, as witnessed by the success of books nominally about reason, logic, and deep disagreement by authors associated with the right-wing/anti-left <a href="https://en.wikipedia.org/wiki/Intellectual_dark_web">“intellectual dark web.”</a> (I’m thinking in particular of <a href="https://medium.com/@cianchartier/a-review-of-stefan-molyneuxs-the-art-of-the-argument-2c1c83fa7802">Stefan Molyneux’</a> <a href="https://freethoughtblogs.com/pharyngula/2017/09/05/barely-an-argument-a-review-of-the-art-of-the-argument-western-civilizations-last-stand-by-stefan-molyneux/"><em>The Art of the Argument</em></a>, <a href="https://arcdigital.media/the-political-pick-up-artists-6bcece72bb92">Boghossian and Lindsay’s <em>How to Have Impossible Conversations</em></a>, and to an extent <a href="https://www.currentaffairs.org/2018/03/the-intellectual-we-deserve">Jordan Peterson’s <em>12 Rules for Life</em></a>. These links are all to reviews or more general critical discussions.) These books have two problems. First, as textbooks in critical thinking they’re not very good. (Check the reviews.) Second, they serve as subtle entry points to the right-wing social media network. I’m not saying that students should never encounter and engage with right-wing ideas. But it should be done with care, and honesty, and none of these books are transparent about their political alignment.</p>
<p>Many standard critical thinking textbooks take one of two approaches. The first approach, which I think is also the most common, focuses on the technicalities of logic: definitions, fallacies, often Aristotelean syllogism and a little sentence logic; maybe a little probability theory as (Bayesian) inductive logic. A good example of this approach is <a href="https://www.google.com/books/edition/A_Concise_Introduction_to_Logic/qGBQAwAAQBAJ">Hurley’s <em>A Concise Introduction to Logic</em></a>, currently in its twelfth edition and paradoxically running to more than 700 pages. If there’s anything in this book about interacting with other people in any way, it doesn’t appear in the table of contents.</p>
<p>I think being familiar with more-or-less formalized logic can be useful, especially for upper-division philosophy majors. But it’s much less useful if what we want to do is talk with people with whom we disagree.</p>
<p>The second approach to critical thinking aims to cultivate what I think of as the “liberal conversational virtues”: open-mindedness, direct and accessible expression of one’s ideas, attentive listening, empathy, interpretive charity, and identifying common ground. Some good examples here are Maureen Linker’s <em>Intellectual Empathy</em>, Martin Fowler’s <em>The Ethical Practice of Critical Thinking</em>, and Anthony Weston’s <em>Rulebook for Arguments</em>. These books come closer to what I want to offer my students. (And that’s one of the main reasons I chose Morrow and Weston’s <em>Workbook for Arguments</em>, which includes the full text of <em>Rulebook</em> plus exercises and larger assignments.)</p>
<p>But these liberal conversational virtues can be — and historically have been — weaponized against cultural movements for social justice. I’m thinking of tactics such as <a href="https://en.wikipedia.org/wiki/Sealioning">sealioning</a>, <a href="https://en.wikipedia.org/wiki/Tone_policing">tone policing</a>, <a href="https://en.wikipedia.org/wiki/Internet_troll#Concern_troll">concern trolling</a>, “just asking questions,” and Alice McIntyre’s notion (via Alison Bailey) of <a href="https://philpapers.org/rec/BAIWTA">“white talk”</a> (especially assertions of one’s anti-racist open-mindedness and empathy as a way to avoid criticism). The liberal conversational virtues become <em>vices</em> when they make my students more susceptible to these tactics.</p>
<p>As a first pass, I might address this limitation of teaching the liberal conversational virtues by assigning some additional readings and activities, discussing these various tactics, helping my students spot them in the wild, and giving them tools to respond appropriately.</p>
<p>I’ll probably do some of this. But it risks turning a chunk of my class into a laundry list of fallacies — just a different list from the standard one you find in Hurley’s textbook. So, as a second pass, I think we (including both teachers and students of critical thinking) need to think about what goes wrong when the liberal conversational virtues are weaponized. Why is the sealion’s endless call to open debate not just annoying but actually ethically problematic?</p>
<p>I think the answer has to do with <em>power</em>. Characteristically, sealioning and similar tactics are directed downwards, from someone who is relatively high in our society’s power hierarchies — and interested in defending those hierarchies — and towards someone who is relatively low — and interesting in criticizing them. In other words, sealioning is a vice because it functions to protect an unjust status quo from criticism. (Linker discusses power dynamics in the introduction and second chapter of her book, though on a quick skim I can’t tell if she’s connecting power to my concern with the ways thinks like empathy and open-mindedness can be weaponized and do harm. I might assign some of her discussion in my course.)</p>
<p>On this analysis, critical thinking — and specifically whether certain approaches to conversation are, in a given case, virtuous or vicious — is loaded with substantive assumptions about power and justice. I can’t assume that students share my substantive assumptions about power and justice. (Though, in my experience so far, most UC Merced students are pretty sympathetic to my views.) But I can, at least, teach them that the deep disagreements in our society also apply to the meta-level; that our deep disagreements about Black Lives Matter or Trump are, at the same time, also deep disagreements about how to talk about our deep disagreements.</p>



 ]]></description>
  <guid>https://dhicks.github.io/posts/2020-07-10-critical-thinking.html</guid>
  <pubDate>Fri, 10 Jul 2020 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Configuring Github and Travis-CI for Automated Lab Feedback</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2020-07-10-travis-lab-feedback.html</link>
  <description><![CDATA[ 




<p>Last summer, anticipating teaching for the first time since 2013, I started reading about <a href="https://www.insidehighered.com/news/2019/04/02/professors-reflections-their-experiences-ungrading-spark-renewed-interest-student">ungrading</a>, and somewhere (maybe even in that piece, I didn’t check) read about a computer scientist who uses <a href="https://en.wikipedia.org/wiki/Continuous_integration">continuous integration</a> (CI) to automatically give students feedback on their CS lab assignments. Each lab assignment has well-specified goals, and the CI automated tests evaluate the students’ solutions for correctness. Successful completion of the assignment can be counted automatically, specifications-grading style, or the instructor can review the code after it’s working for things like coding style and efficiency.</p>
<p>This fall I’ll be teaching a graduate methods course on data science. This seems like a great course for implementing this CI approach. But I haven’t used CI before, and <a href="https://kieranhealy.org/blog/archives/2015/10/16/using-containerized-travis-ci-to-check-r-in-rmarkdown-files/">the tutorials for using Travis-CI with R</a> turned out to be unnecessarily complicated, not least because Travis-CI now has good support for R. The purpose of this post is to briefly review how to set up Github and Travis-CI for automated lab feedback.</p>
<p>Each lab assignment is based on this template repo: <a href="https://github.com/data-science-methods/lab-test" class="uri">https://github.com/data-science-methods/lab-test</a>.</p>
<section id="basic-lab-workflow" class="level1">
<h1>Basic lab workflow</h1>
<p><strong>This setup assumes a workflow for lab assignments where students clone a Github repository with instructions, data, etc.; complete the assignment in a single R script; that a working solution has deterministic values for variables with set names; and that students submit their work using a pull request.</strong> A different programming language (or multiple programming languages) will require different infrastructure for running unit tests. Multiple R scripts (including more complex project structures) will require more articulated test files. Writing unit tests for non-deterministic values will be quite a bit more complicated than the example test in the template.</p>
<p>There are three phases for this approach: one-time setup with accounts, and then steps you and your students will do for each lab assignment.</p>
</section>
<section id="instructor-account-setup" class="level1">
<h1>Instructor: Account setup</h1>
<p>You’ll only need to do these steps once.</p>
<ol type="1">
<li>You’ll need a Github account.
<ul>
<li>I assume you already have one of these and that you know basic git terminology and Github workflows.<br>
</li>
<li><em>Optional:</em> I went ahead and created a new <a href="https://github.com/settings/organizations">organization</a> for my course, so that the course website and lab repos all live together. But that’s strictly unnecessary.<br>
</li>
</ul></li>
<li>You’ll need a <a href="https://travis-ci.org">Travis-CI</a> account, which you create using a Github login. Travis-CI is free for working with public Github repositories.<br>
</li>
<li><em>Optional:</em> If you created a new organization for your course, make sure it shows up in your Travis-CI settings: Click on your profile picture (upper-right corner), then Settings. Look for the list of organizations on the left-hand column. If your organization isn’t there, then at the bottom of that column you should see a link to “Review and add your authorized organizations.”</li>
</ol>
<p>I <em>think</em> that’s basically it. Travis-CI should now be able to see your public repositories.</p>
</section>
<section id="instructor-repository-setup" class="level1">
<h1>Instructor: Repository setup</h1>
<p>You’ll do these steps when you create each lab assignment.</p>
<ol type="1">
<li>In the <a href="https://github.com/data-science-methods/lab-test">template repo</a>, click the green “Use this template” button (where the Clone button usually lives) to create a new repo for the lab.
<ul>
<li>I’m going to use the naming scheme <code>lab-01</code> where <code>01</code> indicates the week of the course.<br>
</li>
</ul></li>
<li>Clone the new repo to the machine where you work.<br>
</li>
<li>Edit <code>lab.R</code> with the assignment instructions.<br>
</li>
<li><strong>If you add any packages to the setup, add them to <code>DESCRIPTION</code> as well.</strong>
<ul>
<li>Travis-CI assumes that R repositories are packages. It will automatically install dependencies, but only if all of the dependencies (including <code>testthat</code>) are listed in the <code>DESCRIPTION</code> file. You don’t need any of the usual package metadata; all you need are the list of <code>Imports</code>.<br>
</li>
</ul></li>
<li>For each problem in the assignment, write appropriate tests in <code>tests/test_lab.R</code>.
<ul>
<li>You write tests using <code>testthat</code> expectations: <a href="https://testthat.r-lib.org/reference/index.html" class="uri">https://testthat.r-lib.org/reference/index.html</a>.<br>
</li>
</ul></li>
<li>If you changed any file names, make sure they’re consistent across <code>lab.R</code>, <code>tests/test_lab.R</code> (which <code>source()</code>es the assignment script), and <code>.travis.yml</code> (it needs to know where to point <code>testthat::test_dir()</code>).<br>
</li>
<li><em>Optional:</em> Create a <code>solutions</code> branch. Fill in solutions for each problem, and run <code>testthat::test_dir('tests')</code> to check that your instructions and tests work as expected. Cherry-pick any corrections back to <code>master</code>.</li>
<li>Push <code>master</code> back up to Github.<br>
</li>
<li>In Travis-CI’s Settings, find the lab repo and flip the switch to turn on CI. (In my experience, it can take like 10 seconds for Travis-CI to see the new repo or a push/pull request to an active repo.)</li>
<li><em>Optional:</em> Push <code>solutions</code> up to ensure that Travis-CI is working as expected.
<ul>
<li>Note that there doesn’t appear to be a way to have a private branch of a public repo.</li>
</ul></li>
</ol>
</section>
<section id="student-lab-workflow" class="level1">
<h1>Student: Lab workflow</h1>
<p>The students will do these steps when they work on the lab assignment.</p>
<ol type="1">
<li>Fork the lab assignment to their own account, then clone the fork to their working machine.<br>
</li>
<li>Modify the yaml header for the lab assignment with their name and so on.</li>
<li>Work in <code>lab.R</code> to complete the assignment, per instructions.<br>
</li>
<li><strong>If any packages are added to the setup, add them to <code>DESCRIPTION</code> as well.</strong></li>
<li>At any point, run <code>testthat::test_dir('tests')</code> to get immediate feedback on their progress.<br>
</li>
<li>At any point, submit a pull request to get automated feedback via Travis-CI.
<ul>
<li><a href="https://docs.travis-ci.com/user/pull-requests/">Travis-CI docs on building pull requests</a></li>
</ul></li>
<li>Submit their work by submitting a final (passing) pull request.<br>
</li>
<li><em>Optional:</em> Use the RStudio knitr button (or <code>rmarkdown::render('lab.R')</code>) to generate a pretty HTML or PDF version of their completed assignment.</li>
</ol>


</section>

 ]]></description>
  <guid>https://dhicks.github.io/posts/2020-07-10-travis-lab-feedback.html</guid>
  <pubDate>Fri, 10 Jul 2020 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Replication, reproducibility, and Strengthening Transparency</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2020-05-18-strengthening-transparency.html</link>
  <description><![CDATA[ 




<p><a href="https://www.epa.gov/osa/strengthening-transparency-regulatory-science"><em>Strengthening Transparency in Regulatory Science</em></a> is an open science rule that the US Environmental Protection Agency proposed in April 2018. If adopted, the rule would restrict the scientific information that EPA uses in policymaking, requiring that the data and analysis code be available either publicly or in a secure data enclave for independent reanalysis. Strengthening Transparency is non-accidentally similar to the <a href="https://science.sciencemag.org/content/356/6342/989">HONEST Act</a> and <a href="http://blogs.edf.org/climate411/2017/02/07/less-science-more-cost-why-the-misguided-secret-science-bill-is-bad-policy/">Secret Science Act</a>, which were attempts by Republicans in previous congresses to introduce the same requirement. Since at least the 1990s, industry and its allies have attempted to argue that environmental public health research can’t be trusted unless “we” can inspect the “raw data.”</p>
<p>Strengthening Transparency tries to justify itself with explicit appeals to the <a href="https://en.wikipedia.org/wiki/Replication_crisis">replication crisis</a> that’s been unfolding in, primarily, social psychology. So the rule has a nice convergence between my ongoing research on public scientific controversies and my side interest in the replication crisis.</p>
<p>In March of this year, EPA published a Supplemental Notice of Proposed Rulemaking, which added definitions for several of the key terms and opened another round of public comments. I submitted a comment based on the distinction between reproducibility (can we run the same data through the same code to get quantitatively identical output) and replicability (can we run a similar experiment to get new data that support qualitatively similar conclusions). I argued that open science can promote reproducibility, but not replicability; while Strengthening Transparency is nominally based on concerns about replicability. So the proposed rule can’t do anything about the problem it’s supposed to address.</p>
<p>While I’ve been following the replication crisis — and even taught an undergraduate course on it last fall — the focus of my research has really been on more public, policy-relevant controversies. So there’s a good chance my comment is the only thing I’ll ever write on the replication crisis. You can read it here: <a href="https://drive.google.com/open?id=1Ze4EgDgtQJCQoW1_p4ebuDGOgtX2oi79" class="uri">https://drive.google.com/open?id=1Ze4EgDgtQJCQoW1_p4ebuDGOgtX2oi79</a></p>



 ]]></description>
  <guid>https://dhicks.github.io/posts/2020-05-18-strengthening-transparency.html</guid>
  <pubDate>Mon, 18 May 2020 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Talk: “When Virtues are Vices: The Weaponization of Scientific Norms”</title>
  <dc:creator>Dan Hicks</dc:creator>
  <link>https://dhicks.github.io/posts/2020-05-18-dayton-talk.html</link>
  <description><![CDATA[ 




<p>Back in March, I was supposed to present at a <a href="https://udayton.edu/artssciences/academics/philosophy/science-and-values/index.php">colloquium at the University of Dayton</a> on “Doing Science in a Pluralistic Society.” Due to the pandemic, the colloquium was turned into a virtual conference over two Fridays in April. The recording of my talk is now <a href="https://www.youtube.com/watch?v=R4-nOlvOhq4">on Youtube</a> and at the bottom of this post.</p>
<section id="when-virtues-are-vices-the-weaponization-of-scientific-norms" class="level2">
<h2 class="anchored" data-anchor-id="when-virtues-are-vices-the-weaponization-of-scientific-norms">When Virtues are Vices: The Weaponization of Scientific Norms</h2>
<p>Conservative critics of mainstream climate science and environmental health science often “weaponize” traditional scientific virtues to manufacture doubt and slow the regulatory process. For example, climate skeptics appeal to Popperian falsifiability and Mertonian norms such as organized skepticism, arguing that mainstream climate science is unfalsifiable and propped up through closed scientific communities. The “Secret Science Reform Act,” which was passed by the US House of Representatives in both 2014 and 2015, would have prohibited US EPA from using “scientific technical information” unless it was “publicly available online in a manner that is sufficient for independent analysis and substantial reproduction of research results.”</p>
<p>These rhetorical appeal to traditional scientific virtues make it difficult to dismiss these anti-environmentalists as “anti-science.” Indeed, the effect of these appeals is to represent anti-environmentalists as defenders of science from the threats of politicization.</p>
<p>I first argue that this kind of appeal to scientific virtues depends on a “narrowly epistemic” conception of the aims of scientific research. On this conception, the sole constitutive aim of science is to produce impartial knowledge. Other aims — such as protecting human health and the environment — are less important. The traditional scientific virtues reflect this strict separation of constitutive epistemic aims and practical uses of science.</p>
<p>I go on to propose an alternative conception of the aims of scientific research, according to which epistemic and pragmatic aims of science can be equally important and mutually influencing. This view of the aims of science supports an alternative understanding of scientific virtues. On this understanding, when traditional scientific “virtues” are weaponized by conservative anti-environmentalists — when they are used to delay protective regulation, frustrating the constitutive pragmatic aims of a scientific field — they are actually vices rather than virtues.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/R4-nOlvOhq4" frameborder="0" allow="picture-in-picture" allowfullscreen="">
</iframe>


</section>

 ]]></description>
  <guid>https://dhicks.github.io/posts/2020-05-18-dayton-talk.html</guid>
  <pubDate>Mon, 18 May 2020 07:00:00 GMT</pubDate>
</item>
</channel>
</rss>
